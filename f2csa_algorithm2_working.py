#!/usr/bin/env python3
"""
F2CSA Algorithm 2 - WORKING Implementation
Uses the correct hypergradient computation from Algorithm 1
"""

import torch
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import argparse
from typing import Dict, Optional, Tuple
from problem import StronglyConvexBilevelProblem
from f2csa_algorithm_corrected_final import F2CSAAlgorithm1Final
import warnings

warnings.filterwarnings('ignore')

class F2CSAAlgorithm2Working:
    """
    F2CSA Algorithm 2: Nonsmooth Nonconvex Algorithm with Inexact Stochastic Hypergradient Oracle
    WORKING implementation using correct hypergradient computation
    """
    
    def __init__(self, problem: StronglyConvexBilevelProblem, device: str = 'cpu', dtype=torch.float64):
        self.problem = problem
        self.device = device
        self.dtype = dtype
        
        # Initialize Algorithm 1 for hypergradient computation
        self.algorithm1 = F2CSAAlgorithm1Final(problem, device=device, dtype=dtype)
        
        # Persistent cache for two-level warm-start (like main.py)
        self.prev_y = None              # Previous inner problem solution
        self.prev_lambda = None         # Previous dual solution  
        self.prev_y_lamb_opt = None     # Previous Lagrangian problem solution
        self.prev_adam_state = None     # Previous Adam optimizer state
        
    def clip_D(self, v: torch.Tensor, D: float) -> torch.Tensor:
        """
        Clipping function: clip_D(v) := min{1, D/||v||} * v
        """
        v_norm = torch.norm(v)
        if v_norm <= D:
            return v
        else:
            return (D / v_norm) * v
    
    def optimize(self, x0: torch.Tensor, T: int, D: float, eta: float, 
                 delta: float, alpha: float, N_g: int = None, 
                 warm_ll: bool = False, keep_adam_state: bool = False,
                 plot_name: str = None, save_warm_name: str = None,
                 perturbation_std: float = 0.01) -> Dict:
        """
        Run F2CSA Algorithm 2 optimization with WORKING hypergradient computation
        Now using perturbation + optimizer approach like main.py for proper warm start
        """
        print("ðŸš€ F2CSA Algorithm 2 - WORKING Implementation")
        print("=" * 70)
        print(f"T = {T}, D = {D:.6f}, Î· = {eta:.6f}")
        print(f"Î´ = {delta:.6f}, Î± = {alpha:.6f}")
        print(f"Perturbation std: {perturbation_std:.6f}")
        print()
        
        # Set default N_g if not provided
        if N_g is None:
            # Balanced N_g for working implementation
            N_g = max(10, min(100, int(1.0 / (alpha**1.5))))
        
        print(f"Using N_g = {N_g} samples for hypergradient estimation")
        print()
        
        # Initialize - using optimizer approach like main.py
        x = x0.clone().detach().requires_grad_(True)
        optimizer = torch.optim.Adam([x], lr=eta)
        
        # Storage for results
        z_history = []
        x_history = []
        g_history = []
        ul_losses = []
        hypergrad_norms = []
        
        print("Starting optimization...")
        print("-" * 50)
        
        # Main optimization loop - Using perturbation + optimizer approach like main.py
        for t in range(1, T + 1):
            # Perturbation like main.py: s ~ Normal(0, perturbation_std)
            s = torch.normal(torch.zeros_like(x), torch.ones_like(x)) * perturbation_std
            xx = x + s  # This is z_t in the paper
            
            # TWO-LEVEL WARM START SYSTEM (like main.py)
            
            # LEVEL 1: Inner Problem Warm Start
            if warm_ll and (self.prev_y is not None or self.prev_lambda is not None):
                # Pass warm-start information to Algorithm 1 for inner problem
                oracle_result = self.algorithm1.oracle_sample(xx, alpha, N_g, 
                                                             prev_y=self.prev_y, 
                                                             prev_lambda=self.prev_lambda,
                                                             keep_adam_state=keep_adam_state)
            else:
                oracle_result = self.algorithm1.oracle_sample(xx, alpha, N_g)
            
            # Extract solutions from inner problem
            g_t = oracle_result[0] if isinstance(oracle_result, tuple) else oracle_result
            if isinstance(oracle_result, tuple) and len(oracle_result) >= 3:
                y_opt = oracle_result[1]  # Inner problem solution (y*)
                lambda_opt = oracle_result[2]  # Dual solution (Î»*)
            else:
                # Fallback: solve inner problem directly
                y_opt, _ = self.problem.solve_lower_level(xx)
                lambda_opt = torch.zeros(self.problem.num_constraints, device=self.device, dtype=self.dtype)
            
            # LEVEL 2: Lagrangian Problem Warm Start
            # This is where we would solve the Lagrangian optimization with warm start
            # For now, we use the inner problem solution as the Lagrangian solution
            # In a full implementation, this would be a separate optimization step
            y_lamb_opt = y_opt  # Simplified: use inner solution as Lagrangian solution
            
            # Compute upper-level loss for monitoring
            ul_loss_t = self.problem.upper_objective(xx, y_opt).item()
            ul_losses.append(ul_loss_t)
            
            # Update optimizer (like main.py)
            x.grad = g_t
            optimizer.step()
            optimizer.zero_grad()
            
            # Store history
            z_history.append(xx.clone().detach())
            x_history.append(x.clone().detach())
            g_history.append(g_t.clone().detach())
            hypergrad_norms.append(torch.norm(g_t).item())
            
            # TWO-LEVEL WARM START PERSISTENCE (like main.py)
            if warm_ll:
                # Level 1: Cache inner problem solutions for next iteration
                self.prev_y = y_opt.clone().detach()
                self.prev_lambda = lambda_opt.clone().detach()
                
                # Level 2: Cache Lagrangian problem solution for next iteration
                self.prev_y_lamb_opt = y_lamb_opt.clone().detach()
                
                # Cache Adam state if requested
                if keep_adam_state and hasattr(self.algorithm1, 'adam_state'):
                    self.prev_adam_state = self.algorithm1.adam_state.copy() if self.algorithm1.adam_state else None
            
            # Print progress
            if t % max(1, T // 20) == 0 or t <= 10:
                g_norm = torch.norm(g_t).item()
                print(f"Iteration {t:4d}/{T}: ||g_t|| = {g_norm:.6f}, ul_loss = {ul_loss_t:.6f}")
                if warm_ll:
                    print(f"  Warm start: y_opt shape={tuple(y_opt.shape)}, Î»_opt shape={tuple(lambda_opt.shape)}")
        
        print()
        print("Computing final output...")
        
        # For the perturbation + optimizer approach, we use the final x as output
        # This is more similar to main.py approach
        x_out = x.clone().detach()
        print(f"Using final x as output (perturbation + optimizer approach)")
        print()
        
        # Compute final gradient norm
        final_oracle_result = self.algorithm1.oracle_sample(x_out, alpha, N_g)
        final_g = final_oracle_result[0] if isinstance(final_oracle_result, tuple) else final_oracle_result
        final_g_norm = torch.norm(final_g).item()
        
        # Compute final upper-level loss f(x, y*) as Algorithm 2 gap
        y_star, _ = self.problem.solve_lower_level(x_out)
        final_ul_loss = self.problem.upper_objective(x_out, y_star).item()
        
        print(f"Final UL loss f(x, y*): {final_ul_loss:.6f}")
        print(f"Final hypergradient norm (diagnostic): {final_g_norm:.6f}")
        print(f"Final point: {x_out}")
        print()
        
        # Plot hypergradient norm and UL loss over iterations
        try:
            fig, ax1 = plt.subplots(figsize=(10, 6))
            ax1.plot(hypergrad_norms, color='tab:blue', label='||âˆ‡FÌƒ||')
            ax1.set_xlabel('Iteration')
            ax1.set_ylabel('Hypergradient norm', color='tab:blue')
            ax1.tick_params(axis='y', labelcolor='tab:blue')
            ax2 = ax1.twinx()
            ax2.plot(ul_losses, color='tab:orange', label='f(x, y*)')
            ax2.set_ylabel('UL loss f(x, y*)', color='tab:orange')
            ax2.tick_params(axis='y', labelcolor='tab:orange')
            plt.title('Algorithm 2: Hypergradient norm and UL loss over iterations')
            fig.tight_layout()
            if plot_name is None:
                plot_name = 'algo2_hg_ul_loss.png'
            plt.savefig(plot_name, dpi=200, bbox_inches='tight')
            plt.close(fig)
            print(f"Saved plot to {plot_name}")
        except Exception as e:
            print(f"Plotting failed: {e}")
        
        # Save warm start for next runs (point with best hypergradient from current run)
        try:
            if save_warm_name is None:
                save_warm_name = 'algo2_warmstart.npy'
            np.save(save_warm_name, x_out.detach().cpu().numpy())
            print(f"Saved warm start (F2CSA Algorithm 2 output) to {save_warm_name}")
            print(f"  Warm start point shape: {tuple(x_out.shape)}")
            print(f"  Warm start point: {x_out}")
        except Exception as e:
            print(f"Failed to save warm start: {e}")
        
        return {
            'x_out': x_out,
            'final_gradient': final_g,
            'final_gradient_norm': final_g_norm,
            'final_ul_loss': final_ul_loss,
            'y_star': y_star,
            'x_history': x_history,
            'z_history': z_history,
            'g_history': g_history,
            'hypergrad_norms': hypergrad_norms,
            'ul_losses': ul_losses,
            'converged': final_g_norm < 1e-3,
            'iterations': T,
            'warm_start_info': {
                'prev_y': self.prev_y,
                'prev_lambda': self.prev_lambda,
                'prev_y_lamb_opt': self.prev_y_lamb_opt,
                'prev_adam_state': self.prev_adam_state
            }
        }

if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='F2CSA Algorithm 2 with warm-start support')
    parser.add_argument('--T', type=int, default=5000, help='Number of iterations')
    parser.add_argument('--D', type=float, default=0.05, help='Clipping parameter')
    parser.add_argument('--eta', type=float, default=0.0001, help='Step size')
    parser.add_argument('--Ng', type=int, default=64, help='Number of gradient samples')
    parser.add_argument('--alpha', type=float, default=0.1, help='Alpha parameter')
    parser.add_argument('--warm-ll', action='store_true', help='Enable lower-level warm-start')
    parser.add_argument('--keep-adam-state', action='store_true', help='Keep Adam optimizer state')
    parser.add_argument('--plot-name', type=str, default=None, help='Plot filename')
    parser.add_argument('--save-warm-name', type=str, default=None, help='Warm start save filename')
    parser.add_argument('--perturbation-std', type=float, default=0.01, help='Perturbation standard deviation')
    parser.add_argument('--dim', type=int, default=5, help='Problem dimension')
    parser.add_argument('--constraints', type=int, default=3, help='Number of constraints')
    
    args = parser.parse_args()
    
    # Initialize problem
    problem = StronglyConvexBilevelProblem(
        dim=args.dim, 
        num_constraints=args.constraints, 
        noise_std=0.1, 
        strong_convex=True
    )
    
    algorithm2 = F2CSAAlgorithm2Working(problem)
    
    # Parameters
    alpha = args.alpha
    delta = alpha**3
    
    # Initialize with random starting point
    x0 = torch.randn(args.dim, dtype=torch.float64)
    print(f"Using random initialization: x0 shape = {tuple(x0.shape)}")
    
    print(f"Test point x0: {x0}")
    print(f"Î± = {alpha}")
    print(f"T = {args.T}, D = {args.D}, Î· = {args.eta}")
    print(f"Î´ = {delta:.6f}, N_g = {args.Ng}")
    print(f"Warm-LL: {args.warm_ll}, Keep Adam: {args.keep_adam_state}")
    print()
    
    # Run Algorithm 2 optimization
    results = algorithm2.optimize(
        x0, args.T, args.D, args.eta, delta, alpha, args.Ng,
        warm_ll=args.warm_ll, keep_adam_state=args.keep_adam_state,
        plot_name=args.plot_name, save_warm_name=args.save_warm_name,
        perturbation_std=args.perturbation_std
    )
    
    # Check convergence
    hg_norms = results['hypergrad_norms']
    print(f"Gradient norms history (first 10): {[f'{g:.1f}' for g in hg_norms[:10]]}")
    print(f"Gradient norms history (last 10): {[f'{g:.1f}' for g in hg_norms[-10:]]}")
    
    # Check for stabilization
    if len(hg_norms) >= 10:
        last_10_norms = np.array(hg_norms[-10:])
        std_dev = np.std(last_10_norms)
        mean_norm = np.mean(last_10_norms)
        print(f"Last 10 gradient norms std dev: {std_dev:.4f}")
        print(f"Last 10 gradient norms range: {np.max(last_10_norms) - np.min(last_10_norms):.4f}")
        if std_dev < 2.0 and (np.max(last_10_norms) - np.min(last_10_norms)) < 5.0:
            print("Status: âœ… CONVERGED (based on heuristic)")
        else:
            print("Status: âŒ NOT CONVERGED (based on heuristic)")
    else:
        print("Not enough iterations to check stability.")
