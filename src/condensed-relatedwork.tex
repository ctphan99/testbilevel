\subsection{Related work}\label{sec:related_work} 
% \pswt{the full related work section is in ``related-work.tex''; we can use it for reference if needed}
% The space of algorithms for bilevel programming is vast. 
The vast body of work on asymptotic results for bilevel programming, starting with classical works such as \cite{anandalingam1990solution, ishizuka1992double, white1993penalty, vicente1994descent, ye1995optimality, ye1997exact}, typically fall into two categories: those based on approximate implicit differentiation ~\cite{domke2012generic, pedregosa2016hyperparameter, gould2016differentiating, amos2017optnet, liao2018reviving, agrawal2019differentiable, grazzi2020iteration, lorraine2020optimizing} and those via iterative differentiation~\cite{domke2012generic, maclaurin2015gradient, franceschi2017forward,   franceschi2018bilevel,  shaban2019truncated, grazzi2020iteration}. Another recent line of work  in this category includes \cite{liu2021value, ye2023difference, khanduri2023linearly, gao2024moreau}, which use various smoothing techniques. 
% smoothing on the problem via some combination of barrier function and Moreau envelope. 
% Other uses of smoothing were seen in \cite{ye2023difference} and \cite{gao2024moreau}, which used tools from the literature on difference of convex programs. These results are mostly asymptotic.  

The first non-asymptotic result for bilevel programming was provided by \cite{ghadimi2018approximation}, which was followed by a flurry of work: for example, algorithms that are single-loop stochastic~\cite{chen2021closing, chen2022single, hong2023two}, projection-free~\cite{akhtar2022projection, jiang2023conditional, abolfazli2023inexact, cao2024projection}, use variance-reduction and momentum~\cite{khanduri2021near, guo2021randomized, yang2021provably, dagreou2022framework}, those for single-variable bilevel programs~\cite{sabach2017first, amini2019iterative, amini2019iterativereg, jiang2023conditional, merchav2023convex}, and  for bilevel programs with special  constraints~\cite{tsaknakis2022implicit, khanduri2023linearly, xu2023efficient, abolfazli2023inexact}. 
 
% There have also been some recent works on constrained bilevel programming, mostly focusing on specific types of constraints. Examples of these works include those by \cite{tsaknakis2022implicit} and \cite{khanduri2023linearly} for linear inequality constraints and \cite{xu2023efficient}, which provides asymptotic guarantees for attaining Clarke stationarity with more general constraints. 

The most direct predecessors of our work are \cite{liu2021value, kwon2023fully} and \cite{yao2024constrained, lu2024firstorder}. As alluded to earlier, until recently, there did not exist any fully first-order algorithms with finite-time stationary guarantees for bilevel programming. The work of \cite{liu2022bome} made a significant contribution to this end by providing a method to approximate the hypergradient of the unconstrained bilevel problem  in a fully-first order fashion. This was extended to the stochastic setting by \cite{kwon2023fully} (which we also build upon), which was simplified and improved by \cite{chen2023near}, and  recently extended to the constrained setting by \cite{yao2024constrained, lu2024firstorder}.

While \cite{yao2024constrained, lu2024firstorder} also study the constrained setting, a key difference of our work lies in the stationarity criteria. \cite{yao2024constrained, lu2024firstorder} use KKT stationarity over both upper and lower-level variables as a proxy to the hypergradient stationarity, whereas we directly work with $(\delta, \epsilon)$-stationarity over the upper-level objective.
Moreover, \cite{yao2024constrained} assumes joint convexity of the lower-level constraints in upper and lower variables to allow for efficient projections, 
while we require convexity only in the lower-level variable. 
% \pswt{can we perhaps explain why our notion of stationarity might be \emph{better}?}
% \pswt{add zhaosong-sanyou paper}
% \pswt{is this true when their upper level is also unconstrained?}

% \pswt{explain why ours is more meaningful}. Secondly, \pswt{nonconvex projection} 