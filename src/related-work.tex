\subsection{Related Work}

%latex note: we are using the nicematrix package to be able to get alternately shaded rows when we have multiple rows per row: this feature seems difficult to get with merely tabular (booktabs) see https://stackoverflow.com/questions/69174994/how-to-create-a-latex-table-with-specific-multicolumns-and-multirows
\begin{table}[h]
  \centering
  \resizebox{\linewidth}{!}{%
  \begin{NiceTabular}{*{11}{c}}
  \CodeBefore
  \rowcolors{1}{}{gray!10}[respect-blocks]
  \Body
    \toprule
    \Block{2-1}{\textbf{Paper}} & \Block{1-2}{\textbf{Upper Objective}} & & \Block{1-2}{\textbf{Upper Constraint}} & & \Block{1-2}{\textbf{Lower Objective}} & & \Block{1-2}{\textbf{Lower Constraint}} & & \Block{1-2}{\textbf{Cost}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
    & $C(x,y)$ & Smoothness & $C(x,y)$ & Smoothness & $C(x,y)$ & Smoothness & $C(x,y)$ & Smoothness & Iters. & Per Iter.\\
    \midrule
    \cite{liu2021value} & Cell 1,2.1 & Cell 1,2.2 & Cell 1,2.3 & Cell 1,3 & Cell 1,4 & Cell 1,5 & None \\
    \cite{ji2021bilevel} & --- & \checkmark & --- & --- & (, \strongconvexity) & \checkmark & --- & --- & $\kappa^3 \varepsilon^{-1}$ & Hess$\times$vec \\  
    \cite{kwon2023fully} & --- & \checkmark & \checkmark (set) & --- & (, \strongconvexity) & \checkmark & --- & --- & $\varepsilon^{-3}$ & FO \\
    \cite{chen2023near} & --- & (\checkmark, \checkmark) & --- & --- & (, \strongconvexity) & \checkmark & --- & --- & $\kappa^4\varepsilon^{-2}$ & FO \\
       \Block{2-1}{\cite{abolfazli2023inexact}} & none & (\checkmark, \checkmark) & \checkmark (set) & --- & (, \strongconvexity) & \checkmark & --- & --- & $\varepsilon^{-2}$ & mv \\
    & (\checkmark, \checkmark) & & & & & & & & $\varepsilon^{-1}$ \\
    \cite{tsaknakis2022implicit} & & & & & & & \checkmark(linear) & &  asymptotic\\ 
    \Block{4-1}{\cite{khanduri2023linearly}} & --- & (\checkmark, \checkmark) & \checkmark (set) & --- & (, \strongconvexity) & \checkmark & \checkmark (linear) & \xmark & asymptotic & --- \\
      & weak ($F$) & (\checkmark, \checkmark) & \checkmark (set) & --- & (, \strongconvexity) & \checkmark & \checkmark (linear) & \xmark & $\varepsilon^{-2}$  & \Block{3-1}{Hess$\times$vec} \\ 
      & convex  ($F$) & (\checkmark, \checkmark) & \checkmark (set) & --- & (, \strongconvexity) & \checkmark & \checkmark (linear) & \xmark & $\varepsilon^{-2}$ & \\ 
      & strongly  ($F$)  & (\checkmark, \checkmark) & \checkmark (set) & --- & (, \strongconvexity) & \checkmark & \checkmark (linear) & \xmark & $\varepsilon^{-1}$ &  \\ 
    \cite{ye2023difference} & diff. of convex & \checkmark & \checkmark (set) & --- & (\checkmark, \checkmark) & --- & (\checkmark, \checkmark) & --- &  asymptotic & \\
    \cite{gao2024moreau} & diff. of weak & & \checkmark (set) & --- & (\checkmark\hspace{-0.5ex}\textsubscript{w}, \checkmark) & \checkmark & (\checkmark, \checkmark) & & asymptotic \\
    \cite{yao2024constrained} & & \checkmark & \checkmark & \checkmark & (, \checkmark) & \checkmark & ?? \\
    \cite{DMLCBO} & --- & \checkmark & \checkmark (set) & & (, \strongconvexity) & \checkmark & \checkmark (set) & --- & $d_2^2 \varepsilon^{-4}$ & {\thead{PO, \\ inv Hess}} \\ 
    \bottomrule
\end{NiceTabular}
  }
  \captionsetup{font=scriptsize}
  \caption{All known (to us) results for two-variable bilevel problems. We use $\strongconvexity$ and $\weakconvexity$ to denote strong and weak convexity, respectively. We use $C(x,y)$ to denote the convexity in the tuple $(x,y)$. In \cite{khanduri2023linearly}, there are only smoothness assumptions on the component functions and (weak/strong) convexity assumptions on the overall smoothed implicit function $F$ (\pswt{I think this is stronger than imposing assumptions on the individual component functions making up $F$}) for nonasymptotic rates. In \cite{DMLCBO}, the $d_2$ is the dimension of the variable $y$. \pswt{Note: this table is still work in progress, and some cells are simply copied from others (when copying the corresponding rows); feel free to correct any of the cells}}
  \label{tab:distinct-columns}
\end{table}


% \begin{table}[h]
%   \centering
%   \resizebox{\linewidth}{!}{%
%   \begin{tabular}{ccccccccccc}
%     \toprule
%     \multirow{2}{*}{\textbf{Paper}} & \multicolumn{2}{c}{\textbf{Upper Objective}} & \multicolumn{2}{c}{\textbf{Upper Constraint}} & \multicolumn{2}{c}{\textbf{Lower Objective}} & \multicolumn{2}{c}{\textbf{Lower Constraint}} & \multicolumn{2}{c}{\textbf{Cost}} \\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
%     & $C(x,y)$ & Smoothness & $C(x,y)$ & Smoothness & $C(x,y)$ & Smoothness & $C(x,y)$ & Smoothness & Iters. & Per Iter.\\
%     \midrule
%     \makecell{\cite{liu2021value}} & \makecell{Cell 1,2.1} & \makecell{Cell 1,2.2} & \makecell{Cell 1,2.3} & \makecell{Cell 1,3} & \makecell{Cell 1,4} & \makecell{Cell 1,5} & \makecell{None} \\
%     \makecell{\cite{kwon2023fully}} & \makecell{Cell 2,2.1} & \makecell{Cell 2,2.2} & \makecell{Cell 2,2.3} & \makecell{Cell 2,3} & \makecell{Cell 2,4} & \makecell{Cell 2,5} & & & \makecell{$\varepsilon^{-3}$} \\
%     \makecell{\cite{chen2023near}} & \makecell{Cell 2,2.1} & \makecell{Cell 2,2.2} & \makecell{Cell 2,2.3} & \makecell{Cell 2,3} & \makecell{Cell 2,4} & \makecell{Cell 2,5} & & &  \makecell{$\varepsilon^{-2}$} \\
%       \multirow{4}{*}{\makecell{\cite{khanduri2023linearly}}} & \makecell{---} & \makecell{(\checkmark, \checkmark)} & \makecell{\checkmark (set)} & \makecell{---} & \makecell{(, strong)} & \makecell{\checkmark} & \makecell{\checkmark (linear)} & \makecell{\xmark} & \makecell{asymptotic} & \makecell{---} \\
%       & \makecell{weak ($F$)} & & & & & & & & \makecell{$\varepsilon^{-2}$} & \makecell{$\geq$Hessian} \\ 
%             & \makecell{convex  ($F$)}  & & & & & & & & \makecell{$\varepsilon^{-2}$} & \makecell{$\geq$Hessian}\\ 
%       & \makecell{strongly  ($F$)}  & & & & & & & & \makecell{$\varepsilon^{-1}$} & \makecell{$\geq$Hessian} \\ 
%   \multirow{2}{*}{\makecell{\cite{abolfazli2023inexact}}} & \makecell{none} & \makecell{(\checkmark, \checkmark)} & \makecell{\checkmark (set)} & \makecell{---} & \makecell{(, strong)} & \makecell{\checkmark} & \makecell{---} & \makecell{---} & \makecell{$\varepsilon^{-2}$} & \makecell{mv} \\
% & \makecell{(\checkmark, \checkmark)} & & & & & & & &  \makecell{$\varepsilon^{-1}$} \\
%     \makecell{\cite{ye2023difference}} & \makecell{} & \makecell{\checkmark} & \makecell{\checkmark} & \makecell{\checkmark, \xmark} & \makecell{\checkmark} & \makecell{\checkmark} & \makecell{None} \\
%     \makecell{\cite{gao2024moreau}} & \makecell{diff. of weak} & \makecell{} & \makecell{\checkmark (set)} & \makecell{---} & \makecell{(weak, \checkmark)} & \makecell{\checkmark} & \makecell{(\checkmark, \checkmark)} & & \makecell{asymptotic} \\
%     \makecell{\cite{yao2024constrained}} & \makecell{} & \makecell{\checkmark} & \makecell{\checkmark} & \makecell{\checkmark} & \makecell{(, \checkmark)} & \makecell{\checkmark} & \makecell{??}  \\
%     \bottomrule
%   \end{tabular}
%   }
%   \caption{All Known (to us) Results for Two-Variable Bilevel Problems. We use $C(x,y)$ to denote the convexity in the tuple $(x,y)$. In \cite{khanduri2023linearly}, there are only smoothness assumptions on the component functions and (weak/strong) convexity assumptions on the overall smoothed implicit function $F$ for nonasymptotic rates. \pswt{Note: this is still work in progress, feel free to correct any of the cells}}
%   \label{tab:distinct-columns}
% \end{table}

\paragraph{Lagrangian Approximation of the Hyperobjective}
Interestingly, so far, this has been the only approach to achieving truly Hessian-free algorithms. \pswt{This (not-yet-fully verified) assertion is based on me currently not understanding how \cite{yao2024constrained} is claimed to be  "hessian-free".} This remarkable breakthrough was achieved by \cite{kwon2023fully} who reformulated the given problem into a Lagrangian form, and with some very simply --- but  clever --- observations, were able to perform hypergradient updates by using \emph{exclusively first-order components}! Their rate was $\varepsilon^{-3}$ iterations, each performing only gradient computations. \pswt{note: the true dependence on $\kappa$ and polylog factors isn't shown here; see \cite{chen2023near} for this.} In another remarkable paper, \cite{chen2023near} did a more careful analysis of the algorithm and technique in \cite{kwon2023fully} to get the optimal number of iterations $\varepsilon^{-2}$. At the heart of \cite{kwon2023fully}'s novelty is a first-order approximation of the hypergradient. \pswt{To extend this to our setting, can we get a similar first-order approximation of the \emph{projected} hypergradient?}

\cite{landry2019differentiable}: no complexity analysis. 

% \cite{kwon2023fully} ($\epsilon^{-3} \log {\frac{1}{\epsilon}}$) and \cite{chen2023near} ($\epsilon^{-2} \log {\frac{1}{\epsilon}}$)

\paragraph{Smoothing the Lower-Level Solution.}
The gradient of the solution $y^\ast(x)$ of the lower-level problem plays a central role in the hypergradient (i.e., gradient of the overall objective). However, one cannot in general assume $y^\ast(x)$ to be differentiable. Some papers impose assumptions on the lower-level objective (and constraint, if needed) so that they can obtain this differentiability for free; others (which we discuss in this subsection) achieve this property via some kind of smoothing. So far, we have observed two kinds of smoothing: one via inf-convolution (also called Moreau envelope) and the other via random perturbation (e.g., \cite{khanduri2023linearly} and \cite{DMLCBO}.). We discuss these approaches below. 

The work of \cite{liu2021value} studies $\min_{x\in \mathcal{X}, y^\ast(x)\in \arg\min f(x,y)\leq 0} F(x, y^\ast(x))$, assuming only continuous differentiability of $F$ and $f$. Conceptually, the paper performs a very simple algorithm: reformulate the problem as one requiring $y$ be in the set $f(x, y) - \min_y f(x, y)\leq 0$, then uses inf-convolution to smoothen $\min_y f(x, y)$. It then transforms the bilevel problem into a single-level one by incorporating the (smoothed) constraint into the objective via a \emph{barrier} function, as is done in interior-point methods. Finally, it uses inf-convolution again to smoothen the resulting objective function of $x$. The algorithm therefore iteratively performs the following sequence of steps: first, compute the minimizer of the smoothed lower-level constraint; second, find the minimizer (with respect to $y$) of the smoothed barrier objective; third, we take a gradient step (with respect to $x$) on the smooth barrier objective. This whole subroutine is then run in the IPM framework. The paper does not provide a non-asymptotic convergence guarantee, but their technique of smoothing via inf-convolution is used in many other papers, and in what follows, we describe some of the most relevant such works. 

First, we note the work of \cite{ye2023difference}, which performs a ``value function smoothing'' via inf-convolution, then notes that the resulting problem is a difference of convex functions. It then uses established techniques from the DC programming literature to solve this problem. The paper provides asymptotic convergence guarantees. 

The (very) recent work of \cite{gao2024moreau} extends the work of \cite{ye2023difference} by removing the convexity assumption on the second variable of the lower-level problem. To do so, the paper proves weak convexity of the Moreau envelope. \pswt{To check: \cite{davis2019stochastic} already proved that the Moreau envelope of any weakly convex function is smooth with Lipschitz continuous gradient; doesn't this already imply weak convexity of the Moreau envelope? Specifically, doesn't Lemma $2.2$ of \cite{davis2019stochastic} subsume Theorems $2$ and $3$ of \cite{gao2024moreau}?} The paper provides asymptotic guarantees (see Theorem $18$).

Another similarly relevant paper that also falls under the framework of ``value function smoothing'' is that of 
  \cite{yao2024constrained}, which studies \[\min_{x\in \mathcal{X}, y^\ast(x)\in \arg\min_{y\in \left\{\mathcal{Y}\cap g(x, y)\leq 0\right\}} f(x,y)} F(x, y^\ast(x)),\] which is essentially identical to our setup. The paper's first idea is to incorporate the lower-level constraint $g(x,y)\leq0$ into the lower-level objective via a Lagrange multipler they call $\lambda$ \pswt{(note that they seem to consider $\lambda$ to be a variable independent of $x$: is this valid?)} and then using the same inf-convolution as in \cite{liu2021value} to smoothen $\min_{y\in \mathcal{Y}} f(x,y) + \lambda g(x,y)$. One key innovation is to bound $\lambda$ in the optimization used in the smoothening process. \pswt{(Note: What is the justification for this bound? How can we ensure that we will not miss the correct $\lambda$ by imposing this bound?)} The algorithm then proceeds iteratively, performing the following sequence of steps: first, it computes the minimizer of these two inf-convolutions with the current $x$, $y$, and $z$; second, it updates $x$ and $y$; third it updates $z$. \pswt{The operation of updating $x$ and $y$ is performed via a projection onto the set $\left\{y\in \mathcal{Y}| g(x, y)\leq 0\right\}$. Would the final runtime not have to consider the cost of this operation? Can we ensure it is Hessian-free, as the title claims the paper to be?} Also, \pswt{This is the only paper that \emph{claims} what our goal is; so we would need to definitely write down their correct runtime in order for us to be able to compare with them}

  Another approach to smoothing is seen in the work of \cite{khanduri2023linearly}. The authors perform a slightly different kind of smoothing, by using random perturbation. Overall, their algorithm is as follows: randomly perturb the lower-level objective, then compute the $y^\ast(x)$ corresponding to this perturbed lower-level (linearly constrained) problem. The perturbation enables differentiability of $y^\ast(x)$ (note that the expression for $\nabla_x y^\ast(x)$ has the optimal lower-level Lagrangian in it; in this setting, because the lower-level constraint is linear, the authors are able to get a closed-form expression for it \pswt{Note: the authors seem to provide a very in-depth analysis of the conditions required for differentiability of $y^\ast(x)$; I wasn't able to see such in-depth proofs in other papers, all of which use the differentiability of $y^\ast(x)$}), which is then used in computing the hypergradient; the variable $x$ is then updated using this hypergradient. The authors provide nonasymptotic rates by imposing additional assumptions of weak convexity/convexity/strong convexity on the hyperobjective obtained by perturbed smoothing. The authors also show "closeness" of the perturbed smoothed hyperobjective from the true hyperobjective. \pswt{My main concern is that the authors do not seem to have proved \emph{under what conditions on the original problem} can one see this imposed assumption on the perturbed smoothed objective $F$. Without such a proof, wouldn't their theorem be essentially incomplete?} 

  In \cite{DMLCBO}, the authors consider bilevel optimization with both upper and lower level constraints, the constraints given as explicitly defined convex sets. The authors use a closed-form formula for the hypergradient citing a result from \cite{blondel2022efficient}. Note: \pswt{This expression requires the computation of the inverse of a product of a matrix with a Gaussian-smoothed projection operator}. Instead of using the true projection operator, they use a Gaussian-smoothed version of it, thus incurring an extra dimension factor in the smoothness constant. The convergence metric used is like a gradient mapping of the projection.\pswt{Is their proof of Lemma 6 correct?}    
 
\paragraph{Projection-Free Approaches}
There has been some attempt at developing projection-free algorithms for various versions of bilevel optimization. Below, describe as many approaches as possible. 

In \cite{abolfazli2023inexact}, the authors consider $\min_{x\in \mathcal{X}, y^\ast(x)\in \arg\min_y g(x,y).} f(x, y^\ast(x))$. Note that the constraint here is in the \emph{upper-level} problem, unlike what we are considering. They assume the lower-level objective to be strongly convex and smooth; they assume the upper-level objective to be smooth, providing results for both convex and nonconvex assumptions on $f$. 
% \pswt{Note I found their assumptions somewhat difficult to parse (I believe they are all fine, just a bit technically dense for me)}. 
Their algorithm is very natural: They compute the hypergradient, followed by updating $x_k$ via Frank-Wolfe (to ensure feasibility in $\mathcal{X}$), followed by updating $y_k$ (which approximates $y^\ast(x_k)$) via a gradient step on $g$. While the naive approach to the first step (taking a gradient step on the hyperobjective) would require an expensive matrix inversion, the authors come up with a simple new idea (presented in their equations $7a$ and $7b$) that essentially changes the order of operations in the naive approach; this circumvents the expensive operations from the naive approach. We note two important points here: First: \pswt{Their equations $7a$ and $7b$, which are two slightly new steps show up in Proposition $1$ of \cite{ji2021bilevel} as well.} Second: \pswt{Given the overall nonconvexity, wouldn't convergence heavily rely on very accurate initialization, and wouldn't the stationary guarantee be weak-ish? I'm also very confused about the fact that Lemma $2.1$'s ``Lipschitz/smoothness constants" look really bad (i.e., square of some condition number), but somehow those bad constants disappear in the final iteration cost (e.g., see the proof of Corollary $4.2$) and are omitted in the results table.} 