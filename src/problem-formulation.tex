\section{Preliminaries}\label{sec:preliminaries}
% We set our notation and review some useful facts from convex analysis before providing formal details of our problem. 
Throughout, we use $\langle \cdot{}, {}\cdot\rangle$ to denote inner products and $\|{}\cdot{}\|$ to denote the Euclidean norm, and unless transposed, all vectors are column vectors. For $f:\reals^{d_2}\to\reals^{d_1}$ its Jacobian with respect to $x\in \reals^{d_2}$ is 
% \pswt{this is dimensionally fine, but notationally horrible}
$\nabla_x f\in \reals^{d_1 \times d_2}$.  For $f:\reals^d\to\reals$, we overload notation and term $\nabla_x f$ its gradient (the transposed Jacobian), a column vector. \pswt{notation for  partial derivatives, higher order derivatives, etc.}

A function $f:\reals^n\to\reals^m$ is $L$-Lipschitz if for any $x,y$, we have $\|f(x) - f(y)\|\leq L \|x-y\|$.
A differentiable function $f:\reals^n\to\reals$ is convex if for any $x, y\in \reals^n$ we have $f(y)\geq f(x) + \nabla f(x)^\top (y-x)$; 
% A differentiable function $f:\reals^n\mapsto\reals$
it is 
 $\mu$-strongly convex
 if $f - \tfrac{\mu}{2}\|{}\cdot{}\|^2$ is convex;
% w.r.t. the $\ell_2$-norm
% if for any $x, y \in \reals^n$, we  have  $f(y) \geq f(x) + \nabla f(x)^\top (y-x) + \tfrac{\mu}{2}\|y-x\|^2$; 
it is  $\beta$-smooth
% w.r.t. the $\ell_2$-norm
if
$\nabla f$ is $\beta$-Lipschitz.
% $0.5\beta\|{}\cdot{}\|^2- f$ is convex. 
% \jz{maybe we can say something like $\norm{\grad f(y)-\grad f(x)}\leq \beta \norm{y-x} \ \forall y,x$. The stated one is equivalent, but less frequently used in the literature.}
% for any $x, y \in \reals^n$, we  have  $f(y) \leq f(x) + \nabla f(x)^\top (y-x) + \tfrac{\beta}{2}\|y-x\|^2$. 


\begin{definition}\label{def:GoldsteinDeltaEpsStationary}
 Consider a locally Lipschitz function $f:\reals^d\to\reals$, a point $x\in\reals^d$, and a parameter $\delta>0$. The {Goldstein subdifferential}~\cite{goldstein1977optimization} of $f$ at $x$ is the set
 $\partial_{\delta} f(x):=\mathrm{conv} (\cup_{y\in \mathbb{B}_{\delta}(x)}\partial f(y)),$ where  $\partial f(x)=\mathrm{conv}\left\{\lim_{n\to \infty} \nabla f(x_n): x_n\to x, ~x_n\in \mathrm{dom}(\nabla f)\right\}$ is the Clarke subdifferential~\cite{clarke1981generalized} of $f$ and $\mathbb B_\delta(x)$ denotes the Euclidean ball of radius $\delta$ around $x$.
 A point $x$ is called $(\delta, \epsilon)$-stationary if $\mathrm{dist}(0,\partial_{\delta}f(x))\leq \epsilon$, where $\mathrm{dist}(x,S):=\inf_{y\in S}\|x-y\|$.
 % for $x\in \reals^d$ and $S \subseteq \reals^d$ is $\inf_{y\in S}\|x-y\|$.

\end{definition}
%
Thus, for a Lipschitz function $f$, a point $x$ is $(\delta, \epsilon)$-stationary if within a $\delta$-ball around $x$, there exists a convex combination of  subgradients of $f$ with norm at most $\epsilon$. 
% Thus, the Goldstein subdifferential of $f$ at $x$ is the convex hull of all Clarke subgradients at points in a  $\delta$-ball around $x$. 
For a differentiable function $f$, we say that $x$ is $\epsilon$-stationary if $\|\nabla f(x)\|\leq \epsilon$. 

\subsection{Assumptions}\label{sec:differentiable-optimization}
We consider \cref{{prob:general-constraints}} with convex inequality constraints (\cref{sec:inequality-bilevel} and \cref{sec:nonsmooth}) under \cref{assumption:linEq_smoothness,item:assumption_safe_constraints}, and with linear equality constraints (\cref{sec:equality-bilevel}) under \cref{assumption:linEq_smoothness,assumption:eq}. 
We have $x\in \reals^{d_x}$ for the upper-level (UL) variable and $y\in \reals^{d_y}$ for the lower-level (LL) variable. We assume that $h:\reals^{d_x} \times \reals^{d_y}\to \reals^{d_h}$ is an element-wise constraint function with $d_h$ constraints. % \pswt{add dims of $h$} \kai{added. To check consistency.}. 
% We have the upper-level (UL) objective function $f:\reals^{d_x}\times\reals^{d_y}\to\reals$ of the UL variable $x\in\reals^{d_x}$ and  $y^*(x)\in\reals^{d_y}$, the minima of a possibly constrained lower-level (LL) objective $g:\reals^{d_x}\times\reals^{d_y}\to\reals$.
% \gk{dimension as above?}. 

\iffalse 
following bilevel optimization problem with constrained lower level problem:
\begin{align}
    \min\nolimits_{x} \quad & F(x):= f(x, y^\ast(x))  &
     \text{s.t.} \quad & y^\ast(x) \in \arg\min\nolimits_{ x,y \text{ feasible}} g(x,y) \label{prob:orig_bilevel}
\end{align}
We consider the case where the lower objective is strongly convex and smooth, and the constraints to be equality constraints ($h(x,y) = 0$) in Section~\ref{sec:equality-bilevel}, and inequalty constrains ($h(x,y) \leq 0$) in Section~\ref{sec:inequality-bilevel} and Section~\ref{sec:nonsmooth}, where $h$ is a vector value function in both cases.

The upper objective just needs to be smooth and Lipschitz in $y$. 

Our assumptions on the objective functions are summarized in Assumption~\ref{assumption:linEq_smoothness} and Assumption~\ref{assumption:smoothness} (additionally for inequality constraints).
\fi 

\begin{assumption}\label{assumption:linEq_smoothness} 
For \cref{prob:general-constraints}, we assume the following for both settings we study: 
% \gk{Having swapped the order of results, should we swap the order of assumptions (for clarity)?}\pswt{(5/13)Yes, Jimmy and I discussed again, and it looks like having equality going \emph{after} inequality makes most sense; so yes, we can present the assumptions in whatever order makes most sense for this!} \kai{I think my plan was Assumption 2.2 is for both ineq and eq, while Assumption 2.3 is for ineq only. Swapping the assumption order may not work in this way?}
\begin{assumpenum}
\compresslist{
\item\label[assum]{item:assumption_upper_level} Upper-level: The objective $f$ is $C_f$-smooth and $L_f$-Lipschitz continuous in $(x,y)$. 
%\jz{Lipschitz on $(x,y)$ implies Lipschitz on $x$ while fixing $y$.}%in $x$, $y$, and $x,y$ jointly
% , i.e., $\norm{\nabla f(x_1,y_1) - \nabla f(x_2,y_2)} \! \leq C_f \norm{(x_1,y_1) - (x_2, y_2)}$ %\pswt{removing this since we already just defined smoothness}
%and $L_f$-Lipschitz in $x$ and $y$. % The constraint $\mathcal{X}$ is a convex compact set. 
    \item \label[assum]{item:assumption_lower_level}Lower-level: The objective $g$ is $C_g$-smooth and the constraint $h$ is $C_h$-smooth in $(x,y)$. Fixing any $x\in\mathcal{X}$, $g(x,\cdot)$ is $\mu_g$-strongly convex and the constraint $h(x,\cdot)$ is convex in $y$.

    \item \label[assum]{item:assumption_tangen_space} LICQ: We assume the LICQ condition holds for the LL problem for every $x$ and $y$, that is,
    the derivative $\nabla_y h(x,y)$ has full row rank on active constraints. In the linear equality case, this corresponds to the constraint $h(x,y)=Ax-By-b$ having a full row rank $B$.
    
    %\item \label[assum]{item:assumption_tangen_space} Tangent space: the derivative $\nabla_y h(x,y)$ has full row rank on active constraints, which in the linear case it corresponds to the constraint $h(x,y)=Ax-By-b$ has  $B$ being full row rank.
    %\jz{Maybe we could justify the full rankness by complete recourse. That is, for any given $x$, there exists some $y$ to make the LL program feasible. I am wondering if we need that assumption for the inequality case? }
}
\end{assumpenum}
\end{assumption}


\begin{assumption}\label{item:assumption_safe_constraints} 
% \pswt{We seem to also use a bound on $\nabla^2_{xy}f$ in the proof of \cref{thm:diff_in_hypergrad_and_gradLagr}; is the constant for that included in the current assumption?}
For \cref{prob:ineq} (with convex inequality constraints), we additionally assume $y^*(x)$ is $L_y$-Lipschitz in $x$ and $\gamma(x)$ is  bounded: $\norm{\gamma(x)} \leq R$, where for a given $x$, we denote the LL primal and dual solution $y^*(x), \gamma(x) = \arg\max\nolimits_\theta \min\nolimits_{\beta \geq 0} g(x,y) + \beta^\top h(x,y)$. % \gk{undefined at this point.} 
    % We assume  . 
    %\jz{maybe we should point out that this implies some full recourse assumption, that is, for every $x$ there exists some $y$ to make the LL problem feasible.} 
% \begin{assumpenum} 
% \compresslist{
%     % \item Upper-level: The objective $f$ is $L_f$-Lipschitz in $y$ and $C_f$-smooth in $x,y$ jointly, i.e., $\norm{\nabla f(x_1,y_1) - \nabla f(x_2,y_2)} \! \leq \! C_f \! \norm{(x_1,y_1) - (x_2, y_2)} $. 
%     % \item Lower-level: The objective $g$ is $C_g$-smooth and $\mu_g$-strongly convex in $y$. The constraint $h$ is convex in $y$ and $C_h$-smooth. 
%     \item\label[assum]{item:assumption_safe_constraints} Primal and dual optimizers: Given $x$, denote the LL primal and dual solution $y^*(x), \gamma(x) = \arg\max\nolimits_\theta \min\nolimits_{\beta \geq 0} g(x,y) + \beta h(x,y)$. % \gk{undefined at this point.} 
%     We assume  $y^*(x)$ is $L_y$-Lipschitz in $x$, and that the dual solution $\gamma(x)$ is  bounded: $\norm{\gamma(x)} \leq R$. 
%     \jz{maybe we should point out that this implies some full recourse assumption, that is, for every $x$ there exists some $y$ to make the LL problem feasible.}
% }   
% \end{assumpenum}
\end{assumption}

\begin{assumption}\label{assumption:eq}For \cref{prob:lin-eq} (with linear equality constraints), we additionally assume that the set $\mathcal{X}$ is convex and compact, and that the objective $g$ is $S_g$-Hessian smooth, that is, $\norm{\grad^2 g(x,y) -\grad^2 g(\bar x,\bar y)}\leq S_g \norm{(x,y) - (\bar x, \bar y)} \forall x,\bar x \in \mathcal{X}, \text{ and } y, \bar y \in \R^{d_y}.$
% \begin{assumpenum}
% \compresslist{
%     \item  
% }
% \end{assumpenum}
\end{assumption}

\looseness=-1\cref{item:assumption_upper_level,item:assumption_lower_level} correspond to standard regularity assumptions in bilevel optimization. 
\cref{{item:assumption_tangen_space}} that  $B$ has full rank is the same as the complete recourse assumption in stochastic programming \cite{shapiro2021lectures}, that is, the LL problem is feasible $y$ for every $x\in\R^{d_x}$.
\cref{item:assumption_safe_constraints} is  needed only for the convex inequality case 
and ensures Lipschitz-continuity of the hyperobjective $F$. 
% to bound the non-smooth change of the LL problem solutions and their impact to the Lipschitzness of hyperobjective objective, which is later used to bound the convergence of bilevel algorithms.
\cref{assumption:eq} is used only in the equality case and guarantees smoothness of $F$.

\iffalse
Specifically, we also assume that the optimal solution $y^*(x)$ to be Lipschitz in $x$, and the dual solution is upper bounded by a constant $R$. 
\begin{assumption}[Bilevel solution assumptions]\label{assumption:safe-constraints}
    Given $x$, the lower level problem primal solution $y^*$, and the dual solution $\gamma(x)$, we assume
    \begin{itemize}
        \item $y^*(x)$ is $L_y$-Lipschitz in $x$. 
        \item The dual solution $\gamma(x)$ is upper bounded: $\norm{\gamma(x)} \leq R$.
        % This implies $\norm{M^{-1} \begin{bmatrix}
        %     \nabla^2_{xy} g + \gamma^\top \nabla_{xy}^2 h \\
        % \text{diag}(\gamma) \nabla_x h
        % \end{bmatrix}} \leq C_M$ with a constant $C > 0$, where $M = \begin{bmatrix}
        % \nabla^2_{yy} g + \gamma \nabla_{yy}^2 h & \nabla_y h_S^\top \\
        % \text{diag}\gamma \nabla_y h_S & 0
        % \end{bmatrix}$ as defined in Section~\ref{sec:differentiable-optimization}.
    \end{itemize}
\end{assumption}
\fi 