\section{Proofs for \cref{sec:nonsmooth}} \label{sec:alg_proof}
Our algorithms are based on the Lipschitzness of $F$, which we prove below. 
\lemLipscConstrBilevel*
\begin{proof}
% \pswt{check!} \gk{Defer proof to \cref{sec:alg_proof}}
    By Lemma $2.1$ of \cite{ghadimi2018approximation}, the hypergradient of $F$ computed with respect to the variable $x$ may be expressed as $\nabla_x F(x) = \nabla_x f(x, y^\ast(x)) + \left(\frac{d y^\ast(x)}{dx}\right)^\top \cdot \nabla_y f(x, y^\ast(x))$. Since we impose Lipschitzness on $f$ and $y^*$, we can bound each of the terms of $\nabla_x F(x)$ by the claimed bound. 
    % \gk{Note that instead of citing, we already mention this in Section 2 so can just ref}. If we impose Lipschitzness assumptions on $f$, with respect to each of the coordinates, and additionally, if we can show $\|\nabla_x y^\ast(x)\|\leq L_{yx}$ for some constant, then it concludes the proof of Lipschitzness of $F$ with a Lipschitz constant $L_F \leq L_{fx} + L_{fy}\cdot L_{yx}$. Note that a bound on $\|\nabla_x y^\ast(x)\|$ was obtained by \cite{ghadimi2018approximation} in terms of second-order properties of $g$ and one in terms of constants from weaker assumptions in \cite{kwon2023fully} --- however, the latter also involves the Lagrange multiplier used therein. 
    % \pswt{How can we get for $y^\ast(x)$ a Lipschitz constant that is independent of any Lagrange multipliers? I think this is perhaps a self-contained question: what is the Lipschitz constant of the minimizer of a (strongly) convex function over the $0$-level set of a (strongly) convex function? Note: Jimmy has a solution for this.} 
\end{proof}


\subsection{Faster algorithm for low upper-level dimensions}\label{sec:zeroth-order-algs}
% Notably, \cref{alg: OIGRM} requires hypergradient estimates, which contributes to the overall complexity of the optimization scheme. On the other hand, estimating the \emph{value} of $F(x)$ at any given point $x$, can be done using $\widetilde{O}(1)$ oracle calls, as it amounts to solving a single strongly convex optimization problem. This is formally stated in the following lemma. \pswt{added this text to the main body}
In this section we analyze \cref{alg: IZO}, which as stated in \cref{{sec:nonsmooth}}, requires evaluating only the hyperobjective $F$ (as opposed to estimating the hypergradient in \cref{alg: OIGRM}).

The motivation for designing such an algorithm, is that
while evaluating $\nabla F$ up to $\alpha$ accuracy requires $O(\alpha^{-1})$ gradient evaluations, the hyperobjective value can be estimated at a linear rate:

\lemZeroOrderApprox*
\begin{proof}[Proof of \cref{lem:ZeroOrderApprox}]
    We note that it suffices to find $\tilde{y}^*$ such that $\norm{\tilde{y}^*-y^*(x)}\leq \alpha/L_f$, since setting $\tF(x):=f(x,\tilde{y}^*)$ will then satisfy $|\tF(x)-F(x)|=|f(x,\tilde{y}^*)-f(x,y^*(x))|\leq L_f\cdot \tfrac{\alpha}{L_f}=\alpha$ by Lispchitzness of $f$, as required. Noting that $y^*(x)=\arg\min_{h(x,y)\leq 0}g(x,y)$ is the solution to a constrained smooth, strongly-convex problem with condition number $C_g/\mu_g$, it is possible to approximate it up to $\alpha/L_f$ with $O(\sqrt{C_g/\mu_g}\log(L_f/\alpha))$ first-order oracle calls using the result of \citet{zhang2022solving}.
\end{proof}


Accordingly, we consider \cref{alg: IZO}, which is a zero-order variant of \cref{alg: OIGRM}, whose guarantee is summarized is the theorem below.



\begin{theorem} \label{thm: Lipschitz-min-with-inexact-zero-oracle}
Suppose $F:\reals^d\to\reals$ is $L$-Lipschitz, 
% $F(x_0)-\inf F\leq \Delta$
and that $|\widetilde{F}(\cdot)-F(\cdot)|\leq\alpha$.
Then running \cref{alg: OIGRM} with
$\rho=\min\left\{\tfrac{\delta}{2},\tfrac{F(x_0)-\inf F}{L}\right\},\nu=\delta-\rho,~D=\Theta\left(\frac{\nu\epsilon^2\rho^2}{d\rho^2 L^2+\alpha^2 d^2}\right),\eta=\Theta\left(\frac{\nu\epsilon^3\rho^4}{(d\rho^2L^2+\alpha^2d^2)^2}\right)$,
outputs a point $x^{\out}$ such that $\E[\mathrm{dist}(0,{\partial}_\delta F(x^{\out}))]\leq\epsilon+\alpha$ with
\[T=
O\left(\frac{d(F(x_0)-\inf F)}{\delta\epsilon^3}\cdot \left(L^2+\alpha^2 (\frac{d}{\delta^{2}}+\frac{dL^2}{(F(x_0)-\inf F)^2})\right)\right)
\text{ calls to } \tF(\cdot).\]
% Under the same setting as \citep[Theorem 7.2]{chen2023bilevel},
% suppose that $\mathrm{SGM}$ (as in \citep[Theorem 7.1]{chen2023bilevel}) is set so that $|\tF(\cdot)-\varphi(\cdot)|\leq \zeta$ for $\zeta=\Theta(\delta\epsilon/d)$. Then
% Algorithm~\ref{alg: IZO} outputs a point $\bx^{\out}$ such that $\E[\min\{\norm{s}:s\in\partial_\delta \varphi(\bx^{\out})\}]\leq\epsilon$ with
% \[
% T=O\left(\frac{d}{\delta\epsilon^3}\right)~.
% \]
\end{theorem}


% We now .
% to \cref{sec:alg_proof}.
Combining the result of \cref{thm: Lipschitz-min-with-inexact-zero-oracle} with the complexity of hyperobjective estimation,
as given by \cref{lem:ZeroOrderApprox},
we obtain convergence to a $(\delta,\epsilon)$-stationary point of  \cref{{prob:ineq}} with $\widetilde{O}(d_x\delta^{-1}\epsilon^{-3})$ gradient calls overall.
% \gk{remark about UL dimension being small in some applications of interest?}


% \pswt{the text before this comment is copied straight from what was previously in the main body and must therefore be appropriately edited for coherence}


\subsubsection{Proof of \cref{thm: Lipschitz-min-with-inexact-zero-oracle}}


Denoting the uniform randomized smoothing $F_\rho(x):=\E_{\norm{z}\leq1}[F(x+\rho\cdot z)]$ where the expectation, here and in what follows, is taken with respect to the uniform measure, it is well known \citep[Lemma 10]{shamir2017optimal} that
\begin{align}
\E_{\norm{w}=1}\left[\tfrac{d}{2\rho}(F(x+\rho w)-F(x-\rho w)) w\right]
&=\nabla F_\rho( x)~,
\nonumber
\\
\E_{ \norm{w}=1}\norm{\nabla F_\rho( x)-\tfrac{d}{2\rho}( F( x+\rho w)- F( x-\rho w)) w}^2
&\lesssim dL^2~.\label{eq:grad_var_bound}
\end{align}
We first show that replacing the gradient estimator with the inexact evaluations $\tF(\cdot)$ leads to a biased gradient estimator of $ F$.

\begin{lemma}\label{lem: inexact gradient}
Suppose $| F(\cdot)-\tF(\cdot)|\leq\alpha$. Denoting
\begin{align*}
 g_x&=\tfrac{d}{2\rho}( F( x+\rho w)- F( x-\rho w)) w~,
\\
\tbg_x&=\tfrac{d}{2\rho}(\tF(x+\rho w)-\tF( x-\rho w)) w~,
\end{align*}
it holds that
\[
\E_{ \norm{w}=1}\norm{ g_x-\tbg_x}\leq\frac{\alpha d}{\rho}~,
~~~~~\text{and}~~~~~
\E_{\norm{w}=1}\norm{\tbg_x}^2\lesssim \frac{\alpha^2 d^2}{\rho^2}+dL^2~.
\]
\end{lemma}

\begin{proof}
For the first bound, we have
\[
\E_{\norm{w}=1}\norm{g_x-\tbg_x}
\leq \frac{d}{2\rho}(2\alpha)\E_{\norm{w}=1}\norm{w}
=\frac{\alpha d}{\rho}~,
\]
while for the second bound
\[
\E_{\norm{w}=1}\norm{\tbg_x}^2
=\E_{\norm{w}=1}\norm{\tbg_x-g_x+g_x}^2
\leq 2\E_{\norm{w}=1}\norm{\tbg_x-g_x}^2+2\E_{\norm{w}=1}\norm{g_x}^2
\lesssim \frac{d^2}{\rho^2}\cdot\alpha^2+dL^2~,
\]
where the last step invoked \cref{eq:grad_var_bound}.
\end{proof}

% The next step would be to prove that online gradient descent minimizes the regret with respect to inexact evaluations. Recalling definitions from online learning, given a sequence of linear losses $\ell_t(\cdot)=\inner{\bg_t,\cdot}$, if the algorithm chooses $\bm{\Delta}_1,\dots,\bm{\Delta}_T$ we denote the regret with respect to $\bu$ as $\mathrm{Reg}_T(\bu):=\sum_{t=1}^{T}\inner{\bg_t,\bm{\Delta}_t-\bu}$. Consider an update rule according to \emph{inexact} online projected gradient descent: $\bDelta_{t+1}:=\mathrm{clip}_{D}(\bDelta_{t}-\eta_t\tbg_t)$.

% \begin{lemma}\label{lem: inexact OGD}
% In the setting above, suppose that $\norm{\tbg_t-\bg_t}\leq\alpha$ and $\norm{\tbg_t}^2\leq\widetilde{G}^2$ for all $t\in[T]$.
% Then for any $\norm{\bu}\leq D$ it holds that
% \[
% \mathrm{Reg}_T(\bu)\leq \frac{D^2}{\eta_T}+\widetilde{G}^2\sum_{t=1}^{T}\eta_t+ \alpha DT~.
% \]
% In particular, setting $\eta_t\equiv\frac{D}{\widetilde{G}\sqrt{T}}$ yields
% \[
% \mathrm{Reg}_T(\bu)\lesssim D\widetilde{G}\sqrt{T}+\alpha DT~.
% \]
% \end{lemma}

% \begin{proof}
% For any $t\in[T]:$
% \begin{align*}
% \norm{\bDelta_{t+1}-\bu}^2
% &=\norm{\mathrm{clip}_{D}(\bDelta_{t}-\eta_t\tbg_t)-\bu}^2
% \\&\leq \norm{\bDelta_{t}-\eta_t\tbg_t-\bu}^2
% =\norm{\bDelta_{t}-\bu}^2+\eta_t^2\norm{\tbg_t}^2-2\eta_t\inner{\bDelta_{t}-\bu,\tbg_t}~,
% \end{align*}
% thus
% \[
% \inner{\tbg_t,\bDelta_t-\bu}
% \leq \frac{\norm{\bDelta_t-\bu}^2-\norm{\bDelta_{t+1}-\bu}^2}{2\eta_t}+\frac{\eta_t}{2}\norm{\tbg_t}^2~,
% \]
% from which we get
% \begin{align*}
% \inner{\bg_t,\bDelta_t-\bu}
% &=\inner{\tbg_t,\bDelta_t-\bu}+
% \inner{\bg_t-\tbg_t,\bDelta_t-\bu}
% \\&\leq\frac{\norm{\bDelta_t-\bu}^2-\norm{\bDelta_{t+1}-\bu}^2}{2\eta_t}+\frac{\eta_t}{2}\widetilde{G}^2+\alpha D~.
% \end{align*}
% Summing over $t\in[T]$, we see that
% \begin{align*}
% \mathrm{Reg}_T(\bu)
% &\leq \sum_{t=1}^{T}\norm{\bDelta_t-\bu}^2\left(\frac{1}{\eta_t}-\frac{1}{\eta_{t-1}}\right)+\frac{\widetilde{G}^2}{2}\sum_{t=1}^{T}\eta_t+T\alpha D
% \\
% &\leq \frac{D^2}{\eta_T}+\widetilde{G}^2\sum_{t=1}^{T}\eta_t+ \alpha DT~.
% \end{align*}
% The simplification for $\eta_t\equiv\frac{D}{\widetilde{G}\sqrt{T}}$ readily follows.

% \end{proof}


We are now ready to analyze \cref{alg: IZO}.
% \begin{proof}[Proof of \cref{{thm: Lipschitz-min-with-inexact-zero-oracle}}]
We denote $\alpha'=\frac{\alpha d}{\rho},~\widetilde{G}=\sqrt{\frac{\alpha^2 d^2}{\rho^2}+dL^2}$. Since $x_t=x_{t-1}+\Delta_{t}$, we have
\begin{align*}
 F_\rho(x_t)- F_\rho(x_{t-1})
&=\int_{0}^{1}\inner{\nabla F_{\rho}(x_{t-1}+s\Delta_t),\Delta_t}ds
\\
&=\E_{s_t\sim\Unif[0,1]}\left[\nabla F_{\rho}(x_{t-1}+s_t{\Delta}_t),\Delta_t\right]
\\&=\E\left[\inner{\nabla F_{\rho}(z_{t}),\Delta_t}\right]~.
% \\&=\E\left[\inner{\tbg_{t},\bDelta_t}\right]+\E\left[\inner{\nabla F_{\delta}(\bz_{t})-\tbg_{t},\bDelta_t}\right]
% \\&\leq\E\left[\inner{\tbg_{t},\bDelta_t}\right]+\E\left[\norm{\nabla F_{\delta}(\bz_{t})-\tbg_{t}}\cdot\norm{\bDelta_t}\right]
% \\& \leq\E\left[\inner{\tbg_{t},\bDelta_t}\right]+D\alpha~.
\end{align*}
% where the last inequality follows from Lemma~\ref{lem: inexact gradient}.
By summing over $t\in[T]=[K\times M]$, we get for any fixed sequence $u_1,\dots,u_K\in\reals^d:$
\begin{align*}
    \inf  F_\rho\leq  F_\rho(x_T)
    &\leq  F_\rho(x_0)+\sum_{t=1}^{T}\E\left[\inner{\nabla F_{\rho}(z_{t}),\Delta_t}\right]
    \\&= F_\rho(x_0)+\sum_{k=1}^{K}\sum_{m=1}^{M}\E\left[\inner{\nabla F_{\rho}(z_{(k-1)M+m}),\Delta_{(k-1)M+m}-u_k}\right]\\
    &~~~~+\sum_{k=1}^{K}\sum_{m=1}^{M}\E\left[\inner{\nabla F_{\rho}(z_{(k-1)M+m}),u_k}\right]
    \\
    &\leq  F_\rho(x_0)+\sum_{k=1}^{K}\mathrm{Reg}_M(u_k)+\sum_{k=1}^{K}\sum_{m=1}^{M}\E\left[\inner{\nabla F_{\rho}(z_{(k-1)M+m}),u_k}\right]
    \\
    &\leq F_\rho(x_0)+KD\widetilde{G}\sqrt{M}+K\alpha'DM+\sum_{k=1}^{K}\sum_{m=1}^{M}\E\left[\inner{\nabla F_{\rho}(z_{(k-1)M+m}),u_k}\right]
\end{align*}
where the last inequality follows by combining \cref{lem: inexact gradient} and \cref{lem: inexact OGD}.
By setting $u_k:=-D\frac{\sum_{m=1}^{M}\nabla F_{\rho}(z_{(k-1)M+m})}{\norm{\sum_{m=1}^{M}\nabla F_{\rho}(z_{(k-1)M+m})}}$, rearranging and dividing by $DT=DKM$ we obtain
\begin{align}
\frac{1}{K}\sum_{k=1}^{K}\E\norm{\frac{1}{M}\sum_{m=1}^{M}\nabla F_{\rho}(z_{(k-1)M+m})}
&\leq \frac{ F_\rho(x_0)-\inf F_\rho}{DT}+\frac{\widetilde{G}}{\sqrt{M}}+\alpha'
\nonumber\\
&=\frac{ F_\rho(x_0)-\inf F_\rho}{K\nu}+\frac{\sqrt{\frac{\alpha^2 d^2}{\rho^2}+L^2d}}{\sqrt{M}}+\frac{\alpha d}{\rho}  
\nonumber\\
&\leq
\frac{ F_\rho(x_0)-\inf F_\rho}{K\nu}
+\frac{{\frac{\alpha d}{\rho}}}{\sqrt{M}}
+\frac{L\sqrt{d}}{\sqrt{M}}
+\frac{\alpha d}{\rho}~. \label{eq: bound eps}
\end{align}
Finally, note that for all $m\in[M]:\norm{z_{(k-1)M+m}-\overline{x}_{k}}\leq M D\leq\nu$, therefore 
$\nabla F_{\rho}(z_{(k-1)M+m})\in\partial_\nu F_\rho(\overline{x}_{k})\subset \partial_{\delta}F(\overline{x}_{k})$, where the last containment is due to \citep[Lemma 4]{kornowski2024algorithm} by using our assignment $\rho+\nu= \delta$.
Invoking the convexity of the Goldstein subdifferential, this implies that
\[
\frac{1}{M}\sum_{m=1}^{M}\nabla F_{\rho}(z_{(k-1)M+m})
\in\partial_{\delta}F(\overline{x}_{k})
~,
\]
thus it suffices to bound the first three summands in \eqref{eq: bound eps} by $\epsilon$ in order to finish the proof.
This happens as long as $
\frac{ F_\rho(x_0)-\inf F_\rho}{K\nu}\leq\frac{\epsilon}{3}$, $\frac{{\frac{\alpha d}{\rho}}}{\sqrt{M}}\leq\frac{\epsilon}{3}$, and $\frac{L\sqrt{d}}{\sqrt{M}}\leq\frac{\epsilon}{3}$, which imply $K\gtrsim \frac{ F_\rho(x_0)-\inf F_\rho}{\nu\epsilon}$, $ M\gtrsim \frac{\alpha^2 d^2}{\rho^2\epsilon^2}$, and $M\gtrsim \frac{L^2 d}{\epsilon^2}~$. 
By our assignments of $\rho$ and $\nu$, these  result in
\begin{align*}
T=KM
&=O\left(\frac{ F_\rho(x_0)-\inf F_\rho}{\nu\epsilon}\cdot \left(\frac{\alpha^2 d^2}{\rho^2\epsilon^2}+\frac{L^2 d}{\epsilon^2}\right)\right)
\\
&=O\left(\frac{(F(x_0)-\inf F)d}{\delta\epsilon^3}\cdot \left(\frac{\alpha^2 d}{\rho^2}+L^2\right)\right)
\\
&=O\left(\frac{(F(x_0)-\inf F)d}{\delta\epsilon^3}\cdot \left(\alpha^2 d\cdot\max\left\{\frac{1}{\delta^2},\frac{L^2}{(F(x_0)-\inf F)^2}\right\}+L^2\right)\right)
~,
\end{align*}
completing the proof.

% \end{proof}








\subsection{Proof of \cref{thm:Lipschitz-min-with-inexact-grad-oracle}}

% Denoting the uniform randomized smoothing $\varphi_\delta(x):=\E_{z\sim\B^d}[\varphi(x+\delta\cdot z)]$, it is well known \citep[Lemma 10]{shamir2017optimal} that
% \begin{align*}
% \E_{\bw\sim\mathbb{S}^{d-1}}\left[\tfrac{d}{2\delta}(\varphi(\bx+\delta\bw)-\varphi(\bx-\delta\bw))\bw\right]
% &=\nabla\varphi_\delta(\bx)~,
% \\
% \E_{\bw\sim\mathbb{S}^{d-1}}\norm{\nabla\varphi_\delta(\bx)-\tfrac{d}{2\delta}(\varphi(\bx+\delta\bw)-\varphi(\bx-\delta\bw))\bw}^2
% &\lesssim dC_\varphi^2~.
% \end{align*}
% We first show that replacing the gradient estimator with the inexact evaluations $\tF(\cdot)$ leads to a biased gradient estimator of $\varphi$.

% \begin{lemma}[Inexact gradient oracle] \label{lem: inexact gradient}
% Suppose $|\varphi(\cdot)-\tF(\cdot)|\leq\zeta$. Denoting
% \begin{align*}
% \bg_\bx&=\tfrac{d}{2\delta}(\varphi(\bx+\delta\bw)-\varphi(\bx-\delta\bw))\bw
% \\
% \tbg_\bx&=\tfrac{d}{2\delta}(\tF(\bx+\delta\bw)-\tF(\bx-\delta\bw))\bw
% \end{align*}
% it holds that
% \[
% \E_{\bw\sim\S^{d-1}}\norm{\bg_\bx-\tbg_\bx}\leq\frac{\zeta d}{\delta}~,
% ~~~~
% \E_{\bw\sim\S^{d-1}}\norm{\tbg_\bx}^2\lesssim \frac{\zeta^2 d^2}{\delta^2}+C_{\varphi}^2d~.
% \]
% \end{lemma}

% \begin{proof}
% For the first bound, we have
% \[
% \E_{\bw\sim\S^{d-1}}\norm{\bg_\bx-\tbg_\bx}
% \leq \frac{d}{2\delta}(2\zeta)\E_{\bw}\norm{\bw}
% =\frac{\zeta d}{\delta}~,
% \]
% while for the second bound
% \[
% \E_{\bw\sim\S^{d-1}}\norm{\tbg_\bx}^2
% =\E_{\bw}\norm{\tbg_\bx-\bg_\bx+\bg_\bx}^2
% \leq 2\E_{\bw}\norm{\tbg_\bx-\bg_\bx}^2+\E_{\bw}\norm{\bg_\bx}^2
% \lesssim \frac{d^2}{\delta^2}\cdot\zeta^2+C_{\varphi}^2d~.
% \]
% \end{proof}
We recall  \cref{thm:Lipschitz-min-with-inexact-grad-oracle} below to keep this section self-contained. 

\thmLipscMinWithInexactGradOracle*


Our analysis is inspired by the reduction from online learning to nonconvex optimization given by \cite{cutkosky2023optimal}.
To that end, we start by proving a seemingly unrelated result, asserting that online gradient descent minimizes the regret with respect to inexact evaluations. Recalling standard definitions from online learning, given a sequence of linear losses $\ell_m(\cdot)=\inner{g_m,\cdot}$, if an algorithm chooses ${\Delta}_1,\dots,{\Delta}_M$ we denote the regret with respect to $u$ as
\[
\mathrm{Reg}_M(u):=\sum_{m=1}^{M}\inner{g_m,{\Delta}_m-u}.
\]
Consider an update rule according to online projected \emph{inexact} gradient descent:
\[
\Delta_{m+1}:=\mathrm{clip}_{D}(\Delta_{m}-\eta_m\tbg_m).
\]

\begin{lemma}[Inexact Online Gradient Descent] \label{lem: inexact OGD}
In the setting above, suppose that $(\tbg_m)_{m=1}^{M}$ are possibly randomized vectors, such that
$\E\norm{\tbg_m-g_m}\leq\alpha$ and $\E\norm{\tbg_m}^2\leq\widetilde{G}^2$ for all $m\in[M]$.
Then for any $\norm{u}\leq D$ it holds that
\[
\E\left[\mathrm{Reg}_M(u)\right]\leq \frac{D^2}{\eta_M}+\widetilde{G}^2\sum_{m=1}^{M}\eta_m+ \alpha DM~,
\]
where the expectation is with respect to the (possible) randomness of $(\tbg_m)_{m=1}^{M}$.
In particular, setting $\eta_m\equiv\frac{D}{\widetilde{G}\sqrt{M}}$ yields
\[
\E\left[\mathrm{Reg}_M(u)\right]\lesssim D\widetilde{G}\sqrt{M}+\alpha DM~.
\]
\end{lemma}

\begin{proof}
For any $m\in[M]:$
\begin{align*}
\norm{\Delta_{m+1}-u}^2
&=\norm{\mathrm{clip}_{D}(\Delta_{m}-\eta_m\tbg_m)-u}^2
\\&\leq \norm{\Delta_{m}-\eta_m\tbg_m-u}^2
=\norm{\Delta_{m}-u}^2+\eta_m^2\norm{\tbg_m}^2-2\eta_m\inner{\Delta_{m}-u,\tbg_m}~,
\end{align*}
thus
\[
\inner{\tbg_m,\Delta_m-u}
\leq \frac{\norm{\Delta_m-u}^2-\norm{\Delta_{m+1}-u}^2}{2\eta_m}+\frac{\eta_m}{2}\norm{\tbg_m}^2~,
\]
from which we get that
\begin{align*}
\E\inner{g_m,\Delta_m-u}
&=\E\inner{\tbg_m,\Delta_m-u}+
\E\inner{g_m-\tbg_m,\Delta_m-u}
\\
&\leq\frac{\norm{\Delta_m-u}^2-\norm{\Delta_{m+1}-u}^2}{2\eta_m}+\frac{\eta_m}{2}\E\norm{\tbg_m}^2+\E\norm{g_m-\tbg_m}\cdot \norm{\Delta_m-u}
\\&\leq\frac{\norm{\Delta_m-u}^2-\norm{\Delta_{m+1}-u}^2}{2\eta_m}+\frac{\eta_m}{2}\widetilde{G}^2+\alpha D~.
\end{align*}
Summing over $m\in[M]$, we see that
\begin{align*}
\E\left[\mathrm{Reg}_M(u)\right]
&\leq \sum_{m=1}^{M}\norm{\Delta_m-u}^2\left(\frac{1}{\eta_m}-\frac{1}{\eta_{m-1}}\right)+\frac{\widetilde{G}^2}{2}\sum_{m=1}^{M}\eta_m+M\alpha D
\\
&\leq \frac{D^2}{\eta_M}+\widetilde{G}^2\sum_{m=1}^{M}\eta_m+ \alpha DM~.
\end{align*}
The simplification for $\eta_m\equiv\frac{D}{\widetilde{G}\sqrt{M}}$ readily follows.
\end{proof}


We are now ready to analyze \cref{alg: OIGRM} in the inexact gradient setting.
% We denote $\alpha=\frac{\zeta d}{\delta},~\widetilde{G}=\sqrt{\frac{\zeta^2 d^2}{\delta^2}+C_{\varphi}^2d}$. By \citet[Lemma 4]{kornowski2023algorithm}, it suffices to show that the algorithm produces a $(\delta,\epsilon)$-stationary point of $\varphi_{\delta}$. To that end,
\begin{proof}[Proof of \cref{thm:Lipschitz-min-with-inexact-grad-oracle}]
Since \cref{alg: OIGRM}  has $x_t=x_{t-1}+\Delta_{t}$, we have
\begin{align*}
F(x_t)-F(x_{t-1})
&=\int_{0}^{1}\inner{\nabla F(x_{t-1}+s\Delta_t),\Delta_t}ds
\\
&=\E_{s_t\sim\Unif[0,1]}\left[\langle\nabla F(x_{t-1}+s_t{\Delta}_t),\Delta_t\rangle\right]
\\&=\E\left[\inner{\nabla F(z_{t}),\Delta_t}\right]~.
% \\&=\E\left[\inner{\tbg_{t},\bDelta_t}\right]+\E\left[\inner{\nabla\varphi_{\delta}(\bz_{t})-\tbg_{t},\bDelta_t}\right]
% \\&\leq\E\left[\inner{\tbg_{t},\bDelta_t}\right]+\E\left[\norm{\nabla\varphi_{\delta}(\bz_{t})-\tbg_{t}}\cdot\norm{\bDelta_t}\right]
% \\& \leq\E\left[\inner{\tbg_{t},\bDelta_t}\right]+D\alpha~.
\end{align*}
% where the last inequality follows from Lemma~\ref{lem: inexact gradient}.
By summing over $t\in[T]=[K\times M]$, we get for any fixed sequence $u_1,\dots,u_K\in\reals^d:$
\begin{align*}
    \inf F\leq F(x_T)
    &\leq F(x_0)+\sum_{t=1}^{T}\E\left[\inner{\nabla F(z_{t}),\Delta_t}\right]
    \\&=F(x_0)+\sum_{k=1}^{K}\sum_{m=1}^{M}\E\left[\inner{\nabla F(z_{(k-1)M+m}),\Delta_{(k-1)M+m}-u_k}\right]\\
    &~~~~+\sum_{k=1}^{K}\sum_{m=1}^{M}\E\left[\inner{\nabla F(z_{(k-1)M+m}),u_k}\right]
    \\
    &\leq  F(x_0)+\sum_{k=1}^{K}\mathrm{Reg}_M(u_k)+\sum_{k=1}^{K}\sum_{m=1}^{M}\E\left[\inner{\nabla F(z_{(k-1)M+m}),u_k}\right]
    \\
    &\leq F(x_0)+KD\widetilde{G}\sqrt{M}+K\alpha DM+\sum_{k=1}^{K}\sum_{m=1}^{M}\E\left[\inner{\nabla F(z_{(k-1)M+m}),u_k}\right]
\end{align*}
where the last inequality follows from \cref{lem: inexact OGD}
for $\widetilde{G}=\sqrt{L^2+\alpha^2},~\eta=\frac{D}{\widetilde{G}\sqrt{M}}$, since 
$\norm{\tbg_t-\nabla F(z_t)}\leq\alpha$ (deterministically) for all $t\in[T]$ by assumption.
Letting $u_k:=-D\frac{\sum_{m=1}^{M}\nabla F(z_{(k-1)M+m})}{\norm{\sum_{m=1}^{M}\nabla F(z_{(k-1)M+m})}}$, rearranging and dividing by $DT=DKM$, we obtain
\begin{align}
\frac{1}{K}\sum_{k=1}^{K}\E\norm{\frac{1}{M}\sum_{m=1}^{M}\nabla F(z_{(k-1)M+m})}
&\leq \frac{F(x_0)-\inf F}{DT}+\frac{\widetilde{G}}{\sqrt{M}}+\alpha
\nonumber\\
&=\frac{F(x_0)-\inf F}{K\delta}
+\frac{\widetilde{G}}{\sqrt{M}}+\alpha~.
\label{eq: bound eps_first}
\end{align}
Finally, note that for all $k\in[K],m\in[M]:\norm{z_{(k-1)M+m}-\overline{x}_{k}}\leq M D\leq\delta$, therefore
$\nabla F(z_{(k-1)M+m})\in\partial_\delta F(\overline{x}_{k})$. Invoking the convexity of the Goldstein subdifferential, we see that
\[
\frac{1}{M}\sum_{m=1}^{M}\nabla F(z_{(k-1)M+m})\in\partial_\delta  F(\overline{x}_{k})~,
\]
thus it suffices to bound the first two summands on the right-hand side in \cref{eq: bound eps_first} by $\epsilon$ in order to finish the proof. This happens as long as $
\frac{F(x_0)-\inf F}{K\delta}\leq\frac{\epsilon}{2}$ and $\frac{\widetilde{G}}{\sqrt{M}}\leq\frac{\epsilon}{2}$. These are  equivalent to $ K\geq \frac{2(F(x_0)-\inf F)}{\delta\epsilon}$ and $M\geq\frac{4\widetilde{G}^2}{\epsilon^2}$, 
% \begin{align*}
% \frac{F(x_0)-\inf F}{K\delta}\leq\frac{\epsilon}{2}
% &\iff K\geq \frac{2(F(x_0)-\inf F)}{\delta\epsilon}
% \\
% \frac{\widetilde{G}}{\sqrt{M}}\leq\frac{\epsilon}{2}
% &\iff M\geq\frac{4\widetilde{G}^2}{\epsilon^2}~,
% \end{align*}
which results in \[T=KM=O\left(\frac{F(x_0)-\inf F}{\delta\epsilon}\cdot \frac{L^2+\alpha^2}{\epsilon^2}\right)=O\left(\frac{(F(x_0)-\inf F)L^2}{\delta\epsilon^3}\right),\] completing the proof.
\end{proof}










\subsection{An implementation-friendly algorithm and its analysis}
\begin{algorithm}[h]
\begin{algorithmic}[1]\caption{Perturbed Inexact GD}\label{alg: PIGD}
\State \textbf{Input:}
Inexact gradient oracle $\widetilde{\nabla}F:\reals^d\to\reals^d$, initialization $x_0\in\reals^d$, spatial parameter $\delta>0$, step size $\eta>0$, iteration budget $T\in\NN$.
\For{$t=0,\dots,T-1$}
\State Sample $w_t\sim\Unif(\S^{d-1})$
\State $\tbg_t=\widetilde{\nabla}F(x_{t}+\delta\cdot w_t)$
\State $x_{t+1}
=x_t-\eta\tbg_t
$
\EndFor
\State \textbf{Output:} $x^{\out}\sim\mathrm{Unif}\{x_0,\dots,x_{T-1}\}$. 
\end{algorithmic}
\end{algorithm}


\begin{theorem}\label{thm:practical_Lipschitz-min-with-inexact-grad-oracle}
Suppose $F:\reals^d\to\reals$ is $L$-Lipschitz, 
and that $\|\widetilde{\nabla} F(\cdot)-\nabla F(\cdot)\|\leq\alpha$. 
Then running \cref{alg: PIGD} with
$\eta=\Theta\left(\frac{{((F(x_0)-\inf F)+\delta L)^{1/2}\delta^{1/2}}}{{T^{1/2} L^{1/2}{d}^{1/4}(\alpha+L)}}\right)$
outputs a point $x^{\out}$ such that $\E[\mathrm{dist}(0,{\partial}_\delta F(x^{\out}))]\leq\epsilon+\sqrt{\alpha L}$, with \[T=O\left(\frac{(F(x_0)-\inf F+\delta L) L^3 \sqrt{d}}{\delta\epsilon^4}\right) \text{ 
calls to } \widetilde{\nabla}F(\cdot).\] 

\end{theorem}

\begin{proof}

Throughout the proof we denote $z_t=x_{t}+\delta\cdot w_t$.
Since $F$ is $L$-Lipschitz, $F_\delta(x):=\E_{w\sim\mathrm{Unif}(\S^{d-1})}[F(x+\delta\cdot w)]$ is $L$-Lipschitz and $O(L\sqrt{d}/\delta)$-smooth. By smoothness we get
\begin{align*}
F_{\delta}(x_{t+1})-F_{\delta}(x_{t})
&\leq \inner{\nabla F_{\delta}(x_t),x_{t+1}-x_{t}}+O\left(\frac{L\sqrt{d}}{\delta}\right)\cdot\norm{x_{t+1}-x_{t}}^2
\\
&=-\eta\inner{\nabla F_{\delta}(x_t),\tbg_t}+O\left(\frac{\eta^2 L\sqrt{d}}{\delta}\right)\cdot\norm{\tbg_t}^2
\\
&=-\eta\inner{\nabla F_{\delta}(x_t),\nabla F(z_t)}-\eta\inner{\nabla F_{\delta}(x_t),\tbg_t-\nabla F(z_t)}+O\left(\frac{\eta^2 L\sqrt{d}}{\delta}\right)\cdot\norm{\tbg_t}^2~.
\end{align*}


Noting that $\E[\nabla F(z_t)]=\nabla F_\delta(x_t)$
and that
$\norm{\tbg_t}\leq
\norm{\tbg_t-\nabla F(z_t)}+\norm{\nabla F(z_t)}
\leq \alpha+L$, we see that
\begin{align*}
    \E[F_{\delta}(x_{t+1})-F_{\delta}(x_{t})]
    \leq -\eta \E\norm{\nabla F_\delta (x_t)}^2+\eta L\alpha+O\left(\frac{\eta^2 L\sqrt{d}}{\delta}(\alpha+L)^2\right)~,
\end{align*}
which implies
\[
\E\norm{\nabla F_\delta (x_t)}^2 \leq \frac{\E[F_\delta(x_t)]-\E[F_\delta(x_{t+1})]}{\eta}+L\alpha+O\left(\frac{\eta L\sqrt{d}(\alpha+L)^2}{\delta}\right)~.
\]
Averaging over $t=0,\dots,T-1$ 
and noting that $F_\delta(x_0)-\inf F_\delta \leq (F(x_0)-\inf F) +\delta L$ results in
\[
\E\norm{\nabla F_\delta (x^{\out})}^2=\frac{1}{T}\sum_{t=0}^{T-1}E\norm{\nabla F_\delta (x_t)}^2
\leq \frac{(F(x_0)-\inf F)+\delta L}{\eta T}+L\alpha+O\left(\frac{\eta L\sqrt{d}(\alpha+L)^2}{\delta}\right)~.
\]
By Jensen's inequality and the sub-additivity of the square root,
\[
\E\norm{\nabla F_\delta (x^{\out})} \leq \sqrt{\frac{(F(x_0)-\inf F)+\delta L}{\eta T}}+\sqrt{L\alpha}+O\left(\sqrt{\frac{\eta L\sqrt{d}(\alpha+L)^2}{\delta}}\right)~.
\]
Setting $\eta=\frac{\sqrt{((F(x_0)-\inf F)+\delta L)\delta}}{\sqrt{T L\sqrt{d}(\alpha+L)^2}}$ yields the final bound
\[
\E\norm{\nabla F_\delta (x^{\out})} \lesssim
\frac{((F(x_0)-\inf F)+\delta L)^{1/4}L^{1/4}d^{1/8}(\alpha+L)^{1/2}}{\delta^{1/4}T^{1/4}}+\sqrt{L\alpha}~,
\]
and the first summand is bounded by $\epsilon$ for $T=O\left(\frac{((F(x_0)-\inf F)+\delta L) L \sqrt{d}(L+\alpha)^2}{\delta\epsilon^4}\right)$.



\end{proof}









