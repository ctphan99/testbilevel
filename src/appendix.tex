\appendix
\section*{Appendix}

\section{Notation}\label{sec:appendix_notation}
We use $\langle \cdot{}, {}\cdot\rangle$ to denote inner products and $\|{}\cdot{}\|$ for the Euclidean norm. Unless transposed, all vectors are column vectors. For $f:\reals^{d_2}\to\reals^{d_1}$ its Jacobian with respect to $x\in \reals^{d_2}$ is 
$\nabla f\in \reals^{d_1 \times d_2}$.  For $f:\reals^d\to\reals$, we overload  $\nabla f$ to refer to its gradient (the transposed Jacobian), a column vector. We use 
$\nabla_x$ to denote partial derivatives with respect to $x$.  
% \pswt{notation for  partial derivatives, higher order derivatives, etc.}

A function $f:\reals^n\to\reals^m$ is $L$-Lipschitz if for any $x,y$, we have $\|f(x) - f(y)\|\leq L \|x-y\|$.
A differentiable function $f:\reals^n\to\reals$ is convex if for any $x, y\in \reals^n$ we have $f(y)\geq f(x) + \nabla f(x)^\top (y-x)$; 
it is 
 $\mu$-strongly convex
 if $f - \tfrac{\mu}{2}\|{}\cdot{}\|^2$ is convex;
it is  $\beta$-smooth
if
$\nabla f$ is $\beta$-Lipschitz.

For a Lipschitz function $f$, a point $x$ is $(\delta, \epsilon)$-stationary if within a $\delta$-ball around $x$, there exists a convex combination of  subgradients of $f$ with norm at most $\epsilon$. 
For a differentiable function $f$, we say that $x$ is $\epsilon$-stationary if $\|\nabla f(x)\|\leq \epsilon$. 


\input{src/appendix-linear-equality}

\input{src/appendix-alg}


\section{Reformulation equivalence}\label{appendix:reformulation-equivalence}
\begin{restatable}[Reformulation equivalence]{theorem}{reformulation}\label{thm:reformulation_equivalence}
When $\lambda^*$ matches to an optimal dual solution to the lower level problem $y^* = \arg\min_y g(x,y) ~\text{s.t.} ~h(x,y) \leq 0$, we show that for each $x$, the reformulation has the same feasible region of $y$. 
\end{restatable}
\begin{proof}
 We first show that lower-level feasibility implies feasibility of the reformulated problem.     
    Let $y^*, \lambda^* = \min\limits_y \max\limits_{\beta \geq 0} g(x,y) + \beta^\top h(x,y)$ be the primal and the dual solution to the lower level problem with parameter $x$.
    We can verify that $y^*$ satisfies all the constraints in the reformulation problem. The feasibility condition $h(x,y^*)$ is automatically satisfied.
    We just need to check:
    % \pswt{I think we had discussed this before, but I forgot: since $h\leq 0$ is a scalar constraint, wouldn't $\lambda^*$ be a scalar? If we are allowing $h$ to be vector-valued, then what do we mean by convexity/smoothness of $h$?} <- ok, but need to change/check dimensions/order of operations everywhere
    \begin{align*}
        g^*(x) & \coloneqq  \min\limits_\theta g(x,\theta) + (\lambda^*)^\top h(x,\theta) \\
        & = g(x,y^*) + (\lambda^*)^\top h(x,y^*).\numberthis\label[eq]{eq:g_gamma_star_equals_g_plus_gamma_h}
    \end{align*}
    Therefore, $x, y^*$ is a feasible point to the reformulation problem.
    
    We now show the other direction, i.e., that feasibility of the reformulaed problem implies that of the lower-level problem. 
    % (Reformulation feasibility $\Longrightarrow$ lower level feasibility)
    Given $\lambda^*$, let us assume $y$ satisfies $g(x,y) \leq g^*_{\lambda^*}(x)$ and $h(x,y) \leq 0$.
    On the other hand, assume $y^*, \lambda^* = \min\limits_y \max\limits_{\beta \geq 0} g(x,y) + \beta^\top h(x,y)$ be the primal and the dual solution.
    We can show that:
    \begin{align}
        g(x,y) + (\lambda^*)^\top h(x,y) \leq g^*(x) \coloneqq \min_\theta g(x,\theta) + (\lambda^*)^\top h(x,\theta). 
    \end{align}
    By the strong convexity of $g + (\lambda^*)^\top h$, we know that $y$ matches to the unique minimum $y^*$, which implies that $y = y^*$ is also a feasible point to the original bilevel problem.
\end{proof}

%\pswt{note: just lower-casing all titles since that's stated explcitily in the NeurIPS CFP}
\section{Active constraints in differentiable optimization}\label{sec:inactive-constraints-in-differentiable-optimization}
By computing the derivative of the KKT conditions in \cref{sec:differentiable-optimization}, we get:
\begin{align}
    (\nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h) + (\nabla^2_{yy} g + (\lambda^*)^\top \nabla_{yy}^2 h) \frac{dy^*}{dx} + (\nabla_y h)^\top \frac{d\lambda^*}{dx} &= 0 \label{eqn:appendix-KKT1} \\
    \text{diag}(\lambda^*) \nabla_x h + \text{diag}(\lambda^*) \nabla_y h \frac{dy^*}{dx} + \text{diag}(h) \frac{d\lambda^*}{dx} &= 0. \label{eqn:appendix-KKT2}
\end{align}

Let $\mathcal{I} = \{ i \in [d_h] | h(x,y^*)_i = 0, \lambda^*_i > 0 \}$ be the set of active constraints with positive dual solution, and $\mathcal{I}_1 = \{ i | h(x,y^*)_i \neq 0 \}$ be the set of inactive constraints and $\mathcal{I}_2 = \{ i | h(x,y^*)_i = 0, \lambda^*_i = 0 \}$. We know that $\bar{\mathcal{I}} = \mathcal{I}_1 \cup \mathcal{I}_2$. For each $i \in \mathcal{I}_1$, due to complementary slackness, we know that $\lambda^*_i = 0$. 

For $i \in \mathcal{I}_1$ in \cref{eqn:appendix-KKT1}, we have $\lambda^*_i \nabla_x h(x,y^*)_i + \lambda^*_i \nabla_y h(x,y^*)_i \frac{dy^*}{dx} + h(x,y^*)_i \frac{d \lambda^*_i}{dx} = 0$, which implies $h(x,y^*)_i \frac{d \lambda^*_i}{dx} = 0$ because  $\lambda^*_i = 0$. This in turn implies $\frac{d \lambda^*_i}{dx} = 0 $ because $h(x,y^*)_i < 0$.
% \begin{align}
%     &  \nonumber \\
%     \Longrightarrow \quad &  \quad \quad \text{(because)} \nonumber \\
%     \Longrightarrow \quad &  \quad \quad \quad \quad \quad ~ ~ \text{(because )} \nonumber
% \end{align}
That means the dual variable $\lambda^*_i = 0$ and has zero gradient $\frac{d \lambda^*_i}{dx} = 0$ for any index $i \in \mathcal{I}_1$.
Therefore, we can remove row $i \in \mathcal{I}_1$ in \cref{eqn:appendix-KKT2} and obtain $\lambda^*_i = 0$ and $\frac{d \lambda^*_i}{dx} = 0$.

For $i \in \mathcal{I}_2$, the KKT condition in \cref{eqn:appendix-KKT2} is  degenerate. Therefore, $\frac{d \lambda^*_i}{d x}$ can be arbitrary, i.e., non-differentiable.
As a subgradient choice, we can set $\frac{d \lambda^*_i}{d x} = 0$ for such $i$. 
This choice will also eliminate its impact on the KKT condition in \cref{eqn:appendix-KKT1} because $\frac{d \lambda^*_i}{d x}$ is set to be $0$.
By this choice of subgradient, we can also remove row $i \in \mathcal{I}_2$ \cref{eqn:appendix-KKT2}.

Thus \cref{eqn:appendix-KKT2} can be written as the following set of equations, for  $h_\mathcal{I} = [h_i]_{i \in \mathcal{I}}$ and $\lambda^*_\mathcal{I} = [\lambda^*_i]_{i\in \mathcal{I}}$:
\begin{align}
    & \text{diag}(\lambda^*) \nabla_x h_\mathcal{I} + \text{diag}(\lambda^*_\mathcal{I}) \nabla_y h_\mathcal{I} \frac{dy^*}{dx} + \text{diag}(h_\mathcal{I}) \frac{d\lambda^*_\mathcal{I}}{dx} = 0 \nonumber \\
    \Longrightarrow \quad & \text{diag}(\lambda^*) \nabla_x h_\mathcal{I} + \text{diag}(\lambda^*_\mathcal{I}) \nabla_y h_\mathcal{I} \frac{dy^*}{dx} = 0 \quad \text{(due to $h_\mathcal{I}(x,y^*) = 0$)}. \label{eqn:appendix-new-KKT2}
\end{align}

In \cref{eqn:appendix-KKT1}, due to $\frac{d \lambda^*_i}{dx} = 0$ for all $i \in \bar{\mathcal{I}}$, we can remove $\frac{d \lambda^*_i}{dx} ~\forall i \in \bar{\mathcal{I}}$ in \cref{eqn:appendix-KKT1} by:
\begin{align}
    0 = & ~ (\nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h) + (\nabla^2_{yy} g + (\lambda^*)^\top \nabla_{yy}^2 h) \frac{dy^*}{dx} + (\nabla_y h)^\top \frac{d\lambda^*}{dx} \nonumber \\
    = & ~ (\nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h) + (\nabla^2_{yy} g + (\lambda^*)^\top \nabla_{yy}^2 h) \frac{dy^*}{dx} + (\nabla_y h_\mathcal{I})^\top \frac{d\lambda^*_\mathcal{I}}{dx}. 
    \label{eqn:appendix-new-KKT1} 
\end{align}

Combining \cref{eqn:appendix-new-KKT1} and \cref{eqn:appendix-new-KKT2}, we get:
\begin{align*}
    (\nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h) + (\nabla^2_{yy} g + (\lambda^*)^\top \nabla_{yy}^2 h) \frac{dy^*}{dx} + (\nabla_y h_\mathcal{I})^\top \frac{d\lambda^*_\mathcal{I}}{dx} &= 0 \\
    \text{diag}(\lambda^*) \nabla_x h_\mathcal{I} + \text{diag}(\lambda^*_\mathcal{I}) \nabla_y h_\mathcal{I} \frac{dy^*}{dx} &= 0, 
\end{align*}
which can be written in its matrix form:
\begin{align}\label{eqn:kkt-system_appendix}
\begin{bmatrix}
\nabla^2_{yy} g + (\lambda^*)^\top \nabla_{yy}^2 h & \nabla_y h_\mathcal{I}^\top \\
\text{diag}(\lambda^*_\mathcal{I}) \nabla_y h_\mathcal{I} & 0
\end{bmatrix}
\begin{bmatrix}
    \frac{dy^*}{dx} \\
    \frac{d\lambda^*_\mathcal{I}}{dx}
\end{bmatrix}
= 
-
\begin{bmatrix}
    \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
    \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
\end{bmatrix}
\end{align}
This concludes the derivation of the derivative of constrained optimization in \cref{eqn:kkt-system}.


\section{Inequality case: bounds on primal solution error and constraint violation}
\solutionApproximation*

\begin{proof}
We first provide the claimed bound on $\|y^*_{\alpha_1, \alpha_2} - y^*(x)\|$. 

\noindent\textbf{Part 1: Bound on the convergence of $y$.}

Since $y_{\lambda^*,\boldsymbol{\alpha}}^*$ minimizes $\mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}(x,y)$, the first-order condition gives us:
% \pswt{change  gradients to subgradients where required (e.g., for $\mathcal{L}$)}
\begin{align*}
    0 = \nabla_y \mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*). 
    % = \nabla_y f(x,y^*_{\alpha}) + \alpha_1(\nabla_y g(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + (\lambda^*)^\top \nabla_y h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)) + \alpha_2 h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top \nabla_y h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) \pswt{we do not need this expansion for this proof.}
\end{align*}
Similarly, we can compute the gradient of $\mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}(x,y)$ at $y^*$: 
% \pswt{Check: is $\lambda^*$ a scalar or a vector?}
\begin{align*}
    \nabla_y \mathcal{L}_{\alpha}(x,y^*) & = \nabla_y f(x,y^*) + \alpha_1(\nabla_y g(x,y^*) + (\lambda^*)^\top \nabla_y h(x,y^*)) + \alpha_2 \nabla_y h_\mathcal{I}(x,y^*)^\top h_\mathcal{I}(x,y^*)  \\
    & = \nabla_y f(x,y^*),
\end{align*} 
% \pswt{Also check: is $h(x,y^*)$ above a scalar or vector? I'm not seeing the dimensions of $\nabla_y g(x,y^*)$ and $(\lambda^*)^\top \nabla_y h(x, y^*)$ matching up?}
where the second step is due to the property of the primal and dual solution: $\nabla_y g(x,y^*) + (\lambda^*)^\top \nabla_y h(x,y^*) = 0$ by the stationarity condition in the KKT conditions, and by definition of the active constraints $h_\mathcal{I}$ where the optimal $y^*$ must have $h_\mathcal{I}(x,y^*) = 0$.

Since, for a sufficiently large $\alpha_1$, the penalty function is $\alpha_1 \mu_g - L_f \geq \frac{\alpha_1 \mu_g}{2}$  strongly convex in $y$, we have:
\begin{align*}
    \frac{\alpha_1 \mu_g}{2} \norm{y^* - y_{\lambda^*,\boldsymbol{\alpha}}^*} \leq \norm{\nabla_y\mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}(x,y^*) - \nabla_y\mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)} = \norm{\nabla_y f(x,y^*)} \leq L_f.
\end{align*}
Therefore, upon rearranging the terms,  we obtain the claimed bound:
\begin{align*}
    \norm{y^* - y^*_{\boldsymbol{\alpha}, \lambda^*}} \leq \frac{2 L_f}{\alpha_1 \mu_g}.
\end{align*}

\noindent\textbf{Part 2: bound on the constraint violation.}

When we plug $y^*$ into \cref{eqn:penalty-lagrangian}, we get:
\begin{align*}
    \mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}(x,y^*) & = f(x,y^*) + \alpha_1 (g(x,y^*) + (\lambda^*)^\top h(x,y^*) - g^*_{\lambda^*}(x)) + \frac{\alpha_2}{2} \norm{h_\mathcal{I}(x,y^*)}^2 = f(x,y^*).
\end{align*}
Plugging in $y^*_{\boldsymbol{\alpha},\lambda^*}$, we may obtain: 
\begin{align*}
    \mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) & = f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \alpha_1 (g(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + (\lambda^*)^\top h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - g^*(x)) + \frac{\alpha_2}{2} \norm{h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)}^2 \\
    & = f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \alpha_1 (  g(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + (\lambda^*)^\top h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - g(x,y^*) - (\lambda^*)^\top h(x,y^*) ) \\
    & \qquad + \frac{\alpha_2}{2} \norm{h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)}^2 \\
    & \geq f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \alpha_1 \mu_g \norm{y^* - y_{\lambda^*,\boldsymbol{\alpha}}^*}^2 + \frac{\alpha_2}{2} \norm{h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)}^2,
\end{align*} where we used the strong convexity (with respect to $y$) of $g(x,y)+(\lambda^*)^\top h(x,y)$ and the optimality of $y^*$ for $g(x,y)+(\lambda^*)^\top h(x,y)$. 
% \pswt{Kai: we seem to be using that the strong convexity of $g+(\lambda^*)^\top h$ is $\mu_g/2$; is it obvious why this is the case (i.e., no dependence on $\lambda^*$)?}
By the optimality of $y_{\lambda^*,\boldsymbol{\alpha}}^*$ for $\mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}$, we know that 
\begin{align*}
    f(x,y^*) = \mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}(x,y^*) \geq \mathcal{L}_{\boldsymbol{\alpha}, \lambda^*}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) \geq f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \alpha_1 \mu_g \norm{y^* - y_{\lambda^*,\boldsymbol{\alpha}}^*}^2 + \frac{\alpha_2}{2} \norm{h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)}^2.
\end{align*}
Therefore, by the Lipschitzness of the function $f$ in terms of $y$, and the bound $\|y^* - y_{\lambda^*,\boldsymbol{\alpha}}^*\| \leq \frac{2L_f}{\alpha_1 \mu_g}$, we know that:\begin{align*}
    \frac{\alpha_2}{2} \norm{h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)}^2 & \leq f(x,y^*) - f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - \alpha_1 \mu_g \norm{y^* - y_{\lambda^*,\boldsymbol{\alpha}}^*}^2 \\
    & \leq L_f \norm{y^* - y_{\lambda^*,\boldsymbol{\alpha}}^*} - \alpha_1 \mu_g \norm{y^* - y_{\lambda^*,\boldsymbol{\alpha}}^*}^2 \\
    & \leq L_f \norm{y^* - y_{\lambda^*,\boldsymbol{\alpha}}^*} \\
    & = O({\alpha_1^{-1}}). 
\end{align*}
Rearranging terms then gives the claimed bound. 
% Thus, we can then show:
% \begin{align}
%     \norm{h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)} \leq O(\frac{1}{\sqrt{\alpha_1 \alpha_2}})
% \end{align}
\end{proof}
The bound on the constraint violation in \cref{{thm:solution-bound}} is an important step in the following theorem.

\section{Proof of \cref{thm:diff_in_hypergrad_and_gradLagr}: gradient approximation for inequality constraints}\label{appendix:proof-of-inexact-gradient}
\gradientApproximation*
\begin{proof}
First, we recall \cref{eqn:penalty-lagrangian} here: \[\mathcal{L}_{\lambda^*,\boldsymbol{\alpha}}(x,y) = f(x,y) + \alpha_1 \left( g(x,y) + (\lambda^*)^\top h(x,y) - g^*(x)  \right) + \frac{\alpha_2}{2} \norm{h_\mathcal{I}(x,y)}^2.\] Next, recall from \cref{eq:g_gamma_star_equals_g_plus_gamma_h}, we can express $g^*(x)=g(x,y^*)+(\lambda^*)^\top h(x,y^*)$, which we use in the first step below: 
% \pswt{Some justification for $\lambda^*$ not being differentiated w.r.t. $x$? Or, would we get an extra term of the form $\alpha_1 \nabla_x \lambda^*(x) \cdot h(x, \ysl)$?} \kai{Why we don't need $\nabla_x \lambda^*(x) \cdot h(x, \ysl)$? For active constraints $h_i(x,y) = 0$, this product is $0$. For strictly inactive constraints $h_i(x,y) < 0$, we can show that $d \lambda^*_i(x) / dx = 0$ by the KKT system so it is also $0$. For $i$ such that $h_i(x,y) = 0$ and $\lambda^*_i = 0$, the KKT system degenerates (non-differentiable) but we can pick a valid subgradient such that $d \lambda^*_i(x) / dx = 0$, and thus its product is still $0$.} \pswt{Very cool; do we need to add this explanation, or just let it be?}
% \kai{Added a definition of $\mathcal{L}$ in Theorem 3.3} \pswt{Justification for $y^*$ being differentiable}

% \kai{I fixed the gradient notations based on our discussion. There is one counter-intuitive one where $\nabla_x F(x) = \nabla_x f(x,y^*) + \frac{d y^*}{d x}^\top \nabla_y f(x,y^*)$ as oppose to $\nabla_x F(x) = \nabla_x f(x,y^*) + \nabla_y f(x,y^*)^\top  \frac{d y^*}{d x}$ due to column v.s. row dimension (check their dimensions and it will be clear).} \pswt{I agree, $\nabla_x F(x) = \nabla_x f(x,y^*) + \frac{d y^*}{d x}^\top \nabla_y f(x,y^*)$; changed it to this throughout}
\begin{align}
    \footnotesize
    & \nabla_x F(x) - \frac{d}{dx} \mathcal{L}_{\lambda^*,\boldsymbol{\alpha}}(x,y_{\lambda^*,\alpha}^*) \nonumber \\
    = & \left( \nabla_x f(x,y^*) + \frac{d y^*}{d x}^\top \nabla_y f(x,y^*) \right) - \Biggl( \nabla_x f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \alpha_1 (\nabla_x g(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \nabla_x h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top \lambda^* \nonumber \\
    &  - \alpha_1(\nabla_x g(x,y^*) + \nabla_x h(x,y^*)^\top \lambda^*) 
    + \alpha_2
    \nabla_x h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)
\Biggl) \nonumber \\
    = & \nabla_x f(x,y^*) - \nabla_x f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) \label{eqn:f-difference} \\
    & + \frac{d y^*}{d x}^\top \nabla_y f(x,y^*) - \frac{d y^*}{d x}^\top \nabla_y f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) \label{eqn:df-difference} \\ 
    & + \frac{d y^*}{d x}^\top \nabla_y f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - \underbrace{\alpha_1 \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top  \begin{bmatrix}
        y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*  \\
        0
    \end{bmatrix} }_{\text{added term 1}} \nonumber\\
    &\qquad- \underbrace{\alpha_2 \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \lambda^* \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top \begin{bmatrix}
        0 \\
        \text{diag}(1/\lambda^*_\mathcal{I}) h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) 
    \end{bmatrix}}_{\text{added term 2}} \label{eqn:df-and-added-term} \\
    & + \alpha_1 \Biggl( \nabla_x g(x,y^*) - \nabla_x g(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \nabla_x h(x, y^*)^\top \lambda^* - \nabla_x h(x, y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top \lambda^* \nonumber\\
    &\qquad+ \underbrace{\begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top \begin{bmatrix}
        y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*  \\
        0 % h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)
    \end{bmatrix}}_{\text{added term 1}}
    \Biggl)
    \label{eqn:dgdh-and-added-term} \\
    & - \alpha_2 \nabla_x h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \underbrace{\alpha_2 \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \lambda^* \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top \begin{bmatrix}
        0 \\
        \text{diag}(1/\lambda^*_\mathcal{I}) h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) 
    \end{bmatrix}}_{\text{added term 2}}. \label{eqn:dh2-difference} 
\end{align}
% \pswt{removed $MM^{-1}$ for now since it's not used in the above steps and can be introduced later as needed;}
According to \cref{eqn:kkt-system} and \cref{eqn:kkt-system_appendix}, we let 
% \pswt{Kai: would the $\lambda^*$ in the $H$ below need to be $\lambda^*_\mathcal{I}$?} \kai{Yes thank you!} 
\[H = \begin{bmatrix}
    \nabla^2_{yy} g + (\lambda^*)^\top \nabla_{yy}^2 h & \nabla_y h_\mathcal{I}^\top \\
    \text{diag}((\lambda^*_\mathcal{I}) \nabla_y h_\mathcal{I} & 0
\end{bmatrix},\] which is invertible by \cref{item:assumption_tangen_space} and by the fact that we remove all the inactive constraints.
% \pswt{note to self: come back to this claim} % , which may not be invertible. But the top left entry $\nabla^2_{yy} g + (\nabla_{yy}^2 h)^\top (\lambda^*$ is strongly convex and invertible.
% \pswt{For \cref{eqn:df-and-added-term}, should we restrict to $S$, since later we say that that's premultiplied by the $S$-restricted $M$ to give us $dy^*/dx$? The same comment applies to the other matrices involving $\nabla^2_{yx} g$.}\pswt{<--- minor comment, can look at it later}
We now bound the terms in \cref{eqn:f-difference}, \cref{eqn:df-difference}, \cref{eqn:df-and-added-term}, \cref{eqn:dgdh-and-added-term}, and \cref{eqn:dh2-difference}.


\noindent\textbf{Bounding \cref{eqn:f-difference} and \cref{eqn:df-difference}:}
\cref{eqn:f-difference} can be easily bounded by the smoothness of $f$ 
% \pswt{aren't we using a bound on $\nabla^2_{yx} f$ here? (rather than one on  $\nabla_{yy}^2 f$)} \kai{Updated: thanks for catching this!} 
in terms of $x$ and $y$, and the bound on $\|y^*- y_{(\lambda^*,\boldsymbol{\alpha}}^*\| \leq O({\alpha_1^{-1}})$ from \cref{thm:solution-bound}. Therefore, we know:
\begin{align*}
    \norm{\nabla_x f(x,y^*) - \nabla_x f(x,y_{(\lambda^*,\boldsymbol{\alpha}}^*)} \leq C_f \norm{y^* - y_{(\lambda^*,\boldsymbol{\alpha}}^*} \leq C_f\cdot O({\alpha_1^{-1}}). 
\end{align*}
Similarly, given \cref{item:assumption_safe_constraints}  by which $y^*(x)$ is $L_y$-Lipschitz in $x$, we have the bound $\norm{\frac{d y^*}{d x}} \leq L_y$. Therefore, \cref{eqn:df-difference} can be bounded by:
% \pswt{Note: we are using Lipschitzness of $y^*$ below.}
\begin{align*}
    \norm{\frac{dy^*}{dx}^\top \nabla_y f(x,y^*) - \frac{dy^*}{dx}^\top \nabla_y f(x,y_{(\lambda^*,\boldsymbol{\alpha}}^*)} \leq C_f \norm{\frac{dy^*}{dx}} \norm{y^* - y_{(\lambda^*,\boldsymbol{\alpha}}^*} \leq C_f L_y\cdot O({\alpha_1^{-1}}). 
\end{align*}
% where $\frac{dy^*}{dx}$ can be bounded by bounding \cref{eqn:dy*dx} in Section~\ref{sec:differentiable-optimization}.


\noindent\textbf{Bounding \cref{eqn:df-and-added-term}:} 
% \pswt{dimensions on LHS and RHS?} 


Using~\cref{eqn:kkt-system} to solve $\begin{bmatrix}
    \frac{d y^*}{d x} \\
    \frac{d \lambda^*}{d x}
\end{bmatrix} = -H^{-1} \begin{bmatrix}
    \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
    \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
\end{bmatrix}$, 
% \pswt{Kai: the RHS in the previous equation looks like it's for the concatenated vector of $dy/dx$ and $d\lambda^*/dx$; so $dy/dx$ is actually some indicator vector times the RHS we have here. (But I think it's fine in the RHS of the next equation because of the added zeroes)} \kai{Thanks for finding the typo! Just fixed it.} 
we can write:
\begin{align*}
    & \frac{d y^*}{d x}^\top \nabla_y f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) =  \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top (H^{-1})^\top \begin{bmatrix}
        - \nabla_y f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) \\ 0
    \end{bmatrix}   \\
     &= -\frac{d y^*}{d x}^\top  \Biggl( \alpha_1 \begin{bmatrix}
        \nabla_y g(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \nabla_y h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top \lambda^* \\
        0
    \end{bmatrix} \\ 
    &\quad + 
    \alpha_2 \begin{bmatrix}
         \nabla_y h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) \\
        0
    \end{bmatrix}
    \Biggl), \numberthis\label{eqn:dfdy_dydx_expansion}
\end{align*}
where we use the optimality of $y_{\lambda^*,\boldsymbol{\alpha}}^*$ from \cref{{eq:def_y_lambda_star}}:
\begin{align*}\numberthis\label{eqn:y_lambda_star_optimality}
    & \nabla_y f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \alpha_1 \left( \nabla_y g(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) + \nabla_y h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top \lambda^* \right) \\
    &\quad+ \alpha_2 \nabla_y h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) = 0. 
\end{align*}
Further, recall that $H$ is non-degenerate by \cref{assumption:linEq_smoothness}, as a result of which, the added term 1 in \cref{eqn:df-and-added-term} can be modified as follows: 
% \pswt{Would there be a factor of $\textrm{diag}(\lambda^*)$ in the lower term of the first matrix on the RHS, since it's in $M$ as well?}<- ok, good
% \pswt{Also, would the terms in the first matrix in the RHS  require the $S$ subscript since they come from $M$?}\pswt{<--- minor comment, will look later}
\begin{align}
    \footnotesize
    & \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top \begin{bmatrix}
        \alpha_1 (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*) \\
        0
    \end{bmatrix} \nonumber \\
    = & \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top (H^{-1})^\top  H^\top  \begin{bmatrix}
        \alpha_1 (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*) \\
        0
    \end{bmatrix} \nonumber \\
    = & \alpha_1 
    \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top (H^{-1})^\top \begin{bmatrix}
        (\nabla^2_{yy} g + (\lambda^*)^\top \nabla_{yy}^2 h)^\top (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*) \\
        \nabla_y h_\mathcal{I}(x,y^*)(y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*)
    \end{bmatrix}. \label{eqn:added_term}
\end{align}

The added term 2 in \cref{eqn:df-and-added-term} can be expanded to: 
\begin{align}
    & \alpha_2 \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \lambda^* \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top \begin{bmatrix}
        0 \\
        \text{diag}(1/\lambda^*_\mathcal{I}) h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) 
    \end{bmatrix} \nonumber \\
    = & \alpha_2 \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \lambda^* \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top (H^{-1})^\top H^\top  \begin{bmatrix}
        0 \\
        \text{diag}(1/\lambda^*_\mathcal{I}) h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) 
    \end{bmatrix} \nonumber \\
    = & \alpha_2  \begin{bmatrix}
        \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \lambda^* \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top (H^{-1})^\top \begin{bmatrix}
        \nabla_y h_\mathcal{I}(x,y^*)^\top h_\mathcal{I}(x,y_{(\lambda^*,\boldsymbol{\alpha}}^*) \\
        0
    \end{bmatrix} \label{eqn:added-term-2}
\end{align}
% \pswt{Kai: the last step in \cref{{eqn:added-term-2}} seems to require $H$ to have $\textrm{diag}(\lambda^*_\mathcal{I})$, not just $\textrm{diag}(\lambda^*)$, to enable the cancellation. } \kai{I think it was fixed at some point. Can you check again?}
% where the first equality is due to the top-left entry in $H$, i.e., $\nabla^2_{yy} g + (\lambda^*)^\top \nabla_{yy}^2 h$, are positive definite due to the strong convexity. This implies that the top-left entry is invertible, and thus the top rows are non-degenerated. Therefore, the product of $H$ and its pseudoinverse $H^{-1}$ will be identity on the top rows, which preserves the product in the first equality.
    
Therefore, we can compute the difference between \cref{eqn:dfdy_dydx_expansion}, \cref{eqn:added_term}, and \cref{eqn:added-term-2} to bound \cref{eqn:df-and-added-term}, and use the fact that $\nabla_y g(x,y^*) + (\lambda^*)^\top \nabla_y h(x,y^*) = 0$: 
% \pswt{Since we are currently bounding \cref{eqn:df-and-added-term}, don't we also need to include the term \[- \alpha_2 \begin{bmatrix}
        % 0 \\
        % \text{diag}(1/\lambda^*) h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) 
    % \end{bmatrix} ^\top M M^{-1}   \begin{bmatrix}
        % \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
        % \text{diag}(\lambda^*) \nabla_x h 
    % \end{bmatrix} = - \alpha_2 \begin{bmatrix}
            % h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top \nabla_y h(x,y^*) \\
            % 0
        % \end{bmatrix}^\top M^{-1}   \begin{bmatrix}
        % \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
        % \text{diag}(\lambda^*) \nabla_x h 
    % \end{bmatrix}\]?   Computing the inner product, this simplifies to \[\alpha_2 \cdot h(x, y^*_{\alpha})_{+} \cdot \|\nabla_x h(x, y^*)\|.\] I'm currently not able to see if it's being bounded somewhere. Note that a very similar-looking term appeared later when bounding \cref{eqn:h2-difference}, where we seem to have used $h(x, y^*_{\alpha})_{+}^\top \nabla_y h(x, y^*)=0$, so perhaps we could use a similar approach for this term, if needed?} \pswt{<- added term} 
\begin{align}
    \footnotesize
    & \frac{d y^*}{d x}^\top \nabla_y 
 f(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - \text{added term 1 } - \text{added term 2}  \nonumber \\
    = & \begin{bmatrix}
    \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
        \text{diag}(\lambda^*_\mathcal{I}) \nabla_x h_\mathcal{I}
    \end{bmatrix}^\top (H^{-1})^\top \biggr( \alpha_1 \begin{bmatrix}
        \nabla_y g(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - \nabla_y g(x,y^*) - \nabla^2_{yy} g(x,y^*) (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*) \\ 
        0
        \end{bmatrix}\label{eqn:g_second_order_difference} \\
     & +
    \alpha_1 \begin{bmatrix}
        \nabla_y h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top \lambda^* - \nabla_y h(x,y^*)^\top \lambda^* - \nabla^2_{yy} h(x,y^*)^\top \lambda^* (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*) \\
        0
    \end{bmatrix}\label{eqn:h_second_order_difference}
    \\
    & \quad -
    \alpha_1 \begin{bmatrix}
        0 \\ 
        \nabla_y h_\mathcal{I}(x,y^*) (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*)
    \end{bmatrix} \label{eqn:constraint_difference}
    \\
    & \quad \quad + \alpha_2  \begin{bmatrix}
        \nabla_y h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)  \\
        0 
    \end{bmatrix} - \begin{bmatrix}
        \nabla_y h_\mathcal{I}(x,y^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) \\
        0
    \end{bmatrix} \biggr)
    \label{eqn:h2-difference}
\end{align}
% \pswt{A factor of $\textrm{diag}(\lambda^*)$ in \cref{{eqn:constraint_difference}}? }\kai{I think this was resolved. It was just due to the matrix transpose.} 
The terms in \cref{eqn:g_second_order_difference} and \cref{eqn:h_second_order_difference} can both be bounded by $\alpha_1 C_{g} L_y \|y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*\|^2$ and $\alpha_1 R C_{h} L_y \|y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*\|^2$ by the smoothness of $g$ and $h^\top \lambda^*$. Further, plugging in   $\|y^*- y_{\lambda^*,\boldsymbol{\alpha}}^*\| \leq O({\alpha_1^{-1}})$ from \cref{thm:solution-bound} bounds both these terms by $O({\alpha_1^{-1}})$. 
% \pswt{Kai: are we perhaps missing an $L_y$ factor in the two bounds here since we also have $\|dy^*/dx\|$?} \kai{Yes you are right! Thank you!}


To bound the term in \cref{eqn:constraint_difference}, we use:
\begin{align*}%\label{eqn:h-smoothness}
    \norm{h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - h_\mathcal{I}(x,y^*) - \nabla_y h_\mathcal{I}(x,y^*) (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*)} \leq C_h \norm{y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*}^2. 
\end{align*}
Therefore, we have: 
\begin{align*}
    % \nabla_y h_\mathcal{I}(x,y^*) (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*) & = h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - h_\mathcal{I}(x,y^*) + C_h O(\norm{y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*}^2)  \\
     \norm{\nabla_y h_\mathcal{I}(x,y^*) (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*)} & \leq  \norm{h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)} +\norm{h_\mathcal{I}(x,y^*)} + C_h O(\norm{y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*}^2) \\
    & \leq O({{\alpha_1^{-1/2} \alpha_2^{-1/2}}}) + 0 + O({\alpha_1^{-2}}) \\
    & = O({{\alpha_1^{-1/2} \alpha_2^{-1/2}}} + {\alpha_1^{-2}}),%\numberthis\label{eqn:gamma-constraint-violation}
\end{align*}
which upon scaling by $\alpha_1$ gives us the following bound on the term in \cref{eqn:constraint_difference}:
\begin{align*}
    \alpha_1 \norm{\nabla_y h_\mathcal{I}(x,y^*) (y_{\lambda^*,\boldsymbol{\alpha}}^* - y^*)} \leq O(\alpha_1^{1/2} \alpha_2^{-1/2} + \alpha_1^{-1}) ~.
\end{align*}


The term in \cref{eqn:h2-difference} can be bounded by:
% \pswt{What is the justification for the first step? (It appears that this follows from $h(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)_+^\top \nabla_y h(x,y^*)=0$, but I don't yet see why this is true)} \kai{Good point! I spent a while thinking about this and found that I need to modify the Lagrangian minimization problem with the change of $h_+$ to $h_\mathcal{I}$ instead. Please verify if there is any other issue.}
\begin{align*}
    & \alpha_2 \norm{\nabla_x h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - \nabla_x h_\mathcal{I}(x,y^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)} \\
    = & \alpha_2 \norm{\nabla_x h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - \nabla_x h_\mathcal{I}(x,y^*)} O(\norm{h_\mathcal{I}(x,y^*_{\boldsymbol{\alpha},\lambda^*})})  \\
    = & \alpha_2\cdot O({\alpha_1^{-1}}) O({{\alpha_1^{-1/2} \alpha_2^{-1/2}}}) \\
    = & O(\alpha_1^{-3/2} \alpha_2^{1/2}) \numberthis\label{eq:bound_on_h_plus_grady_h}
\end{align*}
% \pswt{Kai: just want to check --- in the second equation above, we are using that $h_\mathcal{I}$ is also smooth, right? Does that follow from $h$ being smooth?} \kai{Yes, it follows by $h_\mathcal{I}$ being a subset of the entries of $h$, where $h$ is convex (for each entry).}

\noindent\textbf{Bounding \cref{eqn:dgdh-and-added-term}:}
This can be easily bounded by the smoothness of $g$ and $h$, and the bound on the dual solution $\norm{\lambda^*} \leq R$.
Thus \cref{eqn:dgdh-and-added-term} can be bounded by $R \cdot O({\alpha_1^{-1}}) = O({\alpha_1^{-1}})$.
% \pswt{Kai: just wanted to check, we are keeping $R$ but seem to be putting the smoothness constants in the big-oh notation; is that ok? (perhaps this is a minor question since the final bound doesn't anyway show the constants explicitly)} \kai{added $O(\alpha^{-1})$ right after.}
% \pswt{Isn't there a $\alpha_1$ scaling factor?}

\noindent\textbf{Bounding \cref{eqn:dh2-difference}:}
By the same argument in \cref{eq:bound_on_h_plus_grady_h}, we get:
\begin{align}
    & \alpha_2 \norm{\nabla_y h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - \nabla_y h_\mathcal{I}(x,y^*)^\top h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) } \nonumber \\
    \leq & \alpha_2 \norm{\nabla_y h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*) - \nabla_y h_\mathcal{I}(x,y^*)} \norm{h_\mathcal{I}(x,y_{\lambda^*,\boldsymbol{\alpha}}^*)} \nonumber \\
    = &  \alpha_2\cdot O({\alpha_1^{-1}}) O({{\alpha_1^{-1/2} \alpha_2^{-1/2}}}) \nonumber \\
    = & O(\alpha_1^{-3/2}\alpha_2^{1/2}) ~. \nonumber
\end{align}

Combining all upper bounds gives the claimed bound. 
% :
% \begin{align}\label{eqn:gradient-approximation}
%    & \norm{\nabla_x F(x) - \nabla_x L_{\boldsymbol{\alpha}, \lambda^*}(x, y_{\lambda^*,\boldsymbol{\alpha}}^*)} \leq O(\frac{1}{\alpha_1}) + O(\frac{1}{\alpha_1^{1/2}\alpha_2^{1/2}}) + O(\frac{\alpha_1^{1/2}}{\alpha_2^{1/2}}) + O(\frac{\alpha_2^{1/2}}{\alpha_1^{3/2}})
% \end{align}
\end{proof}

\section{Proof of the main result (\cref{thm:cost_of_computing_ystar_gammastar_inequality}): convergence and computation cost}\label{appendix:cost_of_computing_ystar_gammastar_inequality}
\computationCostInequality*
\begin{proof}
    First, given the bound in \cref{thm:diff_in_hypergrad_and_gradLagr}, we choose $\alpha_1 = \alpha^{-2}$ and $\alpha_2 = \alpha^{-4}$ to ensure the inexactness of the gradient oracle is bounded by $\alpha$. In the later analysis, we will still use $\alpha_1$ and $\alpha_2$ in the penalty function for clarity.

    Now we estimate the computation cost of the inexact gradient oracle:
    
    \noindent\textbf{Lower-level problem.}
    Given the oracle access to the optimal dual solution $\lambda^*(x)$, we can recover the primal solution $y^*(x)$ efficiently (e.g, by \cite{zhang2022solving}). Therefore, we can use the primal and dual solutions to construct the penalty function $\mathcal{L}_{\lambda^*, \alpha}(x,y)$ in ~\cref{eqn:penalty-lagrangian}.
    
    \noindent\textbf{Penalty function minimization problem.}
    The second main optimization problem is the penalty minimization problem in \cref{line:lagrangian-optimization} of \cref{alg:inexact-gradient-oracle}.
    Recall from \cref{eqn:penalty-lagrangian} that 
    \begin{align}\label{eqn:penalty-lagrangian-approximate}
        \mathcal{L}_{\lambda,\boldsymbol{\alpha}}(x,y) = f(x,y) + \alpha_1 \left( g(x,y) + \lambda^\top h(x,y) - g^*(x)  \right) + \frac{\alpha_2}{2} \norm{h_\mathcal{I}(x,y)}^2, 
    \end{align}
    where we use the approximate dual solution $\lambda$ as opposed to the optimal dual solution $\lambda^*$.
    % \begin{remark}
    %     The uniqueness of the optimal dual solution is guaranteed under the LICQ assumption in ~\cref{assumption:linEq_smoothness}) based on the result from ~\cite{wachsmuth2013licq}.
    % \end{remark}

    Given \cref{eqn:penalty-lagrangian-approximate}, we solve the penalty minimization problem: 
    \begin{align*}
        y'_{\lambda, \alpha}(x) \coloneqq \arg\min_y \mathcal{L}_{\lambda,\boldsymbol{\alpha}}(x,y). 
    \end{align*}
    The penalty minimization is a unconstrained strongly convex optimization problem, which is known to have linear convergence rate. We further analyze its convexity and smoothness below to precisely estimate the computation cost:
    \begin{itemize}
        \item The strong convexity of $\mathcal{L}_{\lambda,\boldsymbol{\alpha}}(x,y)$ is lower bounded by $\frac{\alpha_1 \mu_g}{2} = O(\alpha_1)$.
        \item The smoothness of $\mathcal{L}_{\lambda,\boldsymbol{\alpha}}(x,y)$ is dominated by the smoothness of $\alpha_2 \norm{h_\mathcal{I}(x,y)}^2$ since $\alpha_2 \gg \alpha_1$. By \cref{thm:diff_in_hypergrad_and_gradLagr}, we know that the optimal solution must lie in an open ball $B(y^*, O(1/\alpha_1))$ with center $y^*$ (inner optimization primal solution) and a radius of the order of $O(\frac{1}{\alpha_1})$. This implies that we just need to search over a bounded feasible set of $y$, which we can bound $\norm{\nabla_y h(x,y)} \leq L_h$ and $h(x,y) \leq H$ within the bounded region $y \in B(y^*, O(1/\alpha_1))$. We can show that $h^2$ is smooth (gradient Lipschitz) within the bounded region by the following:
        \begin{align}
            \norm{\nabla^2_{yy} h^2} = \norm{h \nabla^2_{yy} h + \nabla_y h^\top \nabla_y h} \leq \norm{h \nabla^2_{yy} h} + \norm{\nabla_y h^\top \nabla_y h} \leq H C_h + L_h^2 \nonumber
        \end{align}
        which also implies $h^2_{\mathcal{I}}$ is also smooth (gradient Lipschitz).
        Therefore, $\alpha h^2_{\mathcal{I}}$ is $( H C_h + L_h^2) \alpha_2 = O(\alpha_2)$ smooth.
    \end{itemize}
     
    Choosing $\alpha_1 = \frac{1}{\alpha^2}$ and $\alpha_2 = \frac{1}{\alpha^4}$, the condition number of $\mathcal{L}_{\boldsymbol{\alpha}, \lambda}(x,y)$ becomes $\kappa = O(\alpha_2 / \alpha_1) = O(\frac{1}{\alpha^2})$.
    Therefore, by the linear convergence of gradient descent in strongly convex smooth optimization, the number of iterations needed to get to $\alpha$ accuracy is $O(\sqrt{\alpha^{-2}} \times \log (\frac{1}{\alpha})) = O(\frac{1}{\alpha} \log (\frac{1}{\alpha}))$.
    Therefore, we can get a near optimal solution $y'_{\lambda,\alpha}$ with inexactness $\alpha$ in $O(\frac{1}{\alpha})$ oracle calls.

\noindent\textbf{Computation cost and results.}
Overall, for the inner optimization, we can invoke the efficient optimal dual solution oracle to get the optimal dual solution $\lambda^*(x)$ and recover the optimal primal solution $y^*(x)$ from there.
For the penalty minimization, we need $O(\frac{1}{\alpha})$ oracle calls to solve an unconstrained strongly convex smooth optimization problem to get to $\alpha$ accuracy.
In conclusion, combining everything in \cref{appendix:cost_of_computing_ystar_gammastar_inequality}, we run $O(\frac{1}{\alpha})$ oracle calls to obtain an $\alpha$ accurate gradient oracle to approximate the hyperobjective gradient $\nabla_x F(x)$. This concludes the proof of \cref{thm:cost_of_computing_ystar_gammastar_inequality}.
\end{proof}

\begin{remark}
    The following analysis  quantifies how the error in the optimal dual solution propagates to the inexact gradient estimate. This is not needed if such a dual solution oracle exists. But in practice, the oracle may come with some error, for which we bound the error.
\end{remark}

\noindent\textbf{Bounding the error propagation in error in dual solution and the penalty minimization.}
First, if we do not get an exact optimal dual solution, the error in the dual solution $\lambda$ with $\norm{\lambda - \lambda^*} \leq \alpha$ will slightly impact the analysis in \cref{thm:diff_in_hypergrad_and_gradLagr}. Specifically, in \cref{appendix:proof-of-inexact-gradient}, the approximate $\lambda$ will impact the inexact gradient $\nabla_x \mathcal{L}_{\lambda,\alpha}(x,y'_{\lambda,\alpha})$ computation and the analysis in \cref{eqn:dgdh-and-added-term} and \cref{eqn:y_lambda_star_optimality}.
% \begin{itemize}
    % \item 
    In \cref{eqn:dgdh-and-added-term}, to change $\lambda$ to $\lambda^*$, we get an additional error:
    \begin{align*}\numberthis\label{eqn:dual-convergence-merit-function-1}
        & \alpha_1 \biggl( \nabla_x h(x,y')^\top (\lambda - \lambda^*) - \nabla_x h(x,y'_{\lambda,\alpha})^\top (\lambda - \lambda^*) \biggl) \nonumber \\
        = & \alpha_1 (\nabla_x h(x,y') - \nabla_x h(x,y'_{\lambda,\alpha}))^\top (\lambda - \lambda^*) \nonumber \\
        \leq & \alpha_1 C_h \norm{y' - y'_{\lambda,\alpha}} (\lambda - \lambda^*) \nonumber \\
        \leq & O(\alpha_1 \alpha_1^{-1} \alpha) = O(\alpha), 
    \end{align*}
    where the last inequality is due to $\norm{y' - y'_{\lambda,\alpha}} \leq O(\alpha_1^{-1})$ that is based on a similar analysis in \cref{thm:solution-bound} with a near-optimal $y'_{\lambda,\alpha}$ under $\alpha^2 = \alpha_1$ accuracy.
    
    Therefore, the error incurred by inexact $\lambda$ in \cref{eqn:dgdh-and-added-term} is at most $O(\alpha)$, which is of the same rate as the current gradient inexactness $O(\alpha)$.
    % \item 
    
    In \cref{eqn:y_lambda_star_optimality}, the optimality holds approximately for the approximate $\lambda$. Therefore, by the near optimality of $y_{\lambda,\boldsymbol{\alpha}}'$ (strongly convex optimization), we know that the following gradient is also $\alpha$-close to $0$, i.e.,
    \begin{align*}\numberthis\label{eqn:inexact-optimality-with-inexact-lambda}
        & \|\nabla_y f(x,y_{\lambda,\boldsymbol{\alpha}}') + \alpha_1 \left( \nabla_y g(x,y_{\lambda,\boldsymbol{\alpha}}') + \nabla_y h(x,y_{\lambda,\boldsymbol{\alpha}}')^\top \lambda \right) \\
        &\quad + \alpha_2 \nabla_y h_\mathcal{I}(x,y_{\lambda,\boldsymbol{\alpha}}')^\top h_\mathcal{I}(x,y_{\lambda,\boldsymbol{\alpha}}')\| \leq \alpha, 
    \end{align*}
    whose inexactness matches the inexactness of the gradient oracle $\alpha$, and thus we do not incur additional order of inexactness here.
    
    Moreover, there is an additional error because we need $\lambda^*$ as opposed to a near-optimal $\lambda$ to make the analysis in \cref{appendix:proof-of-inexact-gradient} work. The error between using $\lambda$ and $\lambda^*$ in \cref{eqn:inexact-optimality-with-inexact-lambda} can be bounded by:
    \begin{align}\label{eqn:dual-convergence-merit-function-2}
        \norm{\nabla_y h(x,y'_{\lambda,\alpha})^\top (\lambda - \lambda^*)} \leq L_h \alpha, 
    \end{align}
    where we use the local Lipschitzness of the function $h$ in an open ball near $y^*$.
    Therefore, the additional error is also  $O(\alpha)$, which matches the inexactness of the inexact gradient oracle. 
% \end{itemize}

Therefore, we conclude that in order to bound the inexactness of the gradient oracle, we just need an efficient inexact dual solution with $\alpha$ accuracy.


\section{Practical oracle to optimal (approximate) dual solution}
Here we discuss how practical the assumption on the oracle access to the optimal dual solution is.

For linear inequality constraint $h(x,y) = Ax - By - b$, the LL problem is a constrained strongly convex smooth optimization problem. 
    % All we need from the LL strongly convex smooth optimization problem is a convergence guarantee for the dual variable.
    % More precisely, we just need the dual variable $(\lambda^*)^t$ to be $1/t$-close to the optimal dual solution $\lambda^*$ in $t$ iterations.
    % In other word, we expect to find a dual solution $\lambda^t$ to the inner optimization problem with $\norm{\lambda^t - \lambda^*} \leq \alpha $ in $t = O(1/\alpha)$ iterations.
To show that we can compute an approximate solution to the optimal dual solution for linear inequality constraints, we apply the result from ~\cite{du2019linear}:
\begin{corollary}[Application of Corollary 3.1 in \cite{du2019linear}]\label{cor:linear-convergence-linear-inequality-LL}
    When $h(x,y) = Ax - By + b$ is linear in $y$, the primal and dual solutions can be written as:
    \begin{align}
        & y^*, \lambda^* = \arg\min_y \max_\lambda g(x,y) + (\lambda^*)^\top h(x,y) = g(x,y) - (\lambda^*)^\top B y + R(x) \nonumber \\
        \iff & y^*, \lambda^* = \arg\min_y \max_\lambda g(x,y) - (\lambda^*)^\top B y
    \end{align}
    where $g$ is strongly convex in $y$ and $B$ is of full rank by \cref{assumption:linEq_smoothness}. According to Corollary 3.1 from \cite{du2019linear}, the primal-dual gradient method guarantees a linear convergence. More precisely, in $t = O(\log \frac{1}{\alpha})$ iterations, we get:
    \begin{align}
        \norm{y^t - y^*} \leq \alpha \text{ and } \norm{\lambda^t - \lambda^*} \leq \alpha. 
    \end{align}
\end{corollary}
Given \cref{cor:linear-convergence-linear-inequality-LL}, we can efficiently approximate the primal and dual solutions up to high accuracy with $O(\log \frac{1}{\alpha})$ oracle calls when the inequality constraints are linear. This gives us an efficient approximate oracle access to the dual solution.

\begin{remark}
    Under the assumption of an optimal dual solution oracle, all the analyses mentioned in ~\cref{sec:inequality-bilevel} hold for the general convex inequality constraints. However, the main technical challenge is that the dual solution oracle for general convex inequality cannot be guaranteed in practice.
    In fact, to the best of our knowledge, there is no iterate convergence in the dual solution $\lambda$ for general convex inequality constraints. Most of the literature in strongly-convex-concave saddle point convergence only guarantees dual solution convergence in terms of its duality gap or some other merit functions. We are not aware of any successful bound on the dual solution iterate convergence, which is an important research question to answer by itself.
    This is the main technical bottleneck for general convex inequality constraints as well.
\end{remark}
\begin{remark}
    On the other hand, we need the dual solution iterate convergence with rate $O(1/\alpha)$ to ensure the error to be bounded. But this is not a necessary condition. To ensure a bound on the error propagation, we just need to bound some forms of merit functions (\cref{eqn:dual-convergence-merit-function-1} and \cref{eqn:dual-convergence-merit-function-2}) of the dual solutions, which we believe that this is much more tractable than the actual iterate dual solution convergence. We leave this as a future direction and this will generalize the analysis from linear inequality constraints to general convex inequality constraints.
\end{remark}

% \section{Numerical precision issue in finding active constraint sets}\label{appendix:misspecified-active-constraint-set}
% In \cref{sec:inequality-bilevel}, we assume access to the active constraint set $\mathcal{I} \coloneqq \{i \in [d_h]: h_i(x,y^*) = 0, \lambda_i > 0 \}$. However, in practice, we may not be able to find the exact active constraint set due to precision issue.
% Even though in the linear inequality case where we have linear convergence in both the primal and dual solutions, we still may mis-specify nearly-active constraints as long as their violation is small.
% In practice, suppose we run the primal and dual solutions up to $O(\epsilon)$ accuracy, we can select an $\epsilon$-approximate active constraint set by choosing $\mathcal{I}_{\text{approx}} = \{i \in [d_h]: \norm{h_i(x,y^*)} < \epsilon, \lambda_i > \epsilon \}$.
% Here, we discuss how to select the accuracy $\epsilon$ for the primal and dual solutions to ensure the error incurred by incorrect active constraint set can be bounded.

% First, this analysis is based on assuming that the eigenvalues of the KKT matrix $H(x)$ (defined in \cref{eqn:kkt-system} for a given $x$) with the corresponding accurate active constraint set $\mathcal{I}$ is lower bounded by $\epsilon_H$ and upper bounded by $C_H$.
% Since $H$ is invertible due to the linear independence of $\nabla_y h_\mathcal{I}$ with positive dual variable $\lambda_\mathcal{I} > 0$, we know that $H$ is invertible and thus its smallest eigenvalue is positive too.
% This assumption on the eigenvalue lower bound may not hold in general, but if we have bounded feasible region $x \in \mathcal{X}$ with LICQ assumption (\cref{item:assumption_safe_constraints}), we know that the KKT matrix is lower bounded due to the invertibility of $H$ and the feasible region $\mathcal{X}$ is compact.
% So for compact upper level feasible set, the assumption automatically holds.


% Now we can formally bound the error incurred by mis-specifying the set $\mathcal{I}_{\text{incor}}$ as active constraints (note that we will correctly identify the active constraints). 
% The incorrectly specified set must have $\norm{h_i(x,y')} \leq \epsilon$ and $\lambda_i > \epsilon$.

% Now we can write the incorrectly specified KKT system in \cref{eqn:kkt-system} by:
% \begin{align}\label{eqn:kkt-system-extended}
% \begin{bmatrix}
% \nabla^2_{yy} g + (\lambda)^\top \nabla_{yy}^2 h & \nabla_y h_\mathcal{I}^\top & \nabla_y h_{\mathcal{I}_{\text{incor}}}^\top \\
% \text{diag}(\lambda_\mathcal{I}) \nabla_y h_\mathcal{I} & 0 & 0 \\
% \text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_y h_{\mathcal{I}_{\text{incor}}} & 0 & 0
% \end{bmatrix}
% \begin{bmatrix}
%     \frac{d y(x)}{d x} \\
%     \frac{d \lambda_\mathcal{I}(x)}{d x} \\
%     \frac{d \lambda_{\mathcal{I}_{\text{incor}}}(x)}{d x}
% \end{bmatrix}
% = 
% -
% \begin{bmatrix}
%     \nabla^2_{yx} g + (\lambda)^\top \nabla_{yx}^2 h \\
%     \text{diag}(\lambda_\mathcal{I}) \nabla_x h_\mathcal{I} \\
%     \text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_x h_{\mathcal{I}_{\text{incor}}}
% \end{bmatrix}
% \end{align}
% where everything is the same except for an additional set of constraints.

% We can write the KKT matrix as a block matrix:
% \begin{align}\label{eqn:inexact-kkt-system}
%     \begin{bmatrix}
%         H' & h_{\mathcal{I}_{\text{incor}}}^\top \\
%         \text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_y h_{\mathcal{I}_{\text{incor}}} & 0
%     \end{bmatrix}
%     \begin{bmatrix}
%         \frac{d [y,\lambda_\mathcal{I}]}{d x} \\
%         \frac{d \lambda_{\mathcal{I}_{\text{incor}}}(x)}{d x}
%     \end{bmatrix}
%     = 
%     - 
%     \begin{bmatrix}
%         b \\
%         \text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_x h_{\mathcal{I}_{\text{incor}}}
%     \end{bmatrix}
% \end{align}
% where $b' = 
% \begin{bmatrix}
%     \nabla^2_{yx} g + (\lambda)^\top \nabla_{yx}^2 h \\
%     \text{diag}(\lambda_\mathcal{I}) \nabla_x h_\mathcal{I}
% \end{bmatrix}$

% We know that $H', b'$ are both $\epsilon$ accurate compared to the correct $H$ and gradients $b = 
% \begin{bmatrix}
%     \nabla^2_{yx} g + (\lambda^*)^\top \nabla_{yx}^2 h \\
%     \text{diag}(\lambda_\mathcal{I}) \nabla_x h_\mathcal{I}
% \end{bmatrix}$, and the correct gradient with the exact active set of constraints is computed by $-H^{-1} b$.
% Therefore, our goal is to bound the difference between the solution to Equation~\cref{eqn:inexact-kkt-system} and $-H^{-1} b$.

% To see this, we write the following using Schur complement:
% \begin{align}
%     & \begin{bmatrix}
%         H' & h_{\mathcal{I}_{\text{incor}}}^\top \\
%         \text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_y h_{\mathcal{I}_{\text{incor}}} & 0
%     \end{bmatrix}^{-1} 
%     \begin{bmatrix}
%         b \\
%         \text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_x h_{\mathcal{I}_{\text{incor}}}
%     \end{bmatrix}
%     \\
%     = & 
%     \begin{bmatrix}
%         I & - H^{-1} \nabla_y h_{\mathcal{I}_{\text{incor}}}^\top \\
%         0 & I
%     \end{bmatrix}
%     \begin{bmatrix}
%         H^{-1} & 0 \\
%         0 & - (\text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_y h_{\mathcal{I}_{\text{incor}}} H^{-1} \nabla_y h_{\mathcal{I}_{\text{incor}}}^\top)^{-1} 
%     \end{bmatrix}  \\
%     &\quad\times
%     \begin{bmatrix}
%         I & 0 \\
%         - \text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_y h_{\mathcal{I}_{\text{incor}}} H^{-1} & I
%     \end{bmatrix}
%     \begin{bmatrix}
%         b \\
%         \text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_x \nabla_y h_{\mathcal{I}_{\text{incor}}}
%     \end{bmatrix} \nonumber \\
%     = & 
%     \begin{bmatrix}
%         I & - H^{-1} \nabla_y h_{\mathcal{I}_{\text{incor}}}^\top \\
%         0 & I
%     \end{bmatrix}
%     \begin{bmatrix}
%         H^{-1} & 0 \\
%         0 & - (\text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) \nabla_y h_{\mathcal{I}_{\text{incor}}} H^{-1} \nabla_y h_{\mathcal{I}_{\text{incor}}}^\top)^{-1} 
%     \end{bmatrix}
%     \begin{bmatrix}
%         b \\
%         O(\epsilon \epsilon_H^{-1})
%     \end{bmatrix} \nonumber \\
%     = & 
%     \begin{bmatrix}
%         I & - H^{-1} \nabla_y h_{\mathcal{I}_{\text{incor}}}^\top \\
%         0 & I
%     \end{bmatrix}
%     \begin{bmatrix}
%         H^{-1} b \\
%         O(\epsilon \epsilon_H^{-1}) \cdot O(\epsilon^{-1} \epsilon_H L_h^{-2})
%     \end{bmatrix} \nonumber \\
%     = & 
%     \begin{bmatrix}
%         H^{-1} b \\
%         O(L_h^{-2} \epsilon_H^{-1} )
%     \end{bmatrix} \nonumber \\
% \end{align}
% where the second to the last equality uses $\norm{\text{diag}(\lambda_{\mathcal{I}_{\text{incor}}}) h_{\mathcal{I}_{\text{incor}}} H^{-1} h_{\mathcal{I}_{\text{incor}}}^\top} \geq \epsilon C_H^{-1} \norm{\nabla_y h_{\mathcal{I}_{\text{incor}}}}^2 \geq \epsilon \epsilon_H^{-1} L_h^2$ with local $h$ Lipschitzness around $y^*$, where in the linear inequality case $h(x,y) = Ax - By - b$ we can directly compute an upper bound using $\norm{B}$.
% Thus its matrix inversion is lower bounded by $\epsilon^{-1} \epsilon_H L_h^{-2}$.





\section{The role of $\lambda^*(x)$ in the derivative of Equation~\cref{eqn:penalty-lagrangian}}
Notice that Equation~\cref{eqn:penalty-lagrangian}, we treat the dual solution $\lambda^*(x)$ as a constant to define the penalty function derivative. Yet, the dual solution $\lambda^*(x)$ is in fact also a function of $x$. Therefore, in theory, we should also compute its derivative with respect to $x$.

However, notice that the following:
\begin{align}\label{eqn:lambda_derivative}
    \nabla_x (\lambda^*(x))^\top h(x,y) & = \nabla_x h(x,y)^\top \lambda^* + \frac{d \lambda(x)}{dx}^\top h(x,y)
\end{align}
The later term in Equation~\cref{eqn:lambda_derivative} can be divided into two cases:
\begin{itemize}
    \item For active constraint $i \in \mathcal{I}$ with $h(x,y^*) = 0$, we know that $y^*_{\lambda,\alpha}$ is close to $y^*$ by \cref{thm:solution-bound}. Therefore, the derivative $\norm{\frac{d \lambda(x)}{dx}^\top h(x,y^*_{\lambda,\alpha})} \leq L_h L_\lambda \alpha_1 = O(\alpha_1) = O(\alpha^2)$ by the local smoothness of $h$ near $y^*$ and the Lipschitzness assumption of $\lambda^*$ in \cref{item:assumption_safe_constraints}.
    \item For inactive constraint $i \in \bar{\mathcal{I}}$ and $\lambda^*_i > 0$, we can solve the KKT conditions and get $\frac{d \lambda(x)}{dx} = 0$. Therefore, the second term becomes $0$.
    \item For inactive constraint $i \in \bar{\mathcal{I}}$ and $\lambda^*_i = 0$, the KKT system degenerates and we need to use subgradient. By solving the KKT system, we find that $\frac{d \lambda(x)}{dx} = 0$ is a valid subgradient. Therefore, by choosing this subgradient, the second term also vanishes.
\end{itemize}
Therefore, we do not need to compute the derivative of $\lambda^*$ as the terms involved its derivative is negligible compared to other major terms.

\input{src/appendix-experiment}