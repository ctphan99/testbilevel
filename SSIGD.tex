Under review as a conference paper at ICLR 2023

\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{geometry}
\geometry{margin=1in}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

\title{Stochastic Smoothed Implicit Gradient Descent for Linearly Constrained Bilevel Optimization}
\author{Research Paper Summary}
\date{}

\begin{document}
\maketitle

\section{Problem Formulation}

We consider the following stochastic bilevel optimization problem with linear constraints in the lower-level:

\begin{align}
\min_{x \in X} \quad & G(x) := f(x, y^*(x)) := \mathbb{E}_\xi[f_e(x, y^*(x); \xi)] \label{eq:ul}\\
\text{s.t.} \quad & y^*(x) \in \arg\min_{y \in \mathbb{R}^{d_\ell}} \left\{ h(x, y) \mid Ay \leq b \right\} \label{eq:ll}
\end{align}

where $\xi \sim D$ represents a stochastic sample, $X \subseteq \mathbb{R}^{d_u}$ is convex and closed, and $h(x, y)$ is strongly convex with respect to $y$.

\section{Smoothed Problem}

To address non-differentiability issues, we introduce a perturbation-based smoothing technique. For any fixed $x \in X$, we modify the lower-level objective with a linear perturbation term $q^T y$:

\begin{align}
g(x, y) &:= h(x, y) + q^T y\\
y_*^*(x) &:= \arg\min_{y \in \mathbb{R}^{d_\ell}} \{g(x, y) \mid Ay \leq b\}
\end{align}

where $q$ is a random vector sampled from a continuous distribution $Q$.

The smoothed implicit function becomes:

$$
F(x) := f(x, y_*^*(x))
$$

\section{Key Assumptions}

\begin{assumption}[Basic Conditions]
\begin{enumerate}
\item[(a)] $f(x, y)$ is continuously differentiable, and $h(x, y)$ is twice continuously differentiable.
\item[(b)] $X$ is closed and convex; $Y = \{y \in \mathbb{R}^{d_\ell} \mid Ay \leq b\}$ is compact.
\item[(c)] $h(x, y)$ is strongly convex in $y$ with modulus $\mu_h$.
\item[(d)] There exists $y$ such that $Ay < b$ (strict feasibility).
\item[(e)] $A(y^*(x))$ is full row rank for every $x \in X$ (LICQ condition).
\end{enumerate}
\end{assumption}

\begin{assumption}[Approximate Solution Quality]
For the approximate solution $\hat{y}(x)$ of the perturbed lower-level problem:
\begin{enumerate}
\item[(a)] $\|\hat{y}(x) - y_*^*(x)\| \leq \delta$ for $\delta > 0$
\item[(b)] $\hat{y}(x)$ is feasible: $A\hat{y}(x) \leq b$
\item[(c)] $A(y_*^*(x)) = A(\hat{y}(x))$ (same active constraints)
\end{enumerate}
\end{assumption}

\begin{assumption}[Lipschitz Conditions]
For all $x, \bar{x} \in X$ and $y, \bar{y} \in \mathbb{R}^{d_\ell}$:
\begin{enumerate}
\item[(a)] $f$ has bounded gradients: $\|\nabla f(x, y)\| \leq L_f$
\item[(b)] $f$ has Lipschitz continuous gradients: $\|\nabla f(x, y) - \nabla f(\bar{x}, \bar{y})\| \leq L_f \|[x; y] - [\bar{x}; \bar{y}]\|$
\item[(c)] $h$ has Lipschitz continuous gradient in $y$: $\|\nabla_y h(x, y) - \nabla_y h(x, \bar{y})\| \leq L_h \|y - \bar{y}\|$
\item[(d)] $h$ has Lipschitz continuous Hessian in $y$: $\|\nabla^2_{yy} h(x, y) - \nabla^2_{yy} h(x, \bar{y})\| \leq L_{hyy} \|y - \bar{y}\|$
\item[(e)] $h$ has Lipschitz continuous Jacobian: $\|\nabla^2_{xy} h(x, y) - \nabla^2_{xy} h(x, \bar{y})\| \leq L_{hxy} \|y - \bar{y}\|$
\end{enumerate}
\end{assumption}

\begin{assumption}[Stochastic Gradients]
The stochastic gradients are unbiased with bounded variance:
\begin{enumerate}
\item[(a)] $\mathbb{E}_\xi[\nabla f_e(x, y; \xi)] = \nabla f(x, y)$
\item[(b)] $\mathbb{E}_\xi\|\nabla f_e(x, y; \xi) - \nabla f(x, y)\|^2 = \sigma_f^2$
\end{enumerate}
\end{assumption}

\section{Stochastic Implicit Gradient}

The approximate stochastic implicit gradient is computed as:

$$
\widehat{\nabla F}(x; \xi) = \nabla_x f_e(x, \hat{y}(x); \xi) + [\widehat{\nabla y_*^*}(x)]^T \nabla_y f_e(x, \hat{y}(x); \xi)
$$

\begin{lemma}[Properties of Stochastic Gradient]
Under Assumptions 1-4, the stochastic gradient estimate is unbiased and has bounded variance:
\begin{align}
\mathbb{E}_\xi[\widehat{\nabla F}(x; \xi)] &= \widehat{\nabla F}(x)\\
\mathbb{E}_\xi\|\widehat{\nabla F}(x; \xi) - \widehat{\nabla F}(x)\|^2 &\leq \sigma_F^2
\end{align}
where $\sigma_F^2 = 2\sigma_f^2 + 2L_{y^*}^2 \sigma_f^2$.
\end{lemma}

\section{Stochastic Algorithm}

\begin{algorithm}
\caption{Stochastic Smoothed Implicit Gradient Descent (SSIGD)}
\begin{algorithmic}[1]
\REQUIRE Initial parameter $x_0$, iterations $T$, step-sizes $\{\beta_r\}_{r=0}^{T-1}$, accuracy $\delta$
\STATE Sample $q \sim Q$ and perturb lower-level problem
\FOR{$r = 0, 1, \ldots, T-1$}
    \STATE Find approximate solution $\hat{y}(x_r)$ satisfying Assumption 2
    \STATE Compute $\widehat{\nabla F}(x_r; \xi_r)$ using stochastic implicit gradient formula
    \STATE Update: $x_{r+1} = \text{proj}_X(x_r - \beta_r \widehat{\nabla F}(x_r; \xi_r))$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Convergence Analysis}

We provide convergence guarantees for three cases based on the convexity properties of the implicit function $F(x)$.

\subsection{Weakly Convex Case}

\begin{assumption}[Weak Convexity]
For some $\rho > 0$, the implicit function satisfies:

$$
F(z) \geq F(x) + \langle\nabla F(x), z - x\rangle - \frac{\rho}{2}\|z - x\|^2 \quad \forall x, z \in \mathbb{R}^{d_u}
$$

\end{assumption}

\begin{definition}[Moreau Envelope]
Given $\lambda > 0$, the Moreau envelope of $H(x) := F(x) + I_X(x)$ is:

$$
H_\lambda(x) := \min_{z \in X} \left\{ F(z) + \frac{1}{2\lambda}\|x - z\|^2 \right\}
$$

\end{definition}

\begin{theorem}[Weakly Convex Convergence]
Under Assumptions 1-5, with constant step-size $\beta_r = \beta$ and $\hat{\rho} > \frac{3\rho}{2}$, the iterates of SSIGD satisfy:

$$
\frac{1}{T}\sum_{r=0}^{T-1} \mathbb{E}\|\nabla H_{1/\hat{\rho}}(x_r)\|^2 \leq \frac{2\hat{\rho}}{2\hat{\rho} - 3\rho} \left[ \frac{H_{1/\hat{\rho}}(x_0) - H^*}{\beta T} + \beta\hat{\rho}(\sigma_F^2 + L_F^2) + \frac{\hat{\rho}}{2\rho}L_F^2\delta^2 \right]
$$

\end{theorem}

This gives $O(1/\sqrt{T})$ convergence rate with step-size $\beta = O(1/\sqrt{T})$.

\subsection{Strongly Convex Case}

\begin{assumption}[Strong Convexity]
The objective $F(x)$ is $\mu_F$-strongly convex:

$$
F(z) \geq F(x) + \langle\nabla F(x), z - x\rangle + \frac{\mu_F}{2}\|x - z\|^2 \quad \forall z, x \in X
$$

\end{assumption}

\begin{theorem}[Strongly Convex Convergence]
Under Assumptions 1-4 and 6 with $\mu_F > 0$, using step-sizes $\beta_r = \frac{1}{\mu_F(r+1)}$, SSIGD satisfies:

$$
\mathbb{E}[F(\bar{x}) - F^*] \leq \frac{(\sigma_F^2 + L_F^2)}{\mu_F} \frac{\log(T)}{T} + D_X L_F \delta
$$

where $\bar{x}$ denotes the algorithm output.
\end{theorem}

This achieves $O(\log(T)/T)$ convergence rate for strongly convex objectives.

\subsection{Convex Case}

\begin{theorem}[Convex Convergence]
Under Assumptions 1-4 and 6 with $\mu_F = 0$, using constant step-size $\beta_r = \beta$, SSIGD satisfies:

$$
\mathbb{E}[F(\bar{x}) - F^*] \leq \frac{\|x_1 - x^*\|^2}{2\beta T} + 2\beta(\sigma_F^2 + L_F^2) + D_X L_F \delta
$$

where $\bar{x} = \frac{1}{T}\sum_{r=0}^{T-1} x_r$.
\end{theorem}

This gives $O(1/\sqrt{T})$ convergence rate with step-size $\beta = O(1/\sqrt{T})$.

\section{Key Properties}

\begin{proposition}[Approximation Quality]
Under Assumptions 1 and 3, the original and smoothed problems are close:

$$
|G(x) - F(x)| \leq L_f \frac{\|q\|}{\mu_g}, \quad \forall x \in X
$$

\end{proposition}

This shows that by choosing $q$ from a distribution with small support, we can make the smoothed problem arbitrarily close to the original.

\section{Conclusion}

The SSIGD algorithm provides the first gradient-based approach for bilevel optimization with linearly constrained lower-level problems. Key contributions include:

\begin{itemize}
\item Perturbation-based smoothing technique ensuring almost-sure differentiability
\item Closed-form expression for the approximate implicit gradient
\item Convergence guarantees for weakly convex, convex, and strongly convex cases
\item Rates of $O(1/\sqrt{T})$, $O(1/\sqrt{T})$, and $O(\log(T)/T)$ respectively
\end{itemize}

The algorithm is suitable for machine learning applications and can handle stochastic objectives, making it more practical than value function-based approaches.

\end{documen

LINEARLY CONSTRAINED BILEVEL OPTIMIZATION: A

SMOOTHED IMPLICIT GRADIENT APPROACH

Anonymous authors

Paper under double-blind review

ABSTRACT

This work develops an analysis and algorithms for solving a class of bilevel optimization problems where the lower-level \(LL\) problems have linear constraints.

Most of the existing approaches for constrained bilevel problems rely on value function based approximate reformulations, which suffer from issues such as nonconvex and non-differentiable constraints. In contrast, in this work, we develop an implicit gradient-based approach, which is easy to implement, and is suitable for machine learning applications. We first provide an in-depth understanding of the problem, by showing that the implicit objective for such problems is in general non-differentiable. However, if we add some small \(linear\) perturbation to the LL objective, the resulting implicit objective becomes differentiable almost surely.

This key observation opens the door for developing \(deterministic and stochastic\) gradient-based algorithms similar to the state-of-the-art ones for unconstrained bi-level problems. We show that when the implicit function is assumed to be strongly-convex, convex and weakly-convex, the resulting algorithms converge with guaranteed rate. Finally, we experimentally corroborate the theoretical findings and evaluate the performance of the proposed framework on numerical and adversarial learning problems. To our knowledge, this is the first time that \(implicit\) gradient-based methods have been developed and analyzed for the considered class of bilevel problems.

1

INTRODUCTION

Bilevel optimization problems \(Colson et al., 2005; Dempe & Zemkoho, 2020\) can be used to model an important class of hierarchical optimization tasks with two levels of hierarchy, the upper-level \(UL\) and the lower-level \(LL\). The key characteristics of bilevel problems are: 1\) the solution of the UL problem requires access to the solution of the LL problem and, 2\) the LL problem is parametrized by the UL variable. Bilevel optimization problems arise in a wide range of machine learning applications, such as meta-learning \(Rajeswaran et al., 2019; Franceschi et al., 2018\), data hypercleaning \(Shaban et al., 2019\), hyperparameter optimization \(Sinha et al., 2020; Franceschi

et al., 2018; 2017; Pedregosa, 2016\), adversarial learning \(Li et al., 2019; Liu et al., 2021a; Zhang

et al., 2021\), as well as in other application domains such as network optimization \(Migdalas, 1995\),

economics \(Cecchini et al., 2013\), and transport research \(Didi-Biha et al., 2006; Kalashnikov et al.,

2010\). In this work, we focus on a special class of stochastic bilevel optimization problems, where the LL problem involves the minimization of a strongly convex objective over a set of linear inequality constraints. More precisely, we consider the following formulation:

n

o

min G\(x\) := f \(x, y∗\(x\)\) := Eξ\[ e

f \(x, y∗\(x\); ξ\)\] ,

\(1a\)

x∈X

n

o

s.t. y∗\(x\) ∈ arg min h\(x, y\) Ay ≤ b ,

\(1b\)

y∈ d

R ℓ

where ξ ∼ D represents a stochastic sample of the objective f \(·, ·\), X ⊆

d

R u is a convex and closed

set, f : X × d

d

R ℓ → R is the UL objective, h : X × R ℓ → R is the LL objective, and f, h are smooth functions. We focus on the problems where h\(x, y\) is strongly convex with respect to y. The matrix A ∈

k×d

k

R

ℓ , and vector b ∈ R define the linear constraints. In the following, we refer to \(1a\) as the UL problem, and to \(1b\) as the LL one.

1

Under review as a conference paper at ICLR 2023

The success of the bilevel formulation and its algorithms in many machine learning applications can be attributed to the use of the efficient \(stochastic\) gradient-based methods \(Liu et al., 2021a\). These methods take the following form, in which an \(approximate\) gradient direction of the UL problem is computed \(using chain rule\), and then the UL variable is updated using gradient descent \(GD\): b

∇G\(x\) ≈ ∇xf \(x, y∗\(x\)\) \+ \[∇y∗\(x\)\]T ∇yf \(x, y∗\(x\)\) GD Update:x\+ = x − β b

∇G\(x\). \(2\)

The gradient of G\(x\) is often referred as the implicit gradient. However, computing this implicit gradient not only requires access to the optimal y∗\(x\), but also assumes differentiability of the mapping y∗\(x\) : X →

d

R ℓ . One can potentially solve the LL problem approximately and obtain an approximation y\(x\) such that y\(x\) ≈ y∗\(x\), and use it to compute the implicit gradient \(Ghadimi &

b

b

Wang, 2018\). Unfortunately, not all solutions y∗\(x\) are differentiable, and when they are not the above approach cannot be applied.

It is known that when the LL problem is strongly convex and unconstrained, then ∇y∗\(x\) can be easily evaluated using the implicit function theorem \(Ghadimi & Wang, 2018\). This is the reason that the majority of recent works have focused on developing algorithms for the class of unconstrained bilevel problems \(Ghadimi & Wang, 2018; Hong et al., 2020; Ji et al., 2021; Khanduri et al., 2021b;

Chen et al., 2021a\). However, when the LL problem is constrained, ∇y∗\(x\) might not even exist. In that case, most works adopt a value function-based approach to solve problems with LL constraints

\(Liu et al., 2021b; Sow et al., 2022; Liu et al., 2021c\). Value-function-based methods typically transform the original problem into a single-level problem with non-convex and non-differentiable constraints. To resolve the latter issue these approaches regularize the problem by adding a strongly-convex penalty term, altering the problem’s structure. In contrast, we introduce a perturbation-based smoothing technique, which at any given x ∈ X makes y∗\(x\) differentiable almost surely, without practically changing the landscape of the original problem \(see \(Lu et al., 2020, pg. 5\)\). It is important to note that the value function-based approaches are more suited for deterministic implementations, and therefore it is difficult to use such algorithms for large scale applications and/or when the data sizes are large. On the other hand, the gradient-based algorithms developed in our work can easily handle stochastic problems. Finally, there is a line of work \(Amos & Kolter, 2017; Agrawal et al.,

2019; Donti et al., 2017; Gould et al., 2021\) about implicit differentiation in deep learning literature.

However, in these works the setting \(e.g. layers of neural network described by optimization tasks\) and the focus \(e.g., on gradient computation and implementation, rather than on algorithms and analysis\) is different. For more details see Appendix A.

Contributions. In this work, we study a class of bilevel optimization problems with strongly convex objective and linear constraints in the LL. Major challenges for solving such problems are the following: 1\) How to ensure that the implicit function G\(x\) is differentiable? and 2\) Even if the implicit function is differentiable, how to compute its \(approximate\) gradient in order to develop first-order methods? Our work addresses these challenges and develops first-order methods to tackle such constrained bilevel problems. Specifically, our contributions are the following:

– We provide an in-depth understanding of bilevel problems with strongly convex linearly constrained LL problems. Specifically, we first show with an example that the implicit objective G\(x\) is in general non-differentiable. To address the non-differentiability, we propose a perturbation-based smoothing technique that makes the implicit objective G\(x\) differentiable in an almost sure sense, and we provide a closed-form expression for the \(approximate\) implicit gradient.

– The smoothed problem we obtain is challenging, since its implicit objective does not have Lipschitz continuous gradients. Therefore, conventional gradient based algorithms may no longer work. To address this issue, we propose the Deterministic Smoothed Implicit Gradient \(\[D\]SIGD\) method that utilizes an \(approximate\) line search-based algorithm and establish asymptotic convergence guarantees. We also analyze \[S\]SIGD for the stochastic version of problem \(1\) \(with fixed/diminishing step-sizes\) and establish finite-time convergence guarantees for the cases when the implicit function is weakly-convex, strongly-convex, and convex \(but not Lipschitz smooth\).

– Finally, we evaluate the performance of the proposed algorithmic framework via experiments on quadratic bilevel and adversarial learning problems.

Bilevel problem 1 captures several important applications. Below we provide two such applications.

Adversarial Training. The problem of robustly training a model ϕ\(x; c\), where x denotes the model parameters and c the input to the model; let \{\(c

dℓ

i, di\)\}N

with c

i , d

i=1

i ∈ R

i ∈ R be the training set

\(Zhang et al., 2021; Goodfellow et al., 2014\). It can be formulated as the following bilevel problem: 2

Under review as a conference paper at ICLR 2023



N

N



X

n X

o

arg min

hi\(ϕ\(x; ci \+ yi\), di\)

min

fi\(ϕ\(x; ci \+ y∗\(x\)\), d

s.t. y∗\(x\) ∈

,

i

i\)

d

\(3\)

ℓ

x∈

y

i

i=1

Rdu

i ∈R

i=1



s.t. − b ≤ y ≤ b

where y = \[yT , . . . , yT \]T ∈

dℓ

dℓ

1

; with y

i

denotes the attack on the ith example and we

N

R

i ∈ R

have PN d

= d

i=1

ℓi

ℓ. Moreover, fi : R × R → R denotes the loss function for learning the model parameter x, while hi : R × R → R denotes the adversarial objective used to design the optimal attack y. Note that the linear constraints in the LL problem −b ≤ y ≤ b models the attack budget.

Distributed Optimization. In distributed optimization \(Chang et al., 2020; Yang et al., 2019\), a set of N agents aim to jointly minimize an objective function G\(x\) over an undirected graph G = \(V, E\).

We consider the following distributed bilevel problem

N

N

n

X

o

n X

o

min

G\(x\) :=

fi\(xi, y∗\(x

h

,

i

i\)\)

s.t. y∗\(x\) ∈ arg min

i\(xi, yi\) s.t. Ay = 0

\{xi∈X |Ax=0\}

d

i=1

y∈R ℓ

i=1

where x = \[x1, . . . , xN \] and y = \[y1, . . . , yN \]. Each agent i ∈ \[N \] has access to fi and hi. The constraint Ay = 0 \(resp. Ax = 0\) is introduced to ensure the consensus of LL \(resp. UL\) variables.

Such problems arise in signal processing and sensor networks \(Yousefian, 2021\). This formulation also models a decentralized meta learning problem where the training and validation data is distributed among agents while each agent aims to solve the meta learning problem globally \(Ji et al., 2021\).

2

PROPERTIES AND IMPLICIT GRADIENT OF BILEVEL PROBLEM \(1\)

2.1

PRELIMINARIES

In this section we study the properties of problem 1. First, let us define the necessary notations. Let A\(y\) be the matrix that contains the rows S\(y\) ⊆ \{1, . . . , k\} of A that correspond to the active constraints of inequality Ay ≤ b in the LL problem, that is we have A\(y\)y = b\(y\), where b\(y\)

∗

contains the elements of b with indices in S\(y\). Also, we denote with λ \(x\) the Lagrange multipliers vector that corresponds to the active constraints at y∗\(x\) Next, we introduce some basic assumptions.

Assumption 1. We assume that the following conditions hold for problem \(1\): \(a\) f \(x, y\) is continuously differentiable, and h\(x, y\) is twice continuously differentiable.

\(b\) X is closed and convex; Y = y ∈

d

R ℓ Ay ≤ b is a compact set.

\(c\) h\(x, y\) is strongly convex in y, for every x ∈ X , with modulus µh.

\(d\) There exists y ∈

d

R ℓ such that Ay < b.

\(e\) A\(y∗\(x\)\) is full row rank, for every x ∈ X 1.

The Assumptions 1\(a\), \(b\) and \(c\) are standard assumptions in bilevel optimization literature and are required to ensure the continuity of the implicit function \(Proposition 1\). Assumption 1\(c\) ensures that the implicit function G\(x\) is well defined as the LL problem returns a single point. Assumption

1\(d\) ensures strict feasibility of the LL problem, while Assumption 1\(e\) implies that the rows of A corresponding to the active constraints are linearly independent. Note that this assumption is necessary to ensure the differentiability of the implicit function \(Lemma 1,2\). Also note that there are some special cases in which Assumption 1\(e\) is automatically satisfied. For instance, consider a problem where the LL problem has box constraints, i.e., a ≤ y ≤ b. Then for any y ∈ Y the only possible non-zero values in the matrix A\(y\) are \+1, −1, and there is only one non-zero value at each column. Therefore, A\(y\) is full row rank. Next, we utilize the above assumptions to analyze the properties of mapping y∗\(x\).

Proposition 1 \(Appendix D.1.1\). Under Assumption 1, the mapping y∗\(x\) : X →

d

R ℓ and the

implicit function G\(x\) are both continuous.

Proposition 1 ensures that y∗\(x\) and G\(x\) are both continuous. Now if we can ensure differentiability of y∗\(x\), then we should be able to implement a gradient-based update rule to solve \(1\). However, as the following example illustrates, y∗\(x\) and thus G\(x\) are not differentiable in general.

1This is the LICQ condition \(Bertsekas, 1998\) of the LL problem. It is used to ensure that the optimal solutions satisfy the KKT conditions.

3

Under review as a conference paper at ICLR 2023

Example. Consider the following problem

p

min x \+ y∗\(x\) s.t. y∗\(x\) ∈ arg min \(y2 − x2\)2

3/5 ≤ y ≤ 1 .

\(4\)

x∈\[0,1\]

y∈R

Original Problem

The mapping y∗\(x\) is y∗\(x\) = x, if x ∈ \[p3/5, 1\], and y∗\(x\) =

0.84

p3/5, if x ∈ \[0, p3/5\). In Figure 1, we plot this mapping. Notice 0.82

\( *x*\) *y*

that at the point x = p3/5 the mapping \(and thus the implicit

0.80

function\) is non-differentiable.

0.78

0.70

0.75

0.80

0.85

To address the non-differentiability issue, we introduce a

x

perturbation-based "smoothing" technique. Specifically, for any Figure 1: Plot of y∗\(x\).

fixed x ∈ X , we modify the LL objective h\(x, y\) with the addition

of the linear perturbation term qT y, where q is a random vector sampled from some continuous distribution Q. We use the following notation for the “smoothed” LL objective g\(x, y\) := h\(x, y\) \+ qT y and y∗\(x\) := arg min\{g\(x, y\) | Ay ≤ b\}.

\(5\)

y∈ d

R ℓ

Also, we denote F \(x\) := f \(x, y∗\(x\)\) as the respective “smoothed” implicit function. Such a perturbation is used to ensure that at a given x ∈ X , the strict complementarity \(SC\) property holds for the LL problem with probability 1 \(w.p. 1\); see the lemma below for the formal statement.

Lemma 1. \(\(Lu et al., 2020, Proposition 1\)\) For a given x ∈ X , if y∗\(x\) is a KKT point of problem min

g\(x, y\), q is generated from a continuous measure, and A\(y∗\(x\)\) is full row rank, then y∈ d

R ℓ

the SC condition holds at x, with probability 1 \(w.p. 1\).

Combining SC ensured by Lemma 1 with Assumption 1, we can show that the implicit mapping y∗\(x\) is \(almost surely\) differentiable, which further implies that the implicit function F \(x\) is differentiable at a given x ∈ X , and obtain a closed-form expression for the gradient \(Lemma 2\). We would like to stress that the properties mentioned above \(i.e., SC and differentiability\) are defined locally, at a given point x ∈ X . These properties will be used later to design algorithms that approximately optimize the original problem \(1\). Finally, it is worth noting that, in the absence of such a perturbation term, we would have to introduce the SC property as an assumption.

2.2

IMPLICIT GRADIENT

In this section, we derive a closed-form expression for the gradient of the implicit function F \(x\).

Lemma 2 \(Implicit Gradient, Appendix D.1.2\). Under Assumption 1, for any given x ∈ X , we have T

∗

∇y∗\(x\) = ∇2 g\(x, y∗\(x\)\)−1 − ∇2 g\(x, y∗\(x\)\) − A ∇λ \(x\)

yy

xy

\(6\)

∗

T

∇λ \(x\) = −A∇2 g\(x, y∗\(x\)\)−1A −1A∇2 g\(x, y∗\(x\)\)−1∇2 g\(x, y∗\(x\)\),

yy

yy

xy

\(7\)

where we set A := A\(y∗\(x\)\).

Note that when LL problem \(1b\) does not have the LL constraints, the implicit gradient derived in Lemma 2 becomes exactly same as the one in Ghadimi & Wang \(2018\); Ji et al. \(2021\). Moreover, if the LL problem has only linear equality constraints, the differentiability of y∗\(x\) follows from the implicit function theorem under Assumptions 1\(a\) and 1\(c\) along with full row rankness of A. In fact, the expression of the implicit gradient stays the same as in Lemma 2 with A and λ∗\(x\) replaced by A and λ∗\(x\), respectively \(i.e., we use the full matrix A\). Finally, using Lemma 2 above we now have an expression of the implicit gradient as

∇F \(x\) = ∇xf \(x, y∗\(x\)\) \+ \[∇y∗\(x\)\]T ∇yf \(x, y∗\(x\)\).

\(8\)

2.2.1

APPROXIMATE IMPLICIT GRADIENT

Note that computing ∇F \(x\) requires the precise knowledge of y∗\(x\) which is not possible for many problems of interest. Therefore, in practice we define the approximate implicit gradient as b

∇F \(x\) = ∇xf \(x, y\(x\)\) \+ \[ b

∇y∗\(x\)\]T ∇

y\(x\)\),

\(9\)

b

y f \(x, b

4

Under review as a conference paper at ICLR 2023

where b

∇y∗\(x\) is defined by setting the approximate LL solution y\(x\) in place of the exact one y∗\(x\) b

in expressions \(6\) and \(7\). In order to ensure that \(9\) returns a useful approximation of the \(exact\) implicit gradient, we impose a few assumptions on the quality of the estimate y\(x\).

b

Assumption 2. The approximate solution of \(perturbed\) LL problem \(5\) y\(x\) satisfies the following b

∀x ∈ X :

\(a\) ∥y\(x\) − y∗\(x\)∥ ≤ δ for δ > 0,

b

\(b\) y\(x\) is a feasible point, i.e., Ay\(x\) ≤ b,

b

b

\(c\) It holds that A\(y∗\(x\)\) = A\(y\(x\)\).

b

The LL problem requires the solution of a strongly convex linearly constrained task. As a result,

Assumptions2\(a\),\(b\) can be easily satisfied. Specifically, we can obtain approximate feasible solutions of given accuracy with known methods, such as projected gradient descent, or by using some convex optimization solver; in section B of the Appendix we provide one such method. Moreover, Assumption 2\(c\) will be satisfied if we find a “sufficiently accurate” solution y\(x\). Specifically, b

from Calamai & Moré \(1987, Theorem 4.1\) we know that if yk\(x\) ∈ Y is an arbitrary sequence that b

converges to a non-degenerate \(i.e., Assumption 1\(e\) and SC holds\) stationary solution y∗\(x\), then there exists an integer k0 such that A\(y∗\(x\)\) = A\(yk\(x\)\), ∀k > k

b

Remark 1. There are certain special cases where we can obtain an upper bound for k0. For instance, in the case of non-negative constraints y ≥ 0 it can be shown2 that Lh log 2L

µ

h∥y0 − y∗\(x\)∥/τ

h

iterations of the projected gradient descent method suffice to ensure that the active set of the approximate solution y\(x\) coincides with the active set of the exact one y∗\(x\) \(see Nutini et al. \(2019,

b

Corollary 1\)\), where τ = mini∈S\(y∗\(x\)\) ∇y g\(x, y∗\(x\)\) and y0 is the algorithm’s initialization. A i

similar result can be derived for the case with bound constraints a ≤ y ≤ b.

Next, we introduce additional assumptions that are required to analyze the properties of \(9\).

Assumption 3. We assume that the following holds for problem \(1\), ∀x, x ∈ X and y, y ∈

d

R ℓ :

\(a\) f has bounded gradients, i.e., ∥∇f \(x, y\)∥ ≤ Lf .

\(b\) f has Lipschitz continuous gradients, i.e., ∥∇f \(x, y\) − ∇f \(x, y\)∥ ≤ Lf ∥\[x; y\] − \[x; y\]∥.

\(c\) h has Lipschitz continuous gradient in y, i.e., ∥∇yh\(x, y\) − ∇yh\(x, y\)∥ ≤ Lh∥y − y∥.

\(d\) h has Lipschitz continuous Hessian in y, i.e., ∥∇2 h\(x, y\) − ∇2 h\(x, y\)∥ ≤ L

∥y − y∥

yy

yy

h

.

yy

\(e\) h has Lipschitz continuous Jacobian, i.e., ∥∇2 h\(x, y\) − ∇2 h\(x, y\)∥ ≤ L

∥y − y∥

xy

xy

h

.

xy

\(f\) h has a bounded Jacobian, ∥∇2 h\(x, y\)∥ ≤ L

xy

h

.

xy

Assumption 3 is standard in bilevel optimization literature \(Ghadimi & Wang, 2018; Hong et al., 2020;

Chen et al., 2021a; Ji et al., 2021\) and is used to derive some useful properties of the \(approximate\) implicit gradient \(Lemma 3, Appendix D.1.3\). It is easy to see that Assumptions 1\(a\),\(c\) and 3 hold directly for the perturbed objective \(5\) with constants µg = µh, Lh = Lg, Lg

= L

, L

=

yy

hyy

gxy

Lh , L

= L

; we also assume that Assumption 1\(e\) holds for the perturbed LL problem \(5\).

xy

gxy

hxy

Lemma 3 \(Appendix D.1.3\). Suppose that Assumptions 1,2,3 hold. Then, for every x ∈ X the following holds

∥ b

∇F \(x\) − ∇F \(x\)∥ ≤ LF · δ,

∥∇F \(x\)∥ ≤ LF

and

∥ b

∇F \(x\)∥ ≤ LF ,

where L

F = Lf \+ Ly∗ Lf \+ Lf Ly∗ , and LF =

1 \+ Ly∗ Lf ; the constants Ly∗ , Ly∗ are defined

in Lemmas 7,9, respectively, provided in the Appendix.

2.2.2

STOCHASTIC IMPLICIT GRADIENT

In the stochastic setting, the \(approximate\) stochastic implicit gradient is computed as: b

∇F \(x; ξ\) = ∇x e

f \(x, y\(x\); ξ\) \+ \[ b

∇y∗\(x\)\]T ∇

y\(x\); ξ\).

\(10\)

b

y e

f \(x, b

Also, we make the following assumption on the stochastic gradients of the UL problem.

Assumption 4. We assume that the stochastic gradients are unbiased, i.e. Eξ\[∇ e f \(x, y; ξ\)\] =

∇f \(x, y\) and have bounded variance, i.e., Eξ∥∇ e

f \(x, y; ξ\) − ∇f \(x, y\)∥2 = σ2 for some σ

f

f > 0.

2Under the assumption that ∇y g\(x, y∗\(x\)\) > 0, ∀i ∈ S\(y∗\(x\)\).

i

5

Under review as a conference paper at ICLR 2023

Algorithm 1 \[Deterministic\] Smoothed Implicit Gradient Descent \(\[D\]SIGD\) 1: Input: Initial parameter x0, \# of iteration T , LL solution accuracy, δr, σ, measure Q, s 2: Sample q ∼ Q and perturb LL problem

3: for r = 0, 1, . . . , T − 1 do

4:

Find an approximate solution y\(xr\) s.t. Assumption 2 is satisfied.

b

5:

Compute b

∇F \(xr\) using \(9\), b

dr = xr − xr with xr = proj

e

e

X \(xr − s b

∇F \(xr\)\)

6:

Select ar s.t. the following Armijo-type rule condition is satisfied b

F \(xr\) − b

F \(xr \+ ar b

dr\) ≥ −σ · ar\[ b

∇F \(xr\)\]T b

dr − ϵ\(δ; r\)

\(11\)

where ϵ\(δ; r\) depends on δr, αr and problem-dependent parameters; b

F \(·\) = f \(·, y\(·\)\).

b

7:

Perform one projected gradient step: xr\+1 = xr \+ ar · b

dr

8: end for

Algorithm 2 \[Stochastic\] Smoothed Implicit Gradient Descent \(\[S\]SIGD\) 1: Input: Initial parameter x0, \# of iterations T , step-sizes \{βr\}T −1, LL solution accuracy δ

r=0

2: Sample q ∼ Q and perturb LL problem

3: for r = 0, 1, . . . , T − 1 do

4:

Find an approximate solution y\(xr\) s.t. Assumption 2 is satisfied.

b

5:

Compute b

∇F \(xr; ξr\) using \(10\)

6:

Perform one stochastic projected gradient descent step: xr\+1 = projX \(xr − βr b

∇F \(xr; ξr\)\)

7: end for

Assumption 4 is a typical assumption required to ensure that the approximate implicit stochastic gradient is also unbiased and has finite variance \(Ghadimi & Wang, 2018; Hong et al., 2020; Chen

et al., 2021a\) as shown in Lemma 4 below.

Lemma 4 \(Appendix D.1.4\). Under Assumptions 1,2,3 and 4, the stochastic gradient estimate in

\(10\) is unbiased, i.e., Eξ\[ b

∇F \(x; ξ\)\] = b

∇F \(x\) and has bounded variance , i.e., Eξ∥ b

∇F \(x; ξ\) −

b

∇F \(x\)∥ ≤ σ2 where σ2 = 2σ2 \+ 2L

; where L

F

F

f

y∗ σ2

f

y∗ is defined in Lemma 7 in the Appendix.

3

THE SIGD ALGORITHMS AND CONVERGENCE ANALYSIS

3.1

THE PROPOSED ALGORITHMS

In this section, we develop gradient-based methods for solving problem \(1\) by leveraging the smoothing based technique introduced in the previous section. Recall that for any x ∈ X , we can introduce a perturbation to make the optimal solution y∗\(x\) of the perturbed LL problem differentiable. Next, to proceed with the algorithm design, there are two options available. First, generate a perturbation for each x ∈ X encountered in the algorithm. Second, generate a single perturbation at the beginning of the algorithm, and use it throughout the execution of the algorithm.

It is worth mentioning that, both approaches perform equally well in our numerical experiments.

However, for the ease of analysis, we adopt the second approach. To justify such an approach, below we show that, just sampling a single q is suffice to make F \(x\) differentiable \(almost surely\) at a sequence of countable points.

Lemma 5 \(Appendix D.2.1\). Let \{xr\}∞

∈ X be an arbitrary sequence. Consider the implicit

r=0

function F \(x\) := f \(x, y∗\(x\)\), where y∗\(x\) is defined in \(5\), where a single perturbation q is used.

Then F \(·\) is differentiable at all the points \{xr\}∞

w.p. 1.

r=0

Due to this result, in the following analysis, we assume that the almost sure differentiability will also be satisfied for the iterates generated by our algorithms as suggested by Lemma 5. Further, our algorithm design is guided by the fact that, unlike bilevel programs with unconstrained LL tasks \(see Lemma 2.2\(c\) in Ghadimi & Wang \(2018\)\), the implicit gradient ∇F \(x\) in \(8\) is not Lipschitz smooth in general. This implies that algorithms that provably converge only under the Lipschitz assumption, will not work in our case, particularly when the implicit function is non-convex. Towards this end, we propose the \[Deterministic\] Smoothed Implicit Gradient Descent \(\[D\]SIGD\) method 6

Under review as a conference paper at ICLR 2023

\(Alg. 1\), a determinstic line-search-based method, which does not require Lipschitz smoothness or another special structure \(e.g., convexity\), and show asymptotic convergence \(Theorem 1\). Moreover, for the cases where the implicit function is weakly-convex, convex or strongly convex \(but still not Lipschitz smooth\) the \[Stochastic\] Smoothed Implicit Gradient Descent \(\[S\]SIGD\) method \(Alg. 2\)

is developed, a stochastic gradient-based method, for which finite-time convergence guarantees are derived \(Theorem 2,3,4\).

3.2

CONVERGENCE ANALYSIS

As discussed above, in the context of algorithm design and analysis we sample a single perturbation q, and keep it fixed during the algorithm execution. As a result, the algorithm is effectively optimizing the following smooth surrogate of the original problem \(1\):

n

o

min F \(x\) = f \(x, y∗\(x\)\) = Eξ\[ e

f \(x, y∗\(x\); ξ\)\]

\(12a\)

x∈X

n

o

s.t. y∗\(x\) ∈ arg min g\(x, y\) = h\(x, y\) \+ qT y Ay ≤ b ,

\(12b\)

y∈ d

R ℓ

where q ∈

d

R ℓ is generated from a continuous measure only once and thus is considered fixed.

Next, we show that the original problem \(1\) and the smoothed surrogate problem \(12\) are “close”.

Specifically, we show below that the original implicit function G\(x\) and the "smoothed" implicit function F \(x\) differ by a quantity that is controlled by the size of the perturbation vector q.

Proposition 2

∥

\(Appendix D.2.2\). Under Ass. 1 and 3, we have: |G\(x\) − F \(x\)| ≤ L

q∥

f

, ∀ x ∈ X .

µg

Note that the only requirement on q is that it is generated from a continuous measure. Therefore we can always choose a distribution such that ∥q∥ is arbitrarily small. Next, let us analyze Alg. 1. We have the following asymptotic result.

Theorem 1 \(Appendix D.2.3\). Suppose that Ass. 1, 3 hold. At each iteration r of Alg. 1

we find 0 < ar < 1 such that the Armijo-type condition \(11\) is satisfied with ϵ\(δ; r\) =

Lf δr \+ LF LF arδr \+ Lf δr\+1 \+ L2 σar \(δr\)2 \+ 2L

F

F LF σar δr . Further, we select δr such that

Ass. 2 is satisfied, limr→∞ δr = 0, and it holds that δr/ar ∼ O \(cr\), where cr is some sequence with limr→∞ cr = 0. In addition, the sequence b

dr is selected such that it is gradient related to b

∇F \(xr\),

i.e., “for any subsequence \{xr\}r∈R converging to a non-stationary point, the corresponding subse-h

iT

quence \{b

dr\}r∈R is bounded and satisfies lim supr→∞,r∈R b

∇F \(xr\)

b

dr < 0” \(Bertsekas, 1998,

eq. 1.13\). Then w.p. 1 the limit point ¯

x of the sequence of iterates generated by the \[D\]SIGD Alg. 1

is a stationary point.

Note that in Theorem 1 only asymptotic convergence is guaranteed. However, this is the best we can do since we do not impose any Lipschitz smoothness or convexity assumptions. On the other hand, in the special cases where the implicit function is weakly-convex, strongly convex or convex \(but still not Lipschitz smooth\), it is possible to derive finite-time convergence guarantees as presented next.

Towards this end, we need to impose the additional assumption that the set X is bounded; combining this property with Assumption 1 implies that X is compact. So, in the following results we assume that X is a compact set with diameter DX := supx,¯x∈X ∥x − ¯

x∥.

Weakly Convex Objective. We make the following assumption on the implicit function F \(·\).

Assumption 5. We assume that for some ρ > 0 the implicit function F \(x\) satisfies: F \(z\) ≥

F \(x\) \+ ⟨∇F \(x\), z − x⟩ − ρ ∥z − x∥2 ∀ x, z ∈

du .

2

R

Assumption 5 implies that the function F \(x\) \+ ˆ

ρ ∥x∥2 for ˆ

ρ = ρ is convex while for ˆ

ρ > ρ is

2

strongly convex with modulus ˆ

ρ − ρ. Many problems of practical interest satisfy the weak-convexity, for example, phase retrieval \(Davis et al., 2020\), covariance matrix estimation \(Chen et al., 2015\),

dictionary learning \(Davis & Drusvyatskiy, 2019\), Robust PCA \(Candès et al., 2011\) etc. \(please see

\(Davis & Drusvyatskiy, 2019\) and \(Drusvyatskiy, 2017\) for more details\). For providing guarantees for the \[S\]SIGD algorithm we utilize a Moreau envelope based analysis. For this purpose, we first rephrase the UL problem as an unconstrained one: minx∈

H\(x\) := F \(x\) \+ I

Rdu

X \(x\), where IX \(x\)

is the indicator function of set X defined as: IX \(x\) := 0 if x ∈ X and IX \(x\) := ∞ if x /

∈ X . Below

we define the Moreau envelope of H\(x\).

7

Under review as a conference paper at ICLR 2023

Definition 1. Given λ > 0, the Moreau envelope of H\(x\) is defined as n

1

o

n

1

o

Hλ\(x\) := min

H\(z\) \+

∥x − z∥2

= min F \(z\) \+

∥x − z∥2 ,

z∈Rdu

2λ

z∈X

2λ

where the second equality follows from the definition of H\(x\). Moreover, we denote the proximal map of H\(x\) as ˆ

x := prox

\(x\) which is defined as

λH

n

1

o

n

1

o

ˆ

x := arg min H\(z\) \+

∥x − z∥2

= arg min F \(z\) \+

∥x − z∥2 .

z∈

2λ

2λ

Rdu

z∈X

The norm of the gradient of the Moreau envelope satisfies the following:

∥x − ˆ

x∥ = λ∥∇Hλ\(x\)∥,

H\(ˆ

x\) ≤ H\(x\),

and

dist\(0; ∂H\(ˆ

x\)\) ≤ ∥∇Hλ\(x\)∥,

\(13\)

where dist\(0; ∂H\(ˆ

x\)\) = − infv:∥v∥≤1 H′\(x; v\) and H′\(x; v\) denotes the directional derivative of H at x in direction v. Note that a small gradient ∥∇Hλ\(x\)∥ implies that x is near some point ˆ

x that

is nearly stationary \(Davis & Drusvyatskiy, 2019\). Then we have the following result.

Theorem 2 \(Appendix D.2.4\). Under Ass. 1, 2, 3, 4 and 5, with step-sizes βr = β for all r ∈

\{0, . . . , T − 1\} and for any constant ˆ

ρ > 3ρ , the iterates generated by Algorithm 2 satisfy \(w.p. 1\) 2

T −1

1

H

X

2 ˆ

ρ

1/ ˆ

ρ\(x0\) − H ∗

2

ˆ

ρ

E∥∇H

\+ β ˆ

ρ σ2 \+ L

\+

L2 δ2 .

T

1/ ˆ

ρ\(xr \)∥2 ≤ 2ˆρ − 3ρ

βT

F

F

2ρ F

r=0

√

Theorem 2 implies that with the choice of β = O\(1/ T \), the \[S\]SIGD algorithm converges to a

√

stationary point at a rate of O\(1/ T \) with an additive error determined by the accuracy of the LL

problem’s solution δ \(see Assumption 2\).

Strongly Convex and Convex Objective. Next, we provide the guarantees for the case when the implicit function is strongly convex. We make the following assumption.

Assumption 6. We assume that the objective F \(x\) is µF -strongly convex, i.e., F \(z\) ≥ F \(x\) \+

⟨∇F \(x\), z − x⟩ \+ µF ∥x − z∥2 ∀z, x ∈ X . Note that for µ

2

F = 0, the objective becomes convex.

Theorem 3 \(Appendix D.2.5\). Under the Assumptions 1, 2, 3, 4 and 6, with µF > 0 and the choice of step-sizes βr =

1

the iterates generated by Algorithm 2 satisfy the following \(w.p. 1\), µF \(r\+1\)

2

\(σ2 \+ L \) log\(T \)

F

F

E\[F \(x\) − F ∗\] ≤

\+ DX LF δ.

µF

T

Theorem 4 \(Appendix D.2.6\). Under Assumption 1, 2, 3, 4 and 6, with µF = 0, and step-sizes βr = β for r ∈ \{0, . . . , T − 1\}, the iterates generated by Algorithm 2 satisfy the following \(w.p. 1\),

∥

T −1

x1 − x∗∥2

2

1 X

E\[F \(x\) − F ∗\] ≤

\+ 2β\(σ2 \+ L \) \+ DX LF δ,

where x =

xr.

βT

F

F

T r=0

The results of Theorems 3 and 4 imply that the implicit function F \(x\) converges to the optimal value at a rate of O\(1/T \) for strongly-convex objectives with diminishing step-sizes, and at a rate

√

√

of O\(1/ T \) for convex objectives with β = O\(1/ T \). Note that convergence is shown to a neighborhood of the optimal solution where its size is determined by the size of the LL error δ.

4

EXPERIMENTS

In this section, we evaluate the performance of Algorithms 1 and 2 via numerical experiments. First, we compare the performance of \[D\]SIGD to the recently proposed PDBO \(Sow et al., 2022\) for constrained bilevel optimization on a quadratic bilevel problem. Then in the second set of experiments, we evaluate the performance of \[S\]SIGD against popular adversarial training algorithms.

Quadratic Bilevel Optimization. Consider the quadratic bilevel problem of the form \(1\) with 1

1

1

f \(x, y\) =

∥x∥2 \+ 10xT y −

∥y∥2 \+ 1T x \+ 1T y \+ 1 and h\(x, y\) = xT y \+

∥y∥2 \+ x1 \+ y2, \(14\)

4

4

2

8

Under review as a conference paper at ICLR 2023

and linear constraints of the form |yi| ≤ 1, i ∈ \{1, 2\}.

20

\[S\]SIGD

Here, x = \[x1, x2\]T , y = \[y1, y2\]T with xi, yi ∈ R for

\[D\]SIGD

i ∈ \{1, 2\}

PDBO

, and 1 = \[1, 1\]T . The evaluation criterion is

15

the stationarity gap ∥∇F \(x\)∥.

*F*\( *x*\)| 10

On this problem we execute \[D\]SIGD \(Algorithm 1\),

|

\[S\]SIGD \(Algorithm 2\), and PDBO \(Sow et al., 2022\).

5

In the first two cases, we solve the inner-level problem

0

using 10 steps of projected gradient descent with stepsize

0

20

40

60

80

10−1. For the stepsize of \[S\]SIGD, we choose β = 0.1,

iterations

while in \[D\]SIGD we find the proper Armijo step-size

Figure 2: ∥∇F \(x\)∥ vs \# of iterations.

by successively adapting \(by increasing m\) the quantity

ar = \(0.9\)m until condition \(11\) is met. In PDBO we select 10−1 for the stepsizes of both the primal and dual steps, and the number of inner iterations is set to 10. In Figure 2, we plot the convergence curves for the three algorithms with respect to number of iterations; the results are averaged over 10

runs. Note that the line search \[D\]SIGD method outperforms the fixed step-size \[S\]SIGD and PDBO

while \[S\]SIGD performs similar to PDBO.

Adversarial Learning. We consider an adversarial learning problem of the form given in \(3\). For the perturbation we focus on the ϵ-tolerant ℓ

d

ℓ

∞-norm attack constraint, namely Y = \{y ∈ R

∥y∥∞ ≤

ϵ\}, which can easily be expressed as a linear inequality constraint as in the LL problem of \(3\).

We consider two widely accepted adversarial learning method as our baselines, namely AT \(Madry

et al., 2017\) and TRADES \(Zhang et al., 2019b\). Also, we consider two representative datasets CIFAR-10/100 \(Krizhevsky et al., 2009\) and adopt the ResNet-18 \(He et al., 2016\) model; the results for CIFAR-10 are provided in Appendix C. In particular, we studied two widely used,\(Madry et al.,

2017; Wong et al., 2020\) attack budget choices ϵ ∈ \{8/255, 16/255\}. In the implementation of our

\[S\]SIGD method, we adopt a perturbation generated by a Gaussian random vector q with variances from the following list σ2 ∈ \{2e−5, 4e−5, 6e−5, 8e−5, 1e−4, \}, in order to study different levels of smoothness. Moreover, for solving the LL problem in each iteration we select a fixed batch of samples. We choose fi to be cross-entropy loss and hi = −fi \+ λ∥yi∥2 for hyper-parameter λ > 0.

For \[S\]SIGD, we follow the implementation of \(Zhang et al., 2021\) but with perturbations in the LL

problem. We evaluate the robustly trained model with two metrics, namely the standard accuracy \(SA\) and robust accuracy \(RA\), where we evaluate the accuracy of the robustified model on the clean and attacked test set, respectively; the attacked set is generated using PGD-50-10 \(Madry et al.,

2017\) \(i.e., 50-step PGD attack with 10 restarts\). Desirably, a well trained model possesses high RA while maintaining simultaneously the SA at a high level. Table 1 shows the performance overview of our experiments. We make the following observations. First, a low level of perturbation variance \(e.g., σ2 ∈ \{2e−5, 4e−5\}\) in general improves both SA as well as RA, which presents an enhanced RA-SA trade-off. For example, in the setting \(CIFAR-100, ϵ = 16/255\), our algorithm boosts the RA by over 0.3% and the SA by 2%. Second, a high level of perturbation variance harms the robustness but results in high SA. This is reasonable, since the stochastic gradient becomes too noisy with large variances. Third, our method outperforms AT and closely matches the performance of the stronger baseline TRADES. However, we would like to stress that the intent of our work is not to design a specialized adversarial learning method, and thus robustness gap between our method and the strong baseline does not diminish the value of our method. Additional details are provided in Appendix C.2.

Table 1: Performance overview of different methods on CIFAR-100 Krizhevsky et al. \(2009\) with ResNet-18 He

et al. \(2016\). The result a±b represents the mean value a with a standard deviation of b over 5 random trials.

CIFAR-100, ϵ = 8/255

\[S\]SIGD \(Gaussian variance σ2\)

Metrics

AT

TRADES

2e−5

4e−5

6e−5

8e−5

1e−4

SA

53.83±0.19

53.33±0.18

53.88±0.22

54.01±0.24

53.79±0.14

54.44±0.18

57.74±0.22

RA

27.36±0.24

28.44±0.17

27.43±0.12

28.22±0.10

28.12±0.14

27.14±0.21

25.22±0.15

ϵ = 16/255

SA

42.06±0.17

42.19±0.23

44.06±0.19

45.66±0.25

46.57±0.22

47.11±0.32

47.46±0.44

RA

15.10±0.28

16.59±0.26

15.51±0.17

14.18±0.22

13.92±0.25

13.54±0.18

13.42±0.26

9

Under review as a conference paper at ICLR 2023

REFERENCES

Amin Abedi, Mohammad Reza Hesamzadeh, and Franco Romerio. An acopf-based bilevel optimization approach for vulnerability assessment of a power system. International Journal of Electrical Power & Energy Systems, 125:106455, 2021. ISSN 0142-0615.

Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter.

Differentiable convex optimization layers. Advances in neural information processing systems, 32, 2019.

G. B. Allende and G. Still. Solving bilevel programs with the kkt-approach. Mathematical Programming, 138:309–332, 2013.

Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.

In International Conference on Machine Learning, pp. 136–145. PMLR, 2017.

Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial training. Advances in Neural Information Processing Systems, 33:16048–16059, 2020.

D. A. Arias, A. A. Mota, L. T. M. Mota, and C. A. Castro. A bilevel programming approach for power system operation planning considering voltage stability and economic dispatch. In 2008

IEEE/PES Transmission and Distribution Conference and Exposition: Latin America, pp. 1–6, 2008. doi: 10.1109/TDC-LA.2008.4641718.

Quentin Bertrand, Quentin Klopfenstein, Mathieu Blondel, Samuel Vaiter, Alexandre Gramfort, and Joseph Salmon. Implicit differentiation of lasso-type models for hyperparameter optimization. In International Conference on Machine Learning, pp. 810–821. PMLR, 2020.

Quentin Bertrand, Quentin Klopfenstein, Mathurin Massias, Mathieu Blondel, Samuel Vaiter, Alexandre Gramfort, and Joseph Salmon. Implicit differentiation for fast hyperparameter selection in non-smooth convex learning. ArXiv, abs/2105.01637, 2021.

Dimitri P Bertsekas. Nonlinear programming, 2nd ed. Athena Scientific Belmont, MA, 1998.

Paul H Calamai and Jorge J Moré. Projected gradient methods for linearly constrained problems.

Mathematical programming, 39\(1\):93–116, 1987.

Emmanuel J Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?

Journal of the ACM \(JACM\), 58\(3\):1–37, 2011.

Mark Cecchini, Joseph Ecker, Michael Kupferschmid, and Robert Leitch. Solving nonlinear principal-agent problems using bilevel programming. European Journal of Operational Research, 230\(2\): 364–373, 2013.

Tsung-Hui Chang, Mingyi Hong, Hoi-To Wai, Xinwei Zhang, and Songtao Lu. Distributed learning in the nonconvex world: From batch data to streaming and beyond. IEEE Signal Processing Magazine, 37\(3\):26–38, 2020. doi: 10.1109/MSP.2020.2970170.

Tianyi Chen, Yuejiao Sun, and Wotao Yin. A single-timescale stochastic bilevel optimization method, 2021a.

Tianyi Chen, Yuejiao Sun, and Wotao Yin. Tighter analysis of alternating stochastic gradient method for stochastic nested problems. arXiv preprint arXiv:2106.13781, 2021b.

Yuxin Chen, Yuejie Chi, and Andrea J Goldsmith. Exact and stable covariance estimation from quadratic sampling via convex programming. IEEE Transactions on Information Theory, 61\(7\): 4034–4059, 2015.

Benoît Colson, Patrice Marcotte, and Gilles Savard. Bilevel programming: A survey. 4or, 3\(2\): 87–107, 2005.

Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization, 29\(1\):207–239, 2019.

10

Under review as a conference paper at ICLR 2023

Damek Davis, Dmitriy Drusvyatskiy, and Courtney Paquette. The nonsmooth landscape of phase retrieval. IMA Journal of Numerical Analysis, 40\(4\):2652–2695, 2020.

Stephan Dempe and Alain Zemkoho. Bilevel optimization. Springer, 2020.

Mohamed Didi-Biha, Patrice Marcotte, and Gilles Savard. Path-based formulations of a bilevel toll setting problem, pp. 29–50. Springer US, Boston, MA, 2006. ISBN 978-0-387-34221-4. doi: 10.1007/0-387-34221-4\_2.

Priya Donti, Brandon Amos, and J Zico Kolter. Task-based end-to-end model learning in stochastic optimization. Advances in neural information processing systems, 30, 2017.

Dmitriy Drusvyatskiy. The proximal point method revisited. arXiv preprint arXiv:1712.06038, 2017.

Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In International Conference on Machine Learning, pp. 1165–1173. PMLR, 2017.

Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning, pp. 1568–1577. PMLR, 2018.

T.L. Friesz and D. Bernstein. Foundations of Network Optimization and Games. Complex Networks and Dynamic Systems. Springer US, 2015. ISBN 9781489975942.

Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246, 2018.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

Stephen Gould, Richard Hartley, and Dylan Campbell. Deep declarative networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44\(8\):3988–4004, 2021.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.

Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint arXiv:2007.05170, 2020.

Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In International Conference on Machine Learning, pp. 4882–4892. PMLR, 2021.

Dr. Vyacheslav Kalashnikov, José-Fernando Camacho-Vallejo, Ronald Askin, and Nataliya Kalash-nykova. Comparison of algorithms for solving a bi-level toll setting problem. International journal of innovative computing, information & control: IJICIC, 6:3529–3549, 08 2010.

Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A momentum-assisted single-timescale stochastic approximation algorithm for bilevel optimization, 2021a.

Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. Advances in Neural Information Processing Systems, 34, 2021b.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

Gérard Lebourg. Generic differentiability of lipschitzian functions. Transactions of the American Mathematical Society, 256:125–144, 1979.

Lecture. 5: Correspondences and berge’s maximum theorem. Math Camp Notes, Department of Economics, Yale University, 2017.

11

Under review as a conference paper at ICLR 2023

Yi Li, Lingxiao Song, Xiang Wu, Ran He, and Tieniu Tan. Learning a bi-level adversarial network with global and local perception for makeup-invariant face verification. Pattern Recognition, 90: 99–108, 2019.

Gui-Hua Lin, Mengwei Xu, and Jane J. Ye. On solving simple bilevel programs with a nonconvex lower level program. Math. Program., 144\(1–2\):277–305, April 2014.

Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. arXiv, 2021a.

Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A value-function-based interior-point method for non-convex bi-level optimization. In International Conference on Machine Learning, pp. 6882–6892. PMLR, 2021b.

Risheng Liu, Xuan Liu, Shangzhi Zeng, Jin Zhang, and Yixuan Zhang. Value-function-based sequential minimization for bi-level optimization. arXiv preprint arXiv:2110.04974, 2021c.

Songtao Lu, Meisam Razaviyayn, Bo Yang, Kejun Huang, and Mingyi Hong. Finding second-order stationary points efficiently in smooth nonconvex linearly constrained optimization problems.

Advances in Neural Information Processing Systems, 33:2811–2822, 2020.

Zhi-Quan Luo, Jong-Shi Pang, and Daniel Ralph. Mathematical programs with equilibrium constraints. Cambridge University Press, 1996.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.

Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.

Julien Mairal, Francis Bach, and Jean Ponce. Task-driven dictionary learning. IEEE transactions on pattern analysis and machine intelligence, 34\(4\):791–804, 2011.

Patrice Marcotte, Gilles Savard, and D. Zhu. A trust region algorithm for nonlinear bilevel programming. Oper. Res. Lett., 29:171–179, 11 2001. doi: 10.1016/S0167-6377\(01\)00092-X.

Athanasios Migdalas. Bilevel programming in traffic planning: Models, methods and challenge.

Journal of global optimization, 7\(4\):381–405, 1995.

James A Mirrlees. The theory of moral hazard and unobservable behaviour: Part i. The Review of Economic Studies, 66\(1\):3–21, 1999.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via curvature regularization, and vice versa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9078–9086, 2019.

Julie Nutini, Mark Schmidt, and Warren Hare. “active-set complexity” of proximal gradient: How long does it take to find the sparsity pattern? Optimization Letters, 13\(4\):645–655, 2019.

Francesca Parise and Asuman Ozdaglar. Sensitivity analysis for network aggregative games. In 2017

IEEE 56th Annual Conference on Decision and Control \(CDC\), pp. 3200–3205. IEEE, 2017.

Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International conference on machine learning, pp. 737–746. PMLR, 2016.

Arvind U Raghunathan and Lorenz T Biegler. Mathematical programs with equilibrium constraints \(mpecs\) in process engineering. Computers & Chemical Engineering, 27\(10\):1381–1392, 2003.

ISSN 0098-1354.

Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.

Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation for bilevel optimization. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1723–1732. PMLR, 2019.

12

Under review as a conference paper at ICLR 2023

Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: from classical to evolutionary approaches and applications. IEEE Transactions on Evolutionary Computation, 22

\(2\):276–295, 2017.

Ankur Sinha, Tanmay Khandait, and Raja Mohanty. A gradient-based bilevel optimization approach for tuning hyperparameters in machine learning. arXiv preprint arXiv:2007.11022, 2020.

Daouda Sow, Kaiyi Ji, Ziwei Guan, and Yingbin Liang. A constrained optimization approach to bilevel optimization with multiple inner minima. arXiv preprint arXiv:2203.01123, 2022.

Heinrich Von Stackelberg and Stackelberg Heinrich Von. The theory of the market economy. Oxford University Press, 1952.

Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training.

arXiv preprint arXiv:2001.03994, 2020.

Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization.

Advances in Neural Information Processing Systems, 34, 2021.

Tao Yang, Xinlei Yi, Junfeng Wu, Ye Yuan, Di Wu, Ziyang Meng, Yiguang Hong, Hong Wang, Zongli Lin, and Karl H Johansson. A survey of distributed optimization. Annual Reviews in Control, 47:278–305, 2019.

JJ Ye and DL Zhu. Optimality conditions for bilevel programming problems. Optimization, 33\(1\): 9–27, 1995.

Farzad Yousefian. Bilevel distributed optimization in directed networks. In 2021 American Control Conference \(ACC\), pp. 2230–2235. IEEE, 2021.

Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate once: Accelerating adversarial training via maximal principle. Advances in Neural Information Processing Systems, 32, 2019a.

Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.

Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning, pp. 7472–7482. PMLR, 2019b.

Yihua Zhang, Guanhua Zhang, Prashant Khanduri, Mingyi Hong, Shiyu Chang, and Sijia Liu.

Revisiting and advancing fast adversarial training through the lens of bi-level optimization. arXiv: 2112.12376, 2021.

13

Under review as a conference paper at ICLR 2023

A

RELATED LITERATURE

Bilevel optimization problems, initially encountered in the context of Stackelberg \(leader-follower\) games \(Von Stackelberg & Von, 1952\), find applications in a multitude of areas, including machine learning \(Liu et al., 2021a\), economics \(Mirrlees, 1999\), power systems \(Abedi et al., 2021; Arias

et al., 2008\), chemical industry \(Raghunathan & Biegler, 2003\), transport research \(Didi-Biha et al.,

2006; Kalashnikov et al., 2010\); see \(Colson et al., 2005; Dempe & Zemkoho, 2020; Sinha et al.,

2017; Liu et al., 2021a\) for a number of survey papers. The “classical” approaches for solving bilevel problems include the use of approximate descent methods \(Shaban et al., 2019; Ghadimi &

Wang, 2018; Franceschi et al., 2017\), penalty methods \(Lin et al., 2014\), KKT reformulations-based approaches \(Allende & Still, 2013\), value function-based methods \(Ye & Zhu, 1995; Sow et al.,

2022\), and trust-region algorithms \(Marcotte et al., 2001\). In addition, bilevel problems are known to be related to mathematical programs with equilibrium constraints \(MPEC\) \(Luo et al., 1996\).

Recently, motivated by machine learning applications, gradient-based approaches have gained popular-ity for solving bilevel optimization problems \(Liu et al., 2021a\), e.g., in hyperparameter optimization

\(Shaban et al., 2019; Franceschi et al., 2017; 2018\), and meta learning \(Rajeswaran et al., 2019;

Franceschi et al., 2018\). The majority of those works are focused on solving bilevel problems with unconstrained strongly convex LL problem, for both stochastic and deterministic objectives \(Ghadimi

& Wang, 2018; Hong et al., 2020; Khanduri et al., 2021a;b; Chen et al., 2021a; Ji et al., 2021; Chen

et al., 2021b; Yang et al., 2021\). An attractive property of such problems is the existence and easy computability of the implicit gradient. Moreover, under mild assumptions, the implicit gradient for these problems can be shown to be Lipschitz smooth \(e.g., see \(Ghadimi & Wang, 2018, Lemma 2.2\) and \(Khanduri et al., 2021b, Lemma 3.1\)\). In contrast, for bilevel problems with linear LL constraints the implicit gradient in general might not exist, and even if it exists computing it in closed-form is a challenging task. As discussed earlier, we develop a perturbation-based smoothing framework for the constrained LL problem that ensures the existence of the implicit gradient in an almost sure sense, and allows us to compute an expression for the implicit gradient.

In Liu et al. \(2021c\) and Sow et al. \(2022\) the authors have considered bilevel optimization with \(general\) constraints in the LL problem. Both papers develop a value function-based framework that leads to a single level problem with non-convex constraints. In Liu et al. \(2021c\) a sequential minimization approach is followed where the value-function and the LL constraints are incorporated into the objective using penalty or barrier functions. In Sow et al. \(2022\) a primal-dual-based framework is proposed in which the problem is regularized with the addition of a strongly-convex penalty term, while a constant error term is added to make the constraint set strictly feasible. In contrast, our approach relies only on a small linear perturbation which can be made arbitrarily small without practically changing the landscape of the LL problem.

There is also a line of works \(Amos & Kolter, 2017; Agrawal et al., 2019; Donti et al., 2017; Gould

et al., 2021\) about implicit differentiation in deep learning literature. These works Deep-Learning-type \(DL-type\) are indeed related to ours, in the sense that at the core of both of them lies the computation of the gradient/Jacobian of the solution of an optimization problem. However, there are some key differences. First, in our work we consider a constrained bilevel optimization problem and we are interested in analyzing this problem from an optimization perspective. On the other hand, in the DL-type works the optimization problems that are studied describe the input-output relationships of neural networks layers and the main focus lies in deriving Jacobians for the backward pass. Secondly, in our work we study a special bilevel problem \(the constraints are linear\) and derive a closed form expression for the implicit gradient. On the contrary, in the DL-type works the underlying problems have more general constraints and the Jacobian is usually computed using numerical methods \(e.g., solving iteratively a system of KKT equations\), rather than analytically. Finally, in our work the focus is on studying the properties of the bilevel problem \(e.g. differentiability, approximation errors\), developing \(deterministic and stochastic\) algorithms, and performing a convergence analysis. On the other hand, DL-type works focus mainly on the Jacobian computation and its implementation.

Finally, there is a number of works on implicit differentiation on non-smooth problems \(Mairal et al.,

2011; Bertrand et al., 2021; 2020\). However, these works typically deal with special \(non-smooth\) LL problems, e.g., in Mairal et al. \(2011\); Bertrand et al. \(2020\) the non-smooth term in the LL is the ℓ1-norm, and in Bertrand et al. \(2021\) the non-smooth term is separable. On the contrary, in our work we are considering smooth LL problems and general linear inequality constraints.

14

Under review as a conference paper at ICLR 2023

B

SOLUTION METHODS FOR THE LL PROBLEM

The LL problem is a strongly convex linearly constrained optimization task. As a result, there exist many efficient ways to find its solutions. In order to discuss about them, we consider two different classes of problems depending on the exact form of the linear constraints and the difficulty of computing the respective projection operator: 1\) the projection has a closed-form solution, 2\) the projection requires the solution of an optimization problem. Before we proceed, we would like to stress that the problem we are solving, i.e., the bilevel problem with linear constraints in the LL, is a very challenging one, regardless of the specific form and the exact way we approach the solution of the LL problem.

In the first class of problems, where the projection can be computed in closed form, we have problems with special linear constraints. One characteristic example is box constraints, i.e. constraints of the form a ≤ y ≤ b, where the inequalities apply in a component-wise manner. These constraints appear in applications, such as adversarial learning \(see the motivating applications in the main text\). In this case, we can use some first-order iterative algorithm to solve the LL problem and project each iterate onto the constraint set using the closed-form expression \(which only incurs a constant cost per iteration\). For instance, we can use the projected gradient descent method which probably converges to the optimal solution with a linear rate.

In the second class of problems, the projection operator does not possess a closed-form expression.

In this case we can approach the LL problem as a convex optimization task, and solve it using some convex optimization solver \(e.g. employing interior-point methods\) to obtain a highly accurate solution with a complexity of O \(p\(dℓ, k\) log\(dℓ/ϵ\)\), where p\(·\) is some polynomial and ϵ is solution accuracy. Alternatively, as mentioned in the previous case, we can use a projected gradient descent-type method that enjoys a linear convergence rate guarantee. Differently from the previous case though the projection operator computed at each iteration requires the solution of an optimization problem. Nonetheless, the projection task we are referring to is a \(strongly convex\) quadratic linearly constrained problem, that is a special quadratic programming task, which is easy to solve in practice.

In algorithm 3 we describe the solution of the LL using a projected gradient descent algorithm.

Algorithm 3 Projected Gradient Descent \(PGD\)

1: Input: Initial parameter y0, Current iterate x, \# iter T , step-sizes \{γr\}T −1, Constraints A, b r=0

2: for r = 0, 1, . . . , T − 1 do

3:

yr\+1 = yr − γr∇yg\(x, yr\)

4:

Project yr\+1 to Y = \{y ∈

d

R ℓ |Ay ≤ b\} by solving the following QP:

min ∥y − yr\+1∥2 s.t. Ay ≤ b

\(15\)

y∈ d

R ℓ

5: end for

C

ADDITIONAL EXPERIMENTS

In this section, we include additional experiments on quadratic bilevel optimization problems and Adversarial training along with the implementation details. First, we evaluate the performance of the

\[D\]SIGD and \[S\]SIGD on quadratic bilevel optimization problems.

C.1

NUMERICAL RESULTS

We consider the following linearly constrained quadratic bilevel problems of the form \(1\) with the UL and the LL objectives defined as:

1

1

1

1

1

f \(x, y\) =

∥x∥2 \+ 5xT y −

∥y∥2 , h\(x, y\) =

∥x∥2 \+

xT y \+

∥y∥2

\(16\)

4

4

4

2

4

1

1

1

f \(x, y\) =

∥x∥2 \+ 2xT y −

∥y∥2 , h\(x, y\) = xT y \+

∥y∥2.

\(17\)

2

2

2

15

Under review as a conference paper at ICLR 2023

12

\[S\]SIGD

\[S\]SIGD

\[D\]SIGD

5

\[D\]SIGD

10

PDBO

4

PDBO

8

*F*\( *x*\)|

3

6

*F*\( *x*\)|

| 4

2

|

2

1

0

0

0

20

40

60

80

0

10

20

30

40

50

iterations

iterations

\(a\) Problem \(16\).

\(b\) Problem \(17\).

15

\[S\]SIGD

\[D\]SIGD

PDBO

10

*F*\( *x*\)|| 5

0 0

20

40

60

80

iterations

\(c\) Problem \(18\).

Figure 3: Convergence curves w.r.t. number of iterations.

1

1

1

f \(x, y\) =

∥x∥2 \+ 2xT y −

∥y∥2 \+ 1 , h\(x, y\) = xT y \+

∥y∥2 \+ 1Tx \+ 1Ty.

\(18\)

4

4

2

In the first two cases, we have du = dl = 2, and the linear constraints in the LL are of the form

−1 ≤ yi ≤ 1, i ∈ \{1, 2\}. In the third example, we have du = dl = 2, and the linear constraints in the LL are of the form −5 ≤ yi ≤ 5, i ∈ \{1, 2\}, −5 ≤ y1 \+ y2 ≤ 5. We compare the performance of SIGD algorithms to recently proposed PDBO \(Sow et al., 2022\). In Figures 3a, 3b and 3c, we present the evolution of the stationarity gap ∥∇F \(x\)∥ during the execution of the three algorithms, for the problems \(16\) \(17\) and \(18\), respectively. The results are averaged over 10 random runs, and the variance of the results across these runs is reflected on the shaded region across the convergence curves. In our experiments, we choose the step-size using the backtracking line search for \[D\]SIGD

as stated in Algorithm 1, while for \[S\]SIGD we choose a constant step-size. Note that since all problems are deterministic \[S\]SIGD utilizes a gradient estimator with zero variance.

In problem \(16\), we solve the LL problem using 10 steps of projected gradient descent with stepsize 0.1; in the case of \[D\]SIGD the stepsize is 1. For the stepsize of \[S\]SIGD, we choose β = 0.1, while in \[D\]SIGD we find the proper Armijo step-size by successively adapting \(by increasing m\) the quantity ar = \(0.9\)m until condition \(11\) is met. In PDBO we select 0.1 for the stepsizes of both the primal and dual steps, and the number of inner iterations is set to 10. In problem \(17\), we solve the LL problem using 20 steps of projected gradient descent with stepsize 0.1; in the case of \[D\]SIGD

the number of steps is 10 and the stepsize is 1. For the stepsize of \[S\]SIGD, we choose β = 0.1, while in \[D\]SIGD we find the proper Armijo step-size by successively adapting \(by increasing m\) the quantity ar = \(0.95\)m until condition \(11\) is met. In PDBO we select 0.1 for the stepsizes of both the primal and dual steps, and the number of inner iterations is set to 20. In problem \(18\), we solve the LL problem \(of both \[D\]SIGD and \[S\]SIGD\) using 10 steps of projected gradient descent with stepsize 0.1. For the stepsize of \[S\]SIGD, we choose β = 0.1, while in \[D\]SIGD we find the proper Armijo step-size by successively adapting \(by increasing m\) the quantity ar = \(0.9\)m until condition \(11\) is met. In PDBO we select 0.1 for the stepsizes of both the primal and dual steps, and the number of inner iterations is set to 10.

16

Under review as a conference paper at ICLR 2023

C.2

ADVERSARIAL LEARNING

In this section, we present some additional results along with the implementation details for the adversarial learning problem. As noted earlier, we consider the adversarial learning problem of form

\(3\). For learning the perturbation y∗\(x\), we focus on the ϵ-tolerant ℓ∞-norm attack constraint, i.e., Y = \{y ∈

d

R ℓ ∥y∥∞ ≤ ϵ\}. Note that this constraint can easily be expressed as a linear inequality constraint as in the LL problem in \(3\). In particular, we evaluate the performance of \[S\]SIGD on two widely used attack budget choices of ϵ ∈ \{8/255, 16/255\} \(Madry et al., 2017; Zhang et al., 2019b;

Wong et al., 2020; Andriushchenko & Flammarion, 2020; Zhang et al., 2019a\). In the implementation of our \[S\]SIGD method, we adopt a perturbation generated by a Gaussian random vector q with variances from the following list σ2 ∈ \{2e−5, 4e−5, 6e−5, 8e−5, 1e−4, \}, in order to study different levels of smoothness. We choose fi to be cross-entropy loss and hi = −fi \+ λ∥yi∥2 with λ > 0 as a hyper-parameter. For solving \(3\), in each iteration we select a fixed batch of samples for both the UL

and LL problems. Also, note that the ReLU-based neural networks commonly lead to a piece-wise linear decision boundary w.r.t. the inputs \(Moosavi-Dezfooli et al., 2019\). This implies that the implicit gradient in \(10\) can be further approximated using a Hessian-free implementation, where the Hessian of the LL problem can be approximated by λI \(Zhang et al., 2021, Eq. \(25\)\). Note that these approximations are common in practice and do not lead to performance degradation compared to the case when full Hessian is used to compute the implicit-gradient \(Zhang et al., 2021, Table 5\). Next, we analyze the effect of adding different perturbations q in the LL problem on the performance of

\[S\]SIGD. Specifically, we choose q ∼ N \(0, σ2I\) and evaluate the performance of \[S\]SIGD with σ2.

In Figure 4, we plot the robust accuracy \(RA\) and the standard accuracy \(SA\) with respect to the variance of the Gaussian perturbation vector used in the LL problem. As can be seen, the RA increases as the variance increases within a certain range. However, with stronger noise \(i.e., σ2 > 10−4\), the RA drops sharply, while the SA increases. This is reasonable, since high variance makes the true LL gradient noisy. For easier observation, in Figure 5 we zoom in the part of Figure 4 where σ2 ∈ \[0, 8 · 10−5\]. It can be clearly seen that adding a small perturbation q helps in improving the RA.

90

90

80

80

70

70

60

60

Accuracy

50

Accuracy

50

Test

40

Test

40

30

Robust Test Accuracy

Robust Test Accuracy

30

20

Standard Test Accuracy

Standard Test Accuracy

10

0

2e-5 4e-5 6e-5 8e-5 1e-4 5e-4 1e-3

0

1e-5 2e-5 4e-5 6e-5 8e-5 1e-4 5e-4 1e-3

Gaussian Noise Variance 2

Gaussian Noise Variance 2

\(a\) ϵ = 8/255

\(b\) ϵ = 16/255

Figure 4: The influence of Gaussian variance on the RA and SA. The experiments are based on CIFAR-10 with ResNet-18 model.

Next, we compare the performance of \[S\]SIGD against two widely accepted adversarial learning methods as baselines, namely AT \(Madry et al., 2017\) and TRADES \(Zhang et al., 2019b\). Here, we present the results for CIFAR-10 dataset \(Krizhevsky et al., 2009\) and adopt the ResNet-18 \(He

et al., 2016\). In Table 2, we compare the performance of \[S\]SIGD for different perturbation variances with classical AT \(Madry et al., 2017\) algorithm and TRADES \(Zhang et al., 2019b\). Note that for appropriate choice of perturbation variance \[S\]SIGD outperforms the classical AT algorithm while performs is only slightly worse compared to TRADES, especially, for higher attack budget of ϵ = 16/255.

17

Under review as a conference paper at ICLR 2023

84.0

52.0

83.8

51.8

83.6

51.6

83.4

51.4

83.2

Accuracy

51.2

Accuracy

83.0

Test

51.0

Test

82.8

50.8

82.6

Standard Test Accuracy

50.6

Robust Test Accuracy

82.4 0

2e-5

4e-5

6e-5

8e-5

0

2e-5

4e-5

6e-5

8e-5

Gaussian Noise Variance 2

Gaussian Noise Variance 2

\(a\) ϵ = 8/255, CIFAR-10

73.00

32.5

72.75

72.50

32.0

72.25

31.5

72.00

Accuracy

Accuracy

71.75

Test

Test 31.0

71.50

71.25

30.5

Standard Test Accuracy

Robust Test Accuracy

71.00 0 1e-5 2e-5 4e-5 6e-5 8e-5

0

1e-5

2e-5

4e-5

6e-5

8e-5

Gaussian Noise Variance 2

Gaussian Noise Variance 2

\(b\) ϵ = 16/255, CIFAR-10

Figure 5: The influence of Gaussian variance on the RA and SA.

Table 2: Performance overview of different methods on CIFAR-10 \(Krizhevsky et al., 2009\) with ResNet-18 \(He et al., 2016\). The result a±b represents the mean value a with a standard deviation of b over 5 random trials.

CIFAR-10, ϵ = 8/255

\[S\]SIGD \(Gaussian variance σ2\)

Metrics

AT

TRADES

2e−5

4e−5

6e−5

8e−5

1e−4

SA

80.78±0.23

80.23±0.23

80.70±0.14

81.20±0.22

81.52±0.21

83.19±0.24

85.08±0.44

RA

50.71±0.21

51.17±0.19

50.78±0.21

51.15±0.19

50.59±0.18

49.83±0.23

47.83±0.13

ϵ = 16/255

SA

70.31±0.11

70.22±0.29

71.43±0.14

72.79±0.24

73.50±0.09

73.98±0.35

75.31±0.33

RA

32.12±0.18

33.35±0.14

32.72±0.25

31.73±0.10

29.97±0.14

29.39±0.15

27.67±0.07

D

PROOFS

D.1

PROOFS OF SECTION 2

D.1.1

PROOF OF PROPOSITION 1

Note that the goal of Proposition 1 is to establish the continuity of the mapping y∗\(x\) and the implicit function G\(x\) := f \(x, y∗\(x\)\). In the following, we will show that under Assumption

1, y∗\(x\) is in fact continuous, which will then utilize to establish the continuity of G\(x\). Before starting the proof we need a few definitions. Consider the LL problem \(1b\) and let us denote the set Y = \{y ∈

d

R ℓ |Ay ≤ b\}. Note that in general the constraint set Y can depend on the UL variable x ∈ X . For such cases, Y\(x\) is a set valued map Y : X →

d

R ℓ and is referred to as a correspondence.

However, for the bilevel problem in \(1a\) and \(1b\) the correspondence Y is independent of x ∈ X and is a fixed set. Also, we define the upper-semi continuity \(USC\) and the lower-semi continuity \(LSC\) for the correspondence Y\(x\). To define these notions of continuity, we will utilize the notion of an ϵ-ball defined below.

18

Under review as a conference paper at ICLR 2023

Definition 2 \(ϵ-Ball\). For Y ⊂

d

R ℓ , and given ϵ > 0, we define the open ball about Y as

B

d

ℓ

ϵ\(Y \) := y ∈ R

∥y − y′ ∥ < ϵ,

for some y′ ∈ Y ,

where ∥ · ∥ is the standard Euclidean norm.

Using the ϵ-ball we define the Upper Semi-Continuity \(USC\) of the correspondence Y.

Definition 3 \(Upper Semi-Continuity \(USC\)\). The correspondence Y : X →

d

R ℓ is USC if for every

x ∈ X and ϵ > 0, there exists a δ > 0 such that Y\(x′\) ⊂ Bϵ\(Y\(x\)\)\), if x′ ∈ X and ∥x − x′∥ < δ.

Next, we define the notion of Lower Semi-Continuity \(LSC\).

Definition 4 \(Lower Semi-Continuity \(LSC\)\). The correspondence Y : X →

d

R ℓ is LSC if for any

sequence xn in X that converges to a point x ∈ X , and y ∈ Y\(x\), there exists a sequence yn such that yn ∈ Y\(xn\), for all n ∈ N, and limn→∞ yn = y.

Theorem 5 \(Berge’s Theorem of Maximum \(Lecture, 2017\)\). Let X ⊂

d

R u be a non-empty set.

Also, let Y : X →

d

R ℓ be a correspondence such that the set Y \(x\) is compact and non-empty for all x ∈ X , and Y is USC and LSC. Then, if g : X ×

d

R ℓ → R is a continuous function with y∗\(x\)

defined as

n

o

y∗\(x\) ∈ arg min g\(x, y\) y ∈ Y\(x\) ,

y∈ d

R ℓ

the correspondence y∗\(x\) is non-empty for all x ∈ X , and USC.

Remark 2. If y∗\(x\) is singleton, then USC implies the continuity of the map y∗\(x\) : X → Y.

Next, we present the proof of Proposition 1.

Proof. The proof of proposition 1 follows from the application of Berge’s theorem.

To begin with, note that for our problem the set Y is a fixed set independent of x ∈ X . We are going to verify the conditions of Theorem 5. First, note from Assumption 1\(b\) that the set Y is non-empty and compact. Then, it is easy to see that Y ⊂ Bϵ\(Y\), for every ϵ > 0, and that implies the LSC of Y.

Moreover, since the set Y is independent of x ∈ X and compact, for every sequence xn → x in X , we can always find a sequence yn → y, such that yn, y ∈ Y. Therefore, Y is LSC. Finally, using Assumption 1\(a\) we see that the function g\(x, y\) is continuous. Then, Theorem 5 implies that the set y∗\(x\) is non-empty and the correspondence USC.

Using the strong-convexity of g\(x, y\) with respect to y \(Assumption 1\(c\)\) we claim that y∗\(x\) will be a singleton, and thereby a continuous mapping. Then, the continuity of y∗\(x\) implies the continuity of G\(x\) := f \(x, y∗\(x\)\), since the composition of two continuous functions is continuous.

The proof is now complete.

D.1.2

PROOF OF LEMMA 2

Proof. In this proof we follow a reasoning similar to \(Parise & Ozdaglar, 2017, Thm. 1\). However, differently from that work we consider bilevel problems rather than Nash games. To begin with, consider the Lagrangian of problem \(5\), i.e.,

L\(x, y, λ\) = g\(x, y\) \+ λT \(Ay − b\) .

Then, for some fixed x ∈ X , consider a KKT point \(y∗\(x\), λ∗\(x\)\) of \(5\), for which it holds that,

• ∇yL\(x, y∗\(x\), λ∗\(x\)\) = ∇yg\(x, y∗\(x\)\) \+ AT λ∗\(x\) = 0

T

• \[λ∗\(x\)\] \(Ay∗\(x\) − b\) = 0

• λ∗\(x\) ≥ 0

• Ay∗\(x\) − b ≤ 0.

19

Under review as a conference paper at ICLR 2023

Now, consider the active constraints at \(y∗\(x\), λ∗\(x\)\), and to simplify notation let us set A :=

A\(y∗\(x\)\). Using the notations defined in Section 2 and the SC property, the KKT conditions given above can be equivalently rewritten as

T

∗

∗

∇yg\(x, y∗\(x\)\) \+ A λ \(x\) = 0,

Ay∗\(x\) − b = 0,

λ \(x\) > 0,

\(19\)

∗

where λ \(x\) is the subvector of λ∗\(x\) that contains only the elements whose indices correspond to the active constraints at y = y∗\(x\). Moreover, notice that the point \(y∗\(x\), λ∗\(x\)\) is unique. The uniqueness of y∗\(x\) follows from the strong convexity of g\(x, ·\); the uniqueness of λ∗\(x\) results from the fact that matrix A has full row rank \(which guarantees regularity, e.g., see Bertsekas \(1998\)\).

As mentioned in section 2, the SC condition \(from Lemma 1\) combined with Assumption 1 implies that the mapping y∗\(x\) is differentiable almost surely \(Friesz & Bernstein, 2015, Theorem 2.22\). As a result, at any given point x, we can consider a sufficiently small neighborhood around it, such that the active constraints A remain unchanged. Then, we can compute the gradient of \(19\) using the implicit function theorem as follows

T

∗

∇2 g\(x, y∗\(x\)\) \+ ∇2 g\(x, y∗\(x\)\)∇y∗\(x\) \+ A ∇λ \(x\) = 0

xy

yy

\(20\)

A∇y∗\(x\) = 0.

\(21\)

Solving the \(20\) for ∇y∗\(x\) yields

T

∗

i

∇y∗\(x\) = ∇2 g\(x, y∗\(x\)\)−1 h−∇2 g\(x, y∗\(x\)\) − A ∇λ \(x\) ,

yy

xy

\(22\)

where we exploited the fact that the Hessian matrix ∇2 g\(x, y∗\(x\)\)

yy

is positive definite and thus

invertible. Substituting \(22\) into \(21\) gives T

∗

i

A∇2 g\(x, y∗\(x\)\)−1 h−∇2 g\(x, y∗\(x\)\) − A ∇λ \(x\) = 0

yy

xy

∗

h

T i−1 h

i

=⇒ ∇λ \(x\) = − A∇2 g\(x, y∗\(x\)\)−1A

A∇2 g\(x, y∗\(x\)\)−1∇2 g\(x, y∗\(x\)\) .

yy

yy

xy

Finally, note that the KKT point y∗\(x\) corresponds to the unique global minimum of \(5\), due to the strong convexity of g\(x, ·\). The proof is now complete.

D.1.3

THE PROOF OF LEMMA 3

The proof of Lemma 3 requires several intermediate results which we provide below. Note that under Assumption 2\(c\) it holds that A\(y∗\(x\)\) = A\(y\(x\)\); for simplicity we will denote these matrices as b

A in the derivations of this subsection. Moreover, for any given matrix A we will denote with LA the maximum value of the quantity ∥A \(y\(x\)\) ∥, across all x ∈ X .

b

Lemma 6. Suppose that Assumption 1,2,3 hold. Then for any x ∈ X , we have: \(a\)

∇2 g\(x, y\)−1

≤ 1 , ∀y ∈

d

R ℓ .

yy

µg

2

\(b\)

∇2 g\(x, y∗\(x\)\)−1 − ∇2 g\(x, y\(x\)\)−1

≤

1

Lg δ.

yy

yy

b

µg

yy

h

T i−1

\(c\)

dℓ

A ∇2 g\(x, y\)−1 A

≤ L

.

yy

A, ∀y ∈ R

h

h

T i−1

T i−1

2

\(d\)

1

A ∇2 g\(x, y∗\(x\)\)−1 A

− A ∇2 g\(x, y\(x\)\)−1 A

≤ L2 L

L

δ.

yy

yy

b

A

A µ2

gyy

g

Proof. a\) We know that g\(x, y\) is strongly convex in y with modulus µg. Therefore, for any x ∈ X

we have

∇2 g\(x, y\) ⪰ µ

dℓ

yy

g I ≻ 0, ∀y ∈ R

1

=⇒ 0 ≺ ∇2 g\(x, y\)−1 ⪯

I, ∀y ∈

dℓ

yy

R

µg

1

=⇒

∇2 g\(x, y\)−1

≤

, ∀y ∈

d

R ℓ .

\(23\)

yy

µg

20

Under review as a conference paper at ICLR 2023

b\) To begin with, notice that for arbitrary square invertible matrices P, Q we have P −1 − Q−1

=

P −1 \(Q − P \)Q−1

≤

Q−1 \(P − Q\)

P −1

≤

Q−1

∥P − Q∥

P −1

.

\(24\)

Then, using the above inequality we get

∇2 g\(x, y∗\(x\)\)−1 − ∇2 g\(x, y\(x\)\)−1

yy

yy

b

≤

∇2 g\(x, y∗\(x\)\)−1

∇2

g\(x, y∗\(x\)\) − ∇2 g\(x, y\(x\)\)

∇2 g\(x, y\(x\)\)−1

yy

yy

yy

b

yy

b

1 2

≤

Lg

∥y∗\(x\) − y\(x\)∥

µ

yy

b

g

1 2

≤

Lg δ,

µ

yy

g

where in the second inequality we used the result from Lemma 6\(a\) and the Lipschitz Hessian property of g in yy \(Assumption 3\(d\)\); in the third inequality we use the Assumption 2\(a\) for y\(x\).

c\) In our problem we have that g strongly convex in y and Lipschitz gradient in y. Thus, for any x ∈ X we have

LyI ⪰ ∇2 g\(x, y∗\) ⪰ µ

yy

g I ≻ 0

1

1

=⇒ 0 ≺

I ⪯ ∇2 g\(x, y\)−1 ⪯

I, ∀y ∈

d

R ℓ .

L

yy

y

µg

Also, for every x ∈ X , we have that

T

2

T

T

A z = zT AA z ≥ λ

dℓ

min\(AA

\)∥z∥2, ∀z ∈ R .

Using the above two lower bound we get

T

1

T

zT A ∇2 g\(x, y\)−1 A z ≥

λ

\)∥z∥2 > 0, ∀z ∈

dℓ \\ \{0\},

yy

min\(AA

R

Ly

where the last inequality follows from the fact that A is full row rank which implies that T

λmin\(AA \) > 0, ∀x ∈ X .

Since the above inequality holds for every z ∈

d

R ℓ \\ \{0\}, it ∀x ∈ X we get that

T

T

λmin\(AA \)

A ∇2 g\(x, y\)−1 A

⪰

I ≻ 0

yy

Ly

h

T i−1

Ly

A ∇2 g\(x, y\)−1 A

⪯

I

yy

T

λmin\(AA \)

h

L

T i−1

y

A ∇2 g\(x, y\)−1 A

≤

.

yy

T

λmin\(AA \)

Finally, for the given matrix A, consider the submatrix A = A\(y\(x\)\) generated by consider-b

ing only the subset of its rows corresponding to the active constraints at y\(x\). From Assump-b

tion 1\(c\) we know that A\(y\(x\)\) is full row rank for every x ∈ X , and so we can ensure that b

λmin A \(y\(x\)\) A \(y\(x\)\)T > 0, ∀x ∈ X . Then, we denote with λ

b

b

min the minimum value of the

quantity λmin A \(y\(x\)\) A \(y\(x\)\)T across all x ∈ X . Therefore, we conclude that b

b

h

L

T i−1

y

A ∇2 g\(x, y\)−1 A

≤

:= L

yy

A.

λmin

21

Under review as a conference paper at ICLR 2023

T

T

d\) Applying formula \(24\) with P = A ∇2 g\(x, y∗\(x\)\)−1 A , Q = A ∇2 g\(x, y\(x\)\)−1 A yy

yy

we

b

get

h

h

T i−1

T i−1

A ∇2 g\(x, y∗\(x\)\)−1 A

− A ∇2 g\(x, y\(x\)\)−1 A

yy

yy

b

h

T i−1

T

T

≤

A ∇2 g\(x, y∗\(x\)\)−1 A

A ∇2 g\(x, y∗\(x\)\)−1 A − A ∇2 g\(x, y\(x\)\)−1 A

yy

yy

yy

b

h

T i−1

A ∇2 g\(x, y\(x\)\)−1 A

yy

b

h

T i−1

h

T

≤

A ∇2 g\(x, y∗\(x\)\)−1 A

A

∇2 g\(x, y∗\(x\)\)−1 − ∇2 g\(x, y\(x\)\)−1i A

yy

yy

yy

b

h

T i−1

A ∇2 g\(x, y\(x\)\)−1 A

yy

b

h

T i−1

T

≤

A ∇2 g\(x, y∗\(x\)\)−1 A

∇2 g\(x, y∗\(x\)\)−1 − ∇2 g\(x, y\(x\)\)−1

A

yy

A

yy

yy

b

h

T i−1

A ∇2 g\(x, y\(x\)\)−1 A

yy

b

2

2

1

≤ L2 L

L

δ,

A

A

g

µ

yy

g

where in the final inequality we used the bounds derived in Lemma 6\(b\), 6\(c\), and the bound

∥A∥ ≤ LA.

The proof is now complete.

Now let us bound the norm of the gradients of the mappings λ∗\(x\) and y∗\(x\).

Lemma 7. Under Assumptions 1,2,3, the gradients of the mappings λ∗\(x\) and y∗\(x\) satisfy the following bounds for every x ∈ X ,

∗

∗

∥∇λ \(x\)∥ ≤ L

λ∗ ,

b

∇λ \(x\) ≤ Lλ∗

∥∇y∗\(x\)∥ ≤ Ly∗, ∥ b

∇y∗\(x\)∥ ≤ Ly∗

∗

where L

λ∗ =

1 L

and L

L

\+ L

. Note that

\(x\) and

µ

ALALg

y∗ =

1

g

ALλ∗

b

∇λ

b

∇y∗\(x\) are

g

xy

µy

xy

∗

obtained by substituting the estimate y\(x\) in place of y∗\(x\) in the expressions ∇λ \(x\) and ∇y∗\(x\), b

respectively \(Please see Lemma 2\).

Proof. From Lemma 2 we have

∗

h

T i−1 h

i

∇λ \(x\) = − A ∇2 g\(x, y∗\(x\)\)−1 A

A ∇2 g\(x, y∗\(x\)\)−1 ∇2 g\(x, y∗\(x\)\)

yy

yy

xy

Then, taking the norm of this quantity we get,

∗

h

i

T i−1 h

∇λ \(x\) = A ∇2 g\(x, y∗\(x\)\)−1 A

A ∇2 g\(x, y∗\(x\)\)−1 ∇2 g\(x, y∗\(x\)\)

yy

yy

xy

h

T i−1

≤

A ∇2 g\(x, y∗\(x\)\)−1 A

∇2 g\(x, y∗\(x\)\)−1

g\(x, y∗\(x\)\)

yy

A

∇2

yy

xy

1

≤ LALA

Lg

:= Lλ∗ ,

µ

xy

g

where in the last inequality we used Lemma 6\(a\), 6\(c\) and Assumption 3\(f\).

∗

Similarly, for ∥ b

∇λ \(x\)∥ we have that

∗

h

i

T i−1 h

b

∇λ \(x\) = A ∇2 g\(x, y\(x\)\)−1 A

A ∇2 g\(x, y\(x\)\)−1 ∇2 g\(x, y\(x\)\)

yy

b

yy

b

xy

b

1

≤ LALA

Lg

= Lλ∗ .

µ

xy

g

22

Under review as a conference paper at ICLR 2023

Moving to the bound of ∥∇y∗\(x\)∥, we know from Lemma 2 that the formula of the gradient of y∗\(x\) is

T

∗

i

∇y∗\(x\) = ∇2 g\(x, y∗\(x\)\)−1 h−∇2 g\(x, y∗\(x\)\) − A ∇λ \(x\) .

yy

xy

\(25\)

Then, we have that

T

∗

i

∥∇y∗\(x\)∥ =

∇2 g\(x, y∗\(x\)\)−1 h−∇2 g\(x, y∗\(x\)\) − A ∇λ \(x\)

yy

xy

h

T

∗

i

≤

∇2 g\(x, y∗\(x\)\)−1

−∇2 g\(x, y∗\(x\)\) − A ∇λ \(x\)

yy

xy

1

∗

≤

∇2

g\(x, y∗\(x\)\)

\+

A

∇λ \(x\)

µ

xy

g

1

≤

L

g

\+ LALλ∗ := Ly∗ ,

µ

xy

g

where in the second inequality we used we used Lemma 6\(a\); the third inequality follows from

∗

Assumption 3\(f\) and the bound for ∇λ \(x\) we derived above.

Similarly, for

b

∇y\(x\) we can obtain the following bound

i

T

∗

b

∇y\(x\) =

∇2 g\(x, y\(x\)\)−1 h−∇2 g\(x, y\(x\)\) − A b

∇λ \(x\)

yy

b

xy

b

1

∗

≤

∇2

g\(x, y\(x\)\)

\+

A

b

∇λ \(x\)

µ

xy

b

g

1

≤

L

g

\+ LALλ∗ = Ly∗ .

µ

xy

g

The proof is now complete.

In the next two results we are going to present bounds for the difference of the exact and approximate

∗

gradients of the mappings λ \(x\) and ∇y∗\(x\).

Lemma 8. Suppose that Assumptions 1,2,3 hold. Then, the following bound holds

∗

∗

∥∇λ \(x\) − b

∇λ \(x\)∥ ≤ Lλ∗δ,

3

2

2

where L

1

1

λ∗ =

L L3 L

L

\+ 1 L

\+

L

L

.

µ

A

g

g

ALALg

ALALg

g

g

A

yy

xy

µg

xy

µg

yy

xy

∗

∗

Proof. Using the derivation of ∇λ \(x\) from Lemma 2, and its approximation b

∇λ \(x\) where we

substitute y∗\(x\) with y\(x\) in the formula of the former, that is,

b

∗

h

T i−1 h

i

b

∇λ \(x\) = − A ∇2 g\(x, y\(x\)\)−1 A

A ∇2 g\(x, y\(x\)\)−1 ∇2 g\(x, y\(x\)\) ,

yy

b

yy

b

xy

b

we obtain

∗

∗

h

i

T i−1 h

∇λ \(x\) −

b

∇λ \(x\) = A ∇2 g\(x, y∗\(x\)\)−1 A

A ∇2 g\(x, y∗\(x\)\)−1 ∇2 g\(x, y∗\(x\)\)

yy

yy

xy

h

T i−1 h

i

− A ∇2 g\(x, y\(x\)\)−1 A

A ∇2 g\(x, y\(x\)\)−1 ∇2 g\(x, y\(x\)\) .

yy

b

yy

b

xy

b

Below, we use the following notation in order to simplify the derivations.

h

T i

H\(x\) = A ∇2 g\(x, y∗\(x\)\)−1 A

g\(x, y∗\(x\)\)−1

g\(x, y∗\(x\)\)

yy

, G\(x\) = ∇2yy

, M \(x\) = ∇2xy

h

T i

b

H\(x\) = A ∇2 g\(x, y\(x\)\)−1 A

g\(x, y\(x\)\)−1

g\(x, y\(x\)\)

yy

, b

G\(x\) = ∇2

, c

M \(x\) = ∇2

b

yy

b

xy

b

23

Under review as a conference paper at ICLR 2023

Then, we have that

∗

∗

∥∇λ \(x\) −

b

∇λ \(x\)∥ = H−1\(x\)AG\(x\)M \(x\) − b

H−1\(x\)A b

G\(x\) c

M \(x\)

\(a\)

≤ H−1\(x\)AG\(x\)M \(x\) −

b

H−1\(x\)AG\(x\)M \(x\)

\+

b

H−1\(x\)AG\(x\)M \(x\) − b

H−1\(x\)A b

G\(x\) c

M \(x\)

≤

H −1\(x\) −

b

H−1\(x\) A

∥G\(x\)∥ ∥M \(x\)∥

\+

b

H−1\(x\) A

G\(x\)M \(x\) − b

G\(x\) c

M \(x\)

\(b\)

≤

H −1\(x\) −

b

H−1\(x\) A

∥G\(x\)∥ ∥M \(x\)∥

h

i

\+

b

H−1\(x\) A

G\(x\)M \(x\) − G\(x\) c

M \(x\) \+ G\(x\) c

M \(x\) − b

G\(x\) c

M \(x\)

≤

H −1\(x\) −

b

H−1\(x\) A

∥G\(x\)∥ ∥M \(x\)∥

\+

b

H−1\(x\) A

∥G\(x\)∥

M \(x\) − c

M \(x\) \+

b

H−1\(x\) A

G\(x\) − b

G\(x\)

c

M \(x\)

\(c\)

2

2

2

1

1

1

1

≤ L L2

L

δL

L

\+ L

L

δ \+ L

L

δL

A

A

g

A

g

ALA

g

ALA

g

g

µ

yy

xy

xy

yy

xy

g

µg

µg

µg

\!

1 3

2

2

1

1

=

L L3 Lg Lg

\+

LALALg

\+

LALALg Lg

δ.

µ

A

A

yy

xy

xy

yy

xy

g

µg

µg

In \(a\) we add and subtract the term b

H−1\(x\)AG\(x\)M \(x\) and apply the triangle inequality. In \(b\) we

add and subtract the term G\(x\) c

M \(x\) and apply the triangle inequality. In \(c\) we use Lemma 6\(d\) for

∥H−1\(x\) − b

H−1\(x\)∥, the bound ∥A∥ ≤ LA, Lemma 6\(a\) for ∥G\(x\)∥, Lemma 6\(c\) for ∥H−1\(x\)∥

and ∥ b

H−1\(x\)∥, Assumption 3\(f\) for ∥M \(x\)∥ and ∥ c

M \(x\)∥, Assumption 3\(e\) for ∥M \(x\) − c

M \(x\)∥,

and finally Lemma 6\(b\) for ∥G\(x\) − b

G\(x\)∥.

The proof is now complete.

Lemma 9. Suppose that Assumptions 1,2,3 hold. Then, the following bound holds

∥∇y∗\(x\) − b

∇y\(x\)∥ ≤ Ly∗δ,

2

2

where L

1

1

y∗ =

L

L

\+ 1 L

\+

L

L

L

µ

g

g

g

g

ALλ∗ \+ 1

ALλ∗ .

g

yy

xy

µg

xy

µg

yy

µg

Proof. From Lemma 2 we have that

T

∗

i

∇y∗\(x\) = ∇2 g\(x, y∗\(x\)\)−1 h−∇2 g\(x, y∗\(x\)\) − A ∇λ \(x\) .

yy

xy

We can also get b

∇y\(x\) by substituting y\(x\) in place of y∗\(x\) in the above formula, i.e., b

T

∗

i

b

∇y\(x\) = ∇2 g\(x, y\(x\)\)−1 h−∇2 g\(x, y\(x\)\) − A

\(x\) .

yy

b

∇λ

b

xy

b

24

Under review as a conference paper at ICLR 2023

Then, we have that

i

−1 h

T

∗

∇y∗\(x\) −

b

∇y\(x\) = \[∇yyg\(x, y∗\(x\)\)\]

−∇xyg\(x, y∗\(x\)\) − A ∇λ \(x\)

T

∗

i

− ∇2 g\(x, y\(x\)\)−1 h−∇

y\(x\)\) − A

\(x\)

yy

b

∇λ

b

xy g\(x, b

\(a\)

≤

∇2 g\(x, y∗\(x\)\)−1 ∇

xy g\(x, y∗\(x\)\) − ∇2 g\(x, y\(x\)\)−1 ∇xy g\(x, y\(x\)\)

yy

yy

b

b

T

∗

T

∗

\+

∇2 g\(x, y∗\(x\)\)−1 A ∇λ \(x\) − ∇2 g\(x, y\(x\)\)−1 A

b

∇λ \(x\)

yy

yy

b

\(b\)

≤

−1

∇2 g\(x, y∗\(x\)\)−1 ∇

xy g\(x, y∗\(x\)\) − \[∇yy g\(x, y\(x\)\)\]

∇xyg\(x, y∗\(x\)\)

yy

b

\+

∇2 g\(x, y\(x\)\)−1 ∇

xy g\(x, y∗\(x\)\) − ∇2 g\(x, y\(x\)\)−1 ∇xy g\(x, y\(x\)\)

yy

b

yy

b

b

T

∗

T

∗

\+

∇2 g\(x, y∗\(x\)\)−1 A ∇λ \(x\) − ∇2 g\(x, y\(x\)\)−1 A ∇λ \(x\)

yy

yy

b

T

∗

T

∗

\+

∇2 g\(x, y\(x\)\)−1 A ∇λ \(x\) − ∇2 g\(x, y\(x\)\)−1 A

b

∇λ \(x\)

yy

b

yy

b

≤

∇2 g\(x, y∗\(x\)\)−1 − ∇2 g\(x, y\(x\)\)−1

∥∇xyg\(x, y∗\(x\)\)∥

yy

yy

b

\+

∇2 g\(x, y\(x\)\)−1

∥∇xyg\(x, y∗\(x\)\) − ∇xyg\(x, y\(x\)\)∥

yy

b

b

T

∗

\+

∇2 g\(x, y∗\(x\)\)−1 − ∇2 g\(x, y\(x\)\)−1

A ∇λ \(x\)

yy

yy

b

T

∗

∗

\+

∇2 g\(x, y\(x\)\)−1

A ∇λ \(x\) −

b

∇λ \(x\)

yy

b

\(c\) 1 2

1

1 2

1

≤

Lg δLg

\+

Lg δ \+

Lg δLALλ∗ \+

LALλ∗ δ

µ

yy

xy

xy

yy

g

µg

µg

µg

\!

1 2

1

1 2

1

=

Lg Lg

\+

Lg

\+

Lg LALλ∗ \+

LALλ∗

δ.

µ

yy

xy

xy

yy

g

µg

µg

µg

In \(a\) the triangle inequality was used.

In \(b\) we add and subtract the expressions

−

T

∗

\[∇

1

yy g\(x, y\(x\)\)\]

∇

g\(x, y\(x\)\)−1 A ∇λ \(x\) in the first and second

b

xy g\(x, y∗\(x\)\) and ∇2

yy

b

terms, respectively. In \(c\) we apply Lemma 6\(a\), 6\(b\), Assumption 3\(e\), 3\(f\), the bound ∥A∥ ≤ LA, Lemmas 7 and 8.

The proof is now complete.

Now we have all the results needed to prove Lemma 3.

Proof of Lemma 3. To begin with, the exact and approximate \(due to the inexact solution of the LL

problem\) implicit gradients of the objective F \(x\), are given below.

∇F \(x\) = ∇xf \(x, y∗\(x\)\) \+ \[∇y∗\(x\)\]T ∇yf \(x, y∗\(x\)\)

h

iT

b

∇F \(x\) = ∇xf \(x, y\(x\)\) \+ b

∇y\(x\)

∇

y\(x\)\).

b

y f \(x, b

25

Under review as a conference paper at ICLR 2023

Then, we can compute the norm of their difference.

∥ b

∇F \(x\) − ∇F \(x\)∥ = ∥∇xf \(x, y\(x\)\) \+ \[∇y\(x\)\]T ∇

y\(x\)\)

b

b

y f \(x, b

− ∇xf \(x, y∗\(x\)\) − \[∇y∗\(x\)\]T ∇yf \(x, y∗\(x\)\)∥

\(a\)

≤ ∥∇xf \(x, y\(x\)\) − ∇

b

xf \(x, y∗\(x\)\)∥

\+ ∥\[ b

∇y\(x\)\]T ∇yf \(x, y\(x\)\) − \[∇y∗\(x\)\]T ∇

b

y f \(x, y∗\(x\)\)∥

\(b\)

≤ ∥∇xf \(x, y\(x\)\) − ∇

b

xf \(x, y∗\(x\)\)∥

\+ ∥\[ b

∇y\(x\)\]T ∇yf \(x, y\(x\)\) − \[∇y∗\(x\)\]T ∇

y\(x\)\)∥

b

y f \(x, b

\+ ∥ \[∇y∗\(x\)\]T ∇yf \(x, y\(x\)\) − \[∇y∗\(x\)\]T ∇

b

y f \(x, y∗\(x\)\)∥

\(c\)

≤ Lf ∥y\(x\) − y∗\(x\)\)∥ \+ ∥ b

∇y\(x\) − ∇y∗\(x\)\)∥∥∇

y\(x\)\)∥

b

y f \(x, b

\+ ∥∇y∗\(x\)\)∥∥∇yf \(x, y\(x\)\) − ∇

b

y f \(x, y∗\(x\)\)∥

\(d\)

≤ Lf ∥y\(x\) − y∗\(x\)\)∥ \+ ∥ b

∇y\(x\) − ∇y∗\(x\)\)∥∥∇

y\(x\)\)∥

b

y f \(x, b

\+ Lf ∥∇y∗\(x\)\)∥∥y\(x\) − y∗\(x\)∥

b

\(e\)

≤ Lf δ \+ Ly∗δLf \+ Lf Ly∗δ

= L

f \+ Ly∗ Lf \+ Lf Ly∗

δ := LF δ,

\(26\)

where LF = Lf \+ Ly∗ Lf \+ Lf Ly∗ . Also, in inequality \(a\) above we apply the triangle inequality; in \(b\) we add and subtract the term \[∇y∗\(x\)\]T ∇yf \(x, y\(x\)\), and use triangle inequality; in \(c\) and b

\(d\) we use the Lipschitz gradient property of f \(Assumption 2\(b\)\); in \(e\) we apply Assumptions 3\(a\)

and 2\(a\), and Lemmas 7 and 9.

Now consider the expression ∥∇F \(x\)∥. We have that

∥∇F \(x\)∥ = ∇

xf \(x, y∗\(x\)\) \+ \[∇y∗\(x\)\]T ∇y f \(x, y∗\(x\)\)

≤ |∇xf \(x, y∗\(x\)\)∥ \+ ∥∇y∗\(x\)∥ ∥∇yf \(x, y∗\(x\)\)∥

≤ 1 \+ L

y∗

Lf := LF ,

where we applied Assumption 3\(a\) and Lemma 7. Similarly, we can see that h

iT

b

∇F \(x\) = ∇xf \(x, y\(x\)\) \+ b

∇y\(x\)

∇yf \(x, y\(x\)\)

b

b

≤ ∥∇

xf \(x, y\(x\)\)∥ \+

b

∇y\(x\) ∥∇

y\(x\)\)∥

b

y f \(x,

b

≤ 1 \+ L

y∗

Lf = LF .

Therefore, the proof is completed.

D.1.4

PROOF OF LEMMA 4

Proof. From the definition of the stochastic gradient in \(10\) we have b

∇F \(x; ξ\) = ∇xf \(x, y\(x\); ξ\) \+ \[ b

∇y∗\(x\)\]T ∇

y\(x\); ξ\).

b

y f \(x, b

Taking expectation on both sides and utilizing Assumption 4, we get Eξ\[ b

∇F \(x; ξ\)\] = Eξ ∇xf\(x, y\(x\); ξ\) \+ \[ b

∇y∗\(x\)\]T ∇

y\(x\); ξ\)

b

y f \(x, b

=

Eξ ∇xf \(x, y\(x\); ξ\) \+ \[ b

∇y∗\(x\)\]T E ∇

y\(x\); ξ\)

b

ξ

y f \(x, b

= ∇xf \(x, y\(x\); ξ\) \+ \[ b

∇y∗\(x\)\]T ∇

y\(x\); ξ\)

b

y f \(x, b

= b

∇F \(x\).

26

Under review as a conference paper at ICLR 2023

Similarly, for the variance of the stochastic implicit gradient, we have Eξ∥ b

∇F \(x; ξ\) − b

∇F \(x\)∥2 = Eξ∇xf\(x, y\(x\); ξ\) \+ \[ b

∇y∗\(x\)\]T ∇

y\(x\); ξ\)

b

y f \(x, b

2

− ∇xf \(x, y\(x\)\) \+ \[ b

∇y∗\(x\)\]T ∇

y\(x\)\)

b

y f \(x, b

\(a\)

≤ 2 Eξ∥∇xf\(x, y\(x\); ξ\) − ∇

y\(x\)\)∥2

b

xf \(x, b

\+ 2 ∥ b

∇y∗\(x\)\]∥2 Eξ∥∇yf\(x, y\(x\); ξ\) − ∇

y\(x\)\)∥2

b

y f \(x, b

\(b\)

≤ 2σ2 \+ 2L

:= σ2 ,

f

y∗ σ2

f

F

where \(a\) follows from ∥x \+ y∥2 ≤ 2∥x∥2 \+ 2∥y∥2 and \(b\) results from Assumption 4 and the application of Lemma 7.

Therefore, we have the proof.

D.2

PROOFS OF SECTION 3

D.2.1

PROOF OF LEMMA 5

Proof. Let \{xr\}∞

r=0 with xr ∈ X be a given arbitrary countable sequence of points. Lemma 1

\(adapted from \(Lu et al., 2020, Proposition 1\)\) states that for any given point xr in the above sequence, the SC condition holds w.p. 1 for the LL problem in \(12\), assuming that q is generated from a continuous measure and A\(y∗\(xr\)\) is full row rank. This further implies that the mapping y∗\(x\), and thereby, the implicit function, F \(x\) is differentiable w.p. 1 for each given xr in the above sequence \(please see the discussion after Lemma 1\), i.e., we have P\(F \(x\) is differentiable at xr\) = 1 for each r = \{0, 1, . . . , ∞\}.

\(27\)

This further implies that we have

∞

\\

P F \(x\) is differentiable for all \{xr\}∞

=

\{F \(x\)

r=0

P

is differentiable at xr\}

r=0

∞

\[

= 1 − P

\{F \(x\) is non-differentiable at xr\}

r=0

∞

X

≥ 1 −

P \(\{F \(x\) is non-differentiable at xr\}\)

r=0

= 1.

where the second equality follows from the fact that P\(ω ∈ A\) = 1 − P\(ω ∈ Ac\) where Ac denotes the complement of a measurable event A; the inequality uses the union bound; and the final equality utilizes \(27\) above.

D.2.2

PROOF OF PROPOSITION 2

Proof. From Assumption 1 we know that h\(x, y\) \(and thus g\(x, y\)\) is strongly convex in y with modulus µg = µh. As a result we have that

µg

h\(x, y∗\(x\)\) ≥ h\(x, y∗\(x\)\) \+ ⟨∇yh\(x, y∗\(x\)\), y∗\(x\) − y∗\(x\)⟩ \+

∥y∗\(x\) − y∗\(x\)∥2

\(28\)

2

µg

g\(x, y∗\(x\)\) ≥ g\(x, y∗\(x\)\) \+ ⟨∇yg\(x, y∗\(x\)\), y∗\(x\) − y∗\(x\)⟩ \+

∥y∗\(x\) − y∗\(x\)∥2.

\(29\)

2

By definition y∗\(x\) is the global minimum of the objective h\(x, y\), and so it holds that

⟨∇yh\(x, y∗\(x\)\), y∗\(x\) − y∗\(x\)⟩ ≥ 0. Similarly, y∗\(x\) is the global minimum of the objective g\(x, y\), and so it holds that ⟨∇yg\(x, y∗\(x\)\), y∗\(x\) − y∗\(x\)⟩ ≥ 0. Then, using the above inequali-27

Under review as a conference paper at ICLR 2023

ties and adding \(28\) and \(29\), we get h\(x, y∗\(x\)\) \+ g\(x, y∗\(x\)\) ≥ h\(x, y∗\(x\)\) \+ g\(x, y∗\(x\)\) \+ µg∥y∗\(x\) − y∗\(x\)∥2

µg∥y∗\(x\) − y∗\(x\)∥2 ≤ \[h\(x, y∗\(x\)\) − g\(x, y∗\(x\)\)\] \+ \[g\(x, y∗\(x\)\) − h\(x, y∗\(x\)\)\]

µg∥y∗\(x\) − y∗\(x\)∥2 ≤ −qT y∗\(x\) \+ qT y∗\(x\)

qT \(y∗\(x\) − y∗\(x\)\)

∥y∗\(x\) − y∗\(x\)∥2 ≤

µg

∥qT ∥∥y∗\(x\) − y∗\(x\)∥

∥y∗\(x\) − y∗\(x\)∥2 ≤

µg

∥q∥

∥y∗\(x\) − y∗\(x\)∥ ≤

.

µg

Using the above bound and the fact that f is Lipschitz continuous \(it follows from the bounded gradient assumption 3\(a\)\) it is easy to see that

∥q∥

|F \(x\) − G\(x\)| = |f \(x, y∗\(x\)\) − f \(x, y∗\(x\)\)| ≤ Lf ∥y∗\(x\) − y∗\(x\)∥ ≤ Lf

.

µg

Therefore, the proof is complete.

D.2.3

PROOF OF THEOREM 1

Lemma 10. Under Assumption 1, 2, 3, ∇F \(x\) is almost surely continuous at a neighborhood around x, for any given x ∈ X .

Proof. To begin with, we already established in Lemma 2 that F is almost surely differentiable at any given x ∈ X . Therefore, for any x ∈ X there exists \(almost surely\) a neighborhood around it such that the matrix A corresponding to the active constraints at y∗\(x\) remains unchanged, where the gradient ∇y∗\(x\) is defined in eq. \(6\), \(7\). Further, since A is locally \(i.e., around any given x\) constant, and the formulas in \(6\), \(7\) can be seen as the results of a number of continuous operations over continuous functions, it is implied that ∇y∗\(x\) is also a continuous function at a neighborhood around x almost surely. As a result, ∇F \(x\) = ∇xf \(x, y∗\(x\)\) \+ \[∇y∗\(x\)\]T ∇yf \(x, y∗\(x\)\) is almost surely continuous locally around any given x ∈ X .

Proof of Theorem 1. Here we follow a reasoning similar to the proof of \(Bertsekas, 1998, Prop.

1.2.1\). However, there are a number of differences that make this proof more challenging. First, in our setting we are optimizing an inexact version of the objective b F \(x\) = f \(x, y\(x\)\), using an

b

approximate version of the gradient b

∇F \(x\). Since the approximate gradient we are using is not the

gradient of the objective b

F \(x\) \(the gradient of this function might not even exist\), we consider a modification of the standard Armijo rule where an additional error term is present. Secondly, in the proof below the \(classical\) mean value theorem \(Bertsekas, 1998, Prop. 1.23\) cannot be applied, since we cannot ensure that F is \(surely\) differentiable at any given interval over x. As we are going to show below, we use an alternative mean value theorem that does not require such assumption.

To begin with, we know that for the exact implicit objective F \(x\) and gradient ∇F \(x\) \(quantities to which we do not have access to\) we can find at each iteration r a step-size ar such that the following condition holds

F \(xr\) − F \(xr \+ ardr\) ≥ −σar \[∇F \(xr\)\]T dr,

\(30\)

where dr = xr − xr with xr = proj

e

e

X \(xr − ∇F \(xr \)\).

Next, the difference between the \(approximate\) objective values of two successive iterates \(for simplicity we will use the notation xr\+1 = xr \+ ardr, xr\+1 = xr \+ ar b dr; b

dr is defined in

b

28

Under review as a conference paper at ICLR 2023

Algorithm 1\) is

b

F \(xr\) − b

F \(xr\+1\) = b

F \(xr\) − F \(xr\) \+ F \(xr\) − F \(xr\+1\) \+ F \(xr\+1\) − F \(xr\+1\) \+ F \(xr\+1\) − b F \(xr\+1\)

b

b

b

b

= f \(xr, y\(xr\)\) − f \(xr, y∗\(xr\)\) \+ F \(xr\) − F \(xr\+1\) \+ F \(xr\+1\) − F \(xr\+1\) b

b

\+ f \(xr\+1, y∗\(xr\+1\)\) − f \(xr\+1, y\(xr\+1\)\)

b

b

b

b

b

≥ −Lf ∥y∗\(xr\) − y\(xr\)∥ \+ F \(xr\) − F \(xr\+1\) − L

xr\+1∥

b

F ∥xr\+1 − b

− Lf ∥y∗\(xr\+1\) − y\(xr\+1\)∥

b

≥ −Lf δr − σar \[∇F \(xr\)\]T dr − LF ar∥dr − b

dr∥ − Lf δr\+1

≥ −Lf δr − σar \[∇F \(xr\)\]T dr − LF LF arδr − Lf δr\+1

= −σar \[∇F \(xr\)\]T dr − ϵ1\(δ; r\),

\(31\)

where we set ϵ1\(δ; r\) = Lf δr \+ LF LF arδr \+ Lf δr\+1. In the first inequality, we used the Lipschitz continuity of f and F ; in the second inequality Assumption 2\(a\) and condition \(30\) were applied; in the third inequality the non-expansive property of the projection operator was used.

Also, we have that

T

∇T F \(xr\)dr = ∇F \(xr\) − b

∇F \(xr\) \+ b

∇F \(xr\)

dr \+ b

dr − b

dr

T

T

= ∇F \(xr\) − b

∇F \(xr\)

dr − b

dr

\+ ∇F \(xr\) − b

∇F \(xr\)

b

dr

h

iT

h

iT

\+ b

∇F \(xr\)

dr − b

dr

\+ b

∇F \(xr\)

b

dr

≤ ∥∇F \(xr\) − b

∇F \(xr\)∥∥dr − b

dr∥ \+ ∥∇F \(xr\) − b

∇F \(xr\)∥∥b

dr∥

h

iT

\+ ∥ b

∇F \(xr\)∥∥dr − b

dr∥ \+ b

∇F \(xr\)

b

dr

h

iT

≤ L2 \(δr\)2 \+ L

F

F LF δr \+ LF LF δr \+

b

∇F \(xr\)

b

dr

h

iT

=

b

∇F \(xr\)

b

dr \+ ϵ2\(δ; r\),

\(32\)

where ϵ2\(δ; r\) = L2 \(δr\)2 \+ 2L

F

F LF δr . Notice that the results in the second inequality follow from Lemma 3.

Then, combining \(31\) and \(32\) we get h

iT

b

F \(xr\) − b

F \(xr\+1\) ≥ −σar b

∇F \(xr\)

b

dr − ϵ

b

1\(δ; r\) − σar ϵ2\(δ; r\)

h

iT

= −σar b

∇F \(xr\)

b

dr − ϵ\(δ; r\),

where ϵ\(δ; r\) = ϵ1\(δ; r\) \+ σarϵ2\(δ; r\); notice that lim ϵ\(δ\) = 0. In conclusion, we can follow δ→0

this \(inexact\) Armijo-type rule in our inexact problem; the existence of the \(Armijo\) step-size is guaranteed by its existence for the exact problem \(30\).

Now let us move to the main part of the proof, which follows the reasoning used in \(Bertsekas, 1998,

Prop. 1.2.1\). Let \{xr\} ∈ X be the iterate sequence of our algorithm, and let ¯

x ∈ X be a limit point;

the existence of such point is guaranteed by the closedness of the set X . Moreover, it is established in Proposition 1 that F \(x\) is continuous, and as a result it holds that limr→\+∞ F \(xr\) = F \(¯

x\). The

latter results combined with the fact that all convergent sequences are also Cauchy sequences, implies that limr→\+∞\(F \(xr\) − F \(xr\+1\)\) = 0.

We want to show that ¯

x is a stationary point of F \(x\). We are going to show that by assuming that the opposite holds, i.e., ¯

x is not a stationary point of F \(x\), and arriving at a contradiction. From the Armijo rule of our problem we have that

h

iT

b

F \(xr\) − b

F \(xr\+1\) ≥ −σar b

∇F \(xr\)

b

dr − ϵ\(δ; r\).

\(33\)

b

29

Under review as a conference paper at ICLR 2023

Then, consider the following

b

F \(xr\) − b

F \(xr\+1\) = b

F \(xr\) − F \(xr\) \+ F \(xr\) − F \(xr\+1\) \+ F \(xr\+1\) − F \(xr\+1\) \+ F \(xr\+1\) − b F \(xr\+1\)

b

b

b

b

= f \(xr, y\(xr\)\) − f \(xr, y∗\(xr\)\) \+ F \(xr\) − F \(xr\+1\) \+ F \(xr\+1\) − F \(xr\+1\) b

b

\+ f \(xr\+1, y∗\(xr\+1\)\) − f \(xr\+1, y\(xr\+1\)\)

b

b

b

b

b

≤ Lf ∥y∗\(xr\) − y\(xr\)∥ \+ F \(xr\) − F \(xr\+1\) \+ L

xr\+1∥

b

F ∥xr\+1 − b

\+ Lf ∥y∗\(xr\+1\) − y\(xr\+1\)∥

b

≤ Lf δr \+ F \(xr\) − F \(xr\+1\) \+ LF ar∥dr − b

dr∥ \+ Lf δr\+1

≤ F \(xr\) − F \(xr\+1\) \+ Lf δr \+ LF LF arδr \+ Lf δr\+1

= F \(xr\) − F \(xr\+1\) \+ ϵ1\(δ; r\).

In the first inequality, we used the Lipschitz continuity of f and F ; in the second inequality Assumption 2\(a\) and condition \(30\) were applied; in the third inequality the non-expansive property of the projection operator was used. Using the above derivation we can bound the left-hand side of inequality \(33\) as follows

h

iT

F \(xr\) − F \(xr\+1\) \+ ϵ1\(δ; r\) ≥ b

F \(xr\) − b

F \(xr\+1\) ≥ −σar b

∇F \(xr\)

b

dr − ϵ\(δ; r\).

b

It is easy to see that the left-hand side in the above inequality tends to 0.

Therefore,

h

iT

limr→\+∞ −σar b

∇F \(xr\)

b

dr − ϵ\(δ; r\)

≤ 0. In addition, we know that limr→\+∞ ϵ\(δ; r\) = 0

h

iT

and −σar b

∇F \(x\)

b

dr ≥ 0, ∀x ∈ X . From the above statements we can conclude that

h

iT

lim σar b

∇F \(xr\)

b

dr = 0.

\(34\)

r→\+∞

Moreover, from the gradient-related assumption we know that for a non-stationary point ¯

x we have

that

h

iT

lim sup

b

∇F \(xr\)

b

dr < 0,

\(35\)

r→∞,r∈R

where\{xr\}R is subsequence with

lim

xr = ¯

x. Then, the conditions \(34\), \(35\) imply that

r→∞,r∈R

lim

ar = 0.

r→∞,r∈R

In the subsequence R we can find an index ¯

r ≥ 0 such that

ar

ar

b

F \(xr\) − b

F

xr \+

b

dr

< −σ

b

∇T F \(xr\)b

dr − ϵ\(δ; r\), ∀r ∈ R, r ≥ ¯

r.

\(36\)

β

β

Similarly with the proof of \(Bertsekas, 1998, Prop. 1.2.1\) let us introduce the following sequences: b

dr

ar∥b

dr∥

pr =

, ¯

ar =

b

∥

β

b

dr∥

The first sequence \{pr\} is bounded and so it admits a limit point ¯

p with ∥¯

p∥ = 1, that is

b

lim

pr = ¯

p, where R denotes the indices of a subsequence of R. In addition, tak-r→\+∞,r∈R

ing into account the facts that limr→\+∞,r∈R ar = 0 and the fact that the sequence \{∥dr∥\}R is bounded we can easily see that limr→\+∞,r∈R ¯

ar = 0.

Dividing both sides of \(36\) by ¯

ar and using the definitions of pr and ¯

a

b

r from above we get

b

F \(xr\) − b

F \(xr \+ ¯

arpr\)

h

iT

b

< −σ b

∇F \(xr\)

pr − ϵ\(δ; r\), ∀r ∈ R, r > ¯

r.

\(37\)

¯

ar

b

30

Under review as a conference paper at ICLR 2023

Then, we have that \(for convenience we adopt the notation xr\+1 = xr \+ ¯

arpr\)

b

b

b

F \(xr\) − b

F \(xr\+1\) = b

F \(xr\) − F \(xr\) \+ F \(xr\) − F \(xr\+1\) \+ F \(xr\+1\) − b

F \(xr\+1\)

b

b

b

b

= f \(xr, y\(xr\)\) − f \(xr, y∗\(xr\)\) \+ F \(xr\) − F \(xr\+1\)

b

b

\+ f \(xr\+1, y∗\(xr\+1\)\) − f \(xr\+1, y\(xr\+1\)\)

b

b

b

b

b

≥ −Lf ∥y∗\(xr\) − y\(xr\)∥ − L

xr\+1\) − y\(xr\+1\)∥ \+ F \(xr\) − F \(xr\+1\)

b

f ∥y∗\(b

b

b

b

≥ F \(xr\) − F \(xr\+1\) − L

b

f δr − Lf δr\+1,

\(38\)

where the first inequality above follows the Lipschitz continuity of f , and the second inequality is an application of Assumption 2\(a\). Incorporating inequality \(38\) into \(37\) results to F \(xr\) − F \(xr \+ ¯

arpr\)

δr \+ δr\+1

h

iT

b

− Lf

< −σ b

∇F \(xr\)

pr − ϵ\(δ; r\), ∀r ∈ R, r > ¯

r.

\(39\)

¯

a

b

r

¯

ar

Lebourg’s mean value theorem \(Lebourg, 1979, Theorem 1.7\) implies that F \(xr\) − F \(xr \+ ¯

arpr\)

b

= uTpr

¯

a

b

r

with u ∈ ϑF\(xr \+ arpr\) and ar ∈ \[0, ¯

ar\], where ϑF \(·\) is the Clarke subdifferntial of F . We know

e

b

e

that F is almost surely continuously differentiable \(Lemma 10\) at any xr \+ arpr ∈ X , and so the e

b

Clarke subdifferential at xr \+ ¯

arpr becomes w.p. 1 equal to ∇F \(xr \+ arpr\). Note that the we

b

e

b

cannot use here the \(classical\) mean value theorem \(Bertsekas, 1998, Prop. 1.23\), as in the proof of

\(Bertsekas, 1998, Prop. 1.2.1\), because it requires that the function F \(x\) is \(surely\) differentiable on the interval \[xr, xr \+ ¯

arpr\].

b

Then, we can rewrite the expression in \(39\) as follows δr \+ δr\+1

h

iT

−Lf β

− \[∇F \(xr \+ arpr\)\]T pr < −σ b

∇F \(xr\)

pr − ϵ\(δ; r\), ∀r ∈ R, r > ¯

r,

e

b

b

b

ar∥b

dr∥

where ar ∈ \[0, ¯

ar\].

e

Using the assumption that 0 ≤ δr ∼ O\(cr\), where cr is some sequence with lim cr = 0,

ar

r→∞,r∈R

and the fact that lim

∥

r→∞,r∈R

b

dr∥ ̸= 0 \(because of the assumption that the sequence xr converges

to a non-stationary point\), we compute the limit in the above expression and get

− \[∇F \(¯

x\)\]T ¯

p < −σ \[∇F \(¯

x\)\]T ¯

p

0 < \(1 − σ\) \[∇F \(¯

x\)\]T ¯

p

0 < \[∇F \(¯

x\)\]T ¯

p.

\(40\)

h

iT

However, note that b

∇F \(xr\)

pr = b

∇T F \(xr \)b

dr and therefore if we take limits in both sides we

b

∥b

dr ∥

obtain

lim supr→∞,r∈R b

∇T F \(xr\)b

dr

\[∇F \(¯

x\)\]T ¯

pr ≤

< 0,

\(41\)

lim sup

∥

r→∞,r∈R

b

dr∥

due to the gradient-related assumption. We notice that expressions \(40\) and \(41\) lead to a contradiction. Therefore, ¯

x is a stationary point of F \(x\).

The proof is now complete.

31

Under review as a conference paper at ICLR 2023

D.2.4

WEAKLY-CONVEX OBJECTIVE: PROOF OF THEOREM 2

n

ˆ

ρ

o

Proof. Define ˆ

xr = arg min H\(z\) \+

∥xr − z∥2 . Using the definition of Moreau envelope, we

z∈

2

Rdu

have

h

ˆ

ρ

i

E\[H1/ˆ

ρ\(xr\+1\)\] ≤ E F \(ˆ

xr\) \+

∥xr\+1 − ˆ

xr∥2

2

\(a\)

h

ˆ

ρ

i

= E F \(ˆ

xr\) \+

∥proj

2

X \(xr − β b

∇F \(xr; ξr\)\) − projX \(ˆxr\)∥2

\(b\)

h

ˆ

ρ

i

≤ E F \(ˆ

xr\) \+

∥xr − β b

∇F \(xr; ξr\) − ˆ

xr∥2

2

ˆ

ρ h

i

= F \(ˆ

xr\) \+

E ∥xr − ˆ

xr∥2 − 2⟨xr − ˆ

xr, β b

∇F \(xr; ξr\)⟩ \+ β2∥ b

∇F \(xr; ξr\)∥2

2

\(c\)

ˆ

ρ h

≤ F \(ˆ

xr\) \+

E ∥xr − ˆ

xr∥2 − 2⟨xr − ˆ

xr, β b

∇F \(xr; ξr\)⟩

2

i

\+ 2β2∥ b

∇F \(xr; ξr\) − b

∇F \(xr\)∥2 \+ 2β2∥ b

∇F \(xr\)∥2

\(d\)

ˆ

ρ h

2

i

≤ F \(ˆ

xr\) \+

∥xr − ˆ

xr∥2 − 2β⟨xr − ˆ

xr,

b

∇F \(xr\)⟩ \+ 2β2 σ2 \+ L

2

F

F

ˆ

ρ h

≤ F \(ˆ

xr\) \+

∥xr − ˆ

xr∥2 \+ 2β ⟨ˆ

xr − xr, ∇F \(xr\)⟩

2

|

\{z

\}

Term I

2

i

\+ 2β ⟨ˆ

xr − xr,

b

∇F \(xr\) − ∇F \(xr\)⟩ \+2β2 σ2 \+ L

,

F

F

|

\{z

\}

Term II

\(42\)

where \(a\) follows from the fact that xr\+1 ∈ X and ˆ

xr ∈ X ; \(b\) results from the non-expansiveness

of the projection operator; \(c\) uses ∥a − b∥2 = 2∥a∥2 \+ ∥b∥2; and \(d\) results from the application of Lemmas 3 and 4.

Next, considering Term I and Term II separately in \(42\) above. For Term I, we get using the weak convexity of F \(·\)

ρ

Term I = ⟨ˆ

xr − xr, ∇F \(xr\)⟩ ≤ F \(ˆ

xr\) − F \(xr\) \+

∥xr − ˆ

xr∥2

2

We bound Term II using the Young’s inequality as

Term II = ⟨ˆ

xr − xr, b

∇F \(xr\) − ∇F \(xr\)⟩

ρ

1

≤

∥xr − ˆ

xr∥2 \+

∥ b

∇F \(xr\) − ∇F \(xr\)∥2

2

2ρ

\(e\) ρ

1

≤

∥xr − ˆ

xr∥2 \+

L2 δ2

2

2ρ F

where \(e\) follows from Lemma 3. Next, substituting the bounds of Term I and Term II in \(42\) and using the definition of ˆ

xr, we get

ˆ

ρ

h

i

E\[H1/ˆ

ρ\(xr\+1\)\] ≤ F \(ˆ

xr\) \+

∥xr − ˆ

xr∥2 \+ ˆ

ρβ F \(ˆ

xr\) − F \(xr\) \+ ρ∥xr − ˆ

xr∥2

2

ˆ

ρβ

2

\+

L2 δ2 \+ β2 ˆ

ρ σ2 \+ L

2ρ F

F

F

h

i

ˆ

ρβ

2

≤ H

1/ ˆ

ρ\(xr \) \+ ˆ

ρβ F \(ˆ

xr\) − F \(xr\) \+ ρ∥xr − ˆ

xr∥2 \+

L2 δ2 \+ β2 ˆ

ρ σ2 \+ L

.

2ρ F

F

F

|

\{z

\}

Term III

\(43\)

32

Under review as a conference paper at ICLR 2023

Next, we bound Term III in \(43\) above.

Term III = F \(ˆ

xr\) − F \(xr\) \+ ρ∥xr − ˆ

xr∥2

ˆ

ρ

2ρ − ˆ

ρ

= F \(ˆ

xr\) \+

∥xr − ˆ

xr∥2 − F \(xr\) \+

∥xr − ˆ

xr∥2

2

2

3ρ − 2 ˆ

ρ

3ρ − 2 ˆ

ρ

≤

∥xr − ˆ

xr∥2 ≤

∥∇H

2

2 ˆ

ρ2

1/ ˆ

ρ\(xr \)∥2,

where the last equality follows from \(13\) and the first inequality follows from the fact that F \(x\) \+

ˆ

ρ ∥xr − x∥2 is \(ˆ

ρ − ρ\)-strongly convex. This implies the following

2

ˆ

ρ

ˆ

ρ − ρ

F \(ˆ

xr\) \+

∥xr − ˆ

xr∥2 − F \(xr\) ≤ −⟨∇F \(ˆ

xr\) \+ ˆ

ρ\(xr − ˆ

xr\), xr − ˆ

xr⟩ −

∥xr − ˆ

xr∥2

2

2

ρ − ˆ

ρ

≤

∥xr − ˆ

xr∥2,

2

where the second inequality results from the definition of ˆ

xr.

Finally, substituting Term III in \(43\) and rearranging the terms we get:

"

\#

2 ˆ

ρ − 3ρ

2

ˆ

ρβ

β

∥∇H

H

\+ L \+

L2 δ2.

2 ˆ

ρ

1/ ˆ

ρ\(xr \)∥2 ≤ E

1/ ˆ

ρ\(xr \) − H1/ ˆ

ρ\(xr\+1\) \+ β2 ˆ

ρ σ2F

F

2ρ F

Summing over all r ∈ \{0, 1, . . . , T − 1\} and dividing by T , we get T −1

1

H

X

2 ˆ

ρ

2

ˆ

ρ

∥∇

1/ ˆ

ρ\(x0\) − H ∗

H

\+ β ˆ

ρ σ2 \+ L \+

L2 δ2 .

T

1/ ˆ

ρ\(xr \)∥2 ≤

2 ˆ

ρ − 3ρ

βT

F

F

2ρ F

r=0

Therefore, we have the result.

D.2.5

STRONGLY-CONVEX OBJECTIVE: PROOF OF THEOREM 3

Proof. Using the update rule of the Algorithm 2, we have E∥xr\+1 − x∗∥2 = E∥proj \(xr − βr

X

b

∇F \(xr; ξr\)\) − x∗∥2

= E∥proj \(xr − βr

\(x∗\)∥2

X

b

∇F \(xr; ξr\)\) − projX

\(a\)

≤ E∥xr − βr b

∇F \(xr; ξr\) − x∗∥2

h

i

= E ∥xr − x∗∥2 \+ \(βr\)2∥ b

∇F \(xr; ξr\)∥2 − 2βr⟨xr − x∗, b

∇F \(xr; ξr\)⟩

\(b\)

h

≤ E ∥xr − x∗∥2 \+ 2\(βr\)2∥ b

∇F \(xr; ξr\) − b

∇F \(xr\)∥2

i

\+ 2\(βr\)2∥ b

∇F \(xr\)∥2 − 2βr⟨xr − x∗, b

∇F \(xr\)⟩

\(c\)

h

i

≤ E ∥xr − x∗∥2 \+ 2\(βr\)2\(σ2 \+ B2 \) − 2βr⟨xr − x∗,

F

F

b

∇F \(xr\)⟩

h

≤E ∥xr − x∗∥2 \+ 2\(βr\)2\(σ2 \+ B2 \) − 2βr⟨xr − x∗, ∇F \(xr\)⟩

F

F

i

− 2βr⟨xr − x∗, b

∇F \(xr\) − ∇F \(xr\)⟩

\(d\)

h

≤ E ∥xr − x∗∥2 \+ 2\(βr\)2\(σ2 \+ B2 \) − 2βr\[F \(xr\) − F ∗\] − µ

F

F

F βr ∥xr − x∗∥2

i

\+ 2βr∥xr − x∗∥∥ b

∇F \(xr\) − ∇F \(xr\)∥

\(e\)

h

i

≤ E \(1 − µF βr\)∥xr − x∗∥2 \+ 2\(βr\)2\(σ2 \+ B2 \) − 2βr\[F \(xr\) − F ∗\] \+ 2βrD

F

F

X LF δ

where in \(a\) the non-expansive property of the projection operator is applied, in \(b\) we add and subtract the term b

∇F \(xr\), use the well-known inequality ∥a \+ b∥2 ≤ 2∥a∥2 \+ 2∥b∥2, and in 33

Under review as a conference paper at ICLR 2023

the last term we utilize Lemma 4; \(c\) results from the application of Lemmas 3 and 4; \(d\) uses strong-convexity \(Assumption 6\) of F \(·\) and Cauchy-Schwartz inequality; finally, \(e\) results from the application of Lemma 3 and Assumption 1\(b\).

Rearranging the terms, we get

h

i

2βrE\[F \(xr\) − F ∗\] ≤ E \(1 − µF βr\)∥xr − x∗∥2 − ∥xr\+1 − x∗∥2 \+ 2\(βr\)2\(σ2 \+ B2 \) \+ 2βrD

F

F

X LF δ

1

µ

F

1

E\[F \(xr\) − F ∗\] ≤ E

−

∥xr − x∗∥2 −

∥xr\+1 − x∗∥2 \+ βr\(σ2 \+ B2 \) \+ DX LF δ .

2βr

2

2βr

F

F

Summing over r ∈ \{0, . . . , T − 1\}, multiplying by 1/T and using the fact that βr =

1

, we get

µF \(r\+1\)

T −1

T −1

1

X

µF X

E\[F \(xr\) − F ∗\] ≤

E r∥xr − x∗∥2 − \(r \+ 1\)∥xr\+1 − x∗∥2

T

2T

r=0

r=0

T −1

1 X

\+

βr\(σ2 \+ B2 \) \+ DX LF δ.

T

F

F

r=0

Telescoping the sum we get the following

T −1

1 X

µF

\(σ2 \+ B2 \) log\(T \)

F

F

E\[F \(xr\) − F ∗\] ≤ −

∥xT − x∗∥2 \+

\+ DX LF δ

T

2

µF

T

r=0

\(σ2 \+ B2 \) log\(T \)

≤

F

F

\+ DX LF δ.

µF

T

Therefore, we have the proof.

D.2.6

CONVEX OBJECTIVE: PROOF OF THEOREM 4

Proof. From the update rule of Algorithm 2, we have for βr = β for all r ∈ \{0, 1, . . . , T − 1\}

E\[∥xr\+1 − x∗∥2\] = E\[∥projX \(xr − β b

∇F \(xr; ξr\)\) − x∗∥2\]

\(a\)

= E\[∥projX \(xr − β b

∇F \(xr\); ξr\) − projX \(x∗\)∥2\]

\(b\)

≤ E\[∥xr − β b

∇F \(xr; ξr\) − x∗∥2\]

\(c\)

= E\[∥xr − x∗∥2 \+ β2∥ b

∇F \(xr; ξr\)∥2 − 2β⟨xr − x∗, b

∇F \(xr; ξr\)⟩\]

\(d\)

≤ E\[∥xr − x∗∥2 \+ 2β2∥ b

∇F \(xr; ξr\) − b

∇F \(xr\)∥2 \+ 2β2∥ b

∇F \(xr\)∥2

− 2β⟨xr − x∗, ∇F \(xr\)⟩ − 2β⟨xr − x∗, b

∇F \(xr\) − ∇F \(xr\)⟩\]

\(e\)

2

≤ E\[∥xr − x∗∥2 \+ 2β2σ2 \+ 2β2L − 2β\(F \(xr\) − F ∗\)

F

F

\+ 2β∥xr − x∗∥ ∥ b

∇F \(xr\) − ∇F \(xr\)∥\]

\(f \)

2

≤ E\[∥xr − x∗∥2 \+ 2β2\(σ2 \+ L \) − 2β\(F \(xr\) − F ∗\) \+ 2βD

F

F

X LF δ\]

where \(a\) follows from the fact that x∗ ∈ X ; \(b\) results from the non-expansiveness of the projection operator; \(c\) uses ∥a − b∥2 = ∥a∥2 \+ ∥b∥2 − 2⟨a, b⟩; \(d\) utilizes the fact that ∥a − b∥2 =

2∥a∥2 \+ 2∥b∥2 and unbiased gradient from Lemma 4; \(e\) results from Lemmas 3 and 4, the convexity assumption of the implicit function \(Assumption 6 with µF = 0\) and the Cauchy-Schwartz inequality; and \(f \) results from Assumption 1\(b\) and Lemma 3.

Summing over r ∈ \{0, 1 . . . , T − 1\}, multiplying by 1/T and rearranging the terms, we get T −1

1 X

∥x1 − x∗∥2

2

E\[F \(xr\) − F ∗\] ≤

\+ β\(σ2 \+ L \) \+ DX LF δ.

T

2βT

F

F

r=0

34

Under review as a conference paper at ICLR 2023

Using Jensen’s inequality and denoting x = 1 PT −1 xr, we get

T

r=0

∥x1 − x∗∥2

2

E\[F \(x\) − F ∗\] ≤

\+ β\(σ2 \+ L \) \+ DX LF δ.

2βT

F

F

Therefore, we have the proof.

35

# Document Outline

+ Introduction
+ Properties and Implicit Gradient of Bilevel Problem \(1\)

  + Preliminaries
  + Implicit Gradient
    + Approximate Implicit Gradient
    + Stochastic Implicit Gradient
+ The SIGD Algorithms and Convergence Analysis

  + The Proposed Algorithms
  + Convergence Analysis
+ Experiments
+ Related Literature
+ Solution methods for the LL problem
+ Additional Experiments

  + Numerical Results
  + Adversarial Learning
+ Proofs

  + Proofs of Section 2

    + Proof of Proposition 1
    + Proof of Lemma 2
    + The proof of Lemma 3
    + Proof of Lemma 4
  + Proofs of Section 3

    + Proof of Lemma 5
    + Proof of Proposition 2
    + Proof of Theorem 1
    + Weakly-Convex Objective: Proof of Theorem 2
    + Strongly-Convex Objective: Proof of Theorem 3
    + Convex Objective: Proof of Theorem 4
